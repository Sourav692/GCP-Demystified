
So now let's look at submitting jobs to a Dataproc cluster using the Web interface for Google Cloud Platform and also using Google Cloud Shell. If we make changes to our input and output sources replacing the HDFS with GS references, we can now read and write any persistent data to Google Cloud Storage. We can reduce or eliminate the need for the Hadoop file system. So it's just being used for temporary working space on the persistent workers. You might remember HDFS couldn't be run on preemptable worker nodes because the preemptable nodes could be taken away at any time which would cause the Hadoop file system to have to replicate the charge to other nodes. So moving persistent files to Google cloud storage is going to allow us to use less persistent workers and more preemptables since we won't now have a bottleneck potential on the Hadoop file system. You could build a cluster that uses, say, one master two workers so you have a bare minimum Hadoop file system for temporary working space and then tens or even hundreds of preemptable nodes optimizing your costs utilization. And since your input and output are now coming from Google Cloud Storage, you won't have any bottlenecks for the input or the ultimate writing of your output data. Now, to submit a job to dataproc we can use the web console or we can do it via G-cloud. Sending our jobs to the master node which then distributes the work. Hadoop, Spard, Pig, and all of those can now write directly to Google Cloud storage. So we really just need to submit jobs and have the appropriate references. So how do we move dataproc from on premise? Well, we use the idea of lift and shift. First, we want to copy the data that we're going to need the cluster to utilize in a Google Cloud Storage bucket. We could do this by uploading the files manually or we could install a connector to do it. The second step would be to up date in our code the references so we're no longer pointing to hdfs:// and instead, we're pointing to the correct path in gs://, our new cloud storage location. And third, we create a dataproc cluster, we run the job, and when we're done we delete the cluster. Now, if we read and write data to an H base database, what we're going to want to migrate that H base database up into a Bigtable, and update the references. It uses the same API as H base, so it's a drop in replacement when you utilize Bigtable. So the code changes should be minimal for that. And if we read and write data to sequel as part of our jobs, we might move our data into BigQuery so that we can query it very quickly and get back out our data sets and also on the output we can output our information into big query if we are going to be using it for analytics. Or if we're going to be using it for real time solutions, we can output our results into Bigtable. So let's say we currently run a 50 node Hadoop cluster at our organization and each day we are in a series of 10 jobs. It takes several hours to run. So we run them one after the other. So we submit our jobs at 5:00 pm, in about 2:00 am our jobs are done, and we're dealing with them the next morning. Well, unless your output of one job is depended on, or it is an input to the next job, why not run those 10 jobs in parallel instead of running them in sequence? So, we create 10, 15 node Hadoop clusters or hack make the clusters larger, submit the jobs in parallel, and they get your results much faster. So we have 10 jobs at 5:00 every day we fire up 10 clusters, we submit each job to a different node. If they're using cloud storage while cloud storage is going to provide the data to all of the notes. And if we're outputting to cloud storage, all our results can go that way. Since we create, use, and destroy the cluster, we're only going to pay for the compute time we're using. And we're not going to worry about having to worry, you know, about the cost of running that when the systems are idle because by deleting the cluster we give the resources back to Google. Now, with this in mind we're now able to basically, we can get our results faster. So if we had you know those 10 jobs were taken a total of seven hours to run or nine hours, instead, we could get them done in an hour or two and we can have our results by 7:00 pm. That's great. That's the cloud. So migrating your cloud dot or migrating your code to the cloud is really easy. Look at the code on the right. The first line has a reference where it's basically getting it's text files from an HTFS. You see we've crossed out HTFS and instead we're now referencing gs:// and the path which would identify the Google Cloud Storage bucket and then the path within that bucket to the files. Once we do this, we've now reduced our use of HTFS to just temporary storages that are persistent and we've made our cluster stateless. So, that's what we did by changing the HTFS to GS. So migrating the code is really easy. Look at the code on the right. The first line has a reference to an HDFS path. Just replace it with the new path and identify the Google Cloud Storage bucket and then the path within the bucket to the files. So, once we do this, we've reduced our use of HTFS to temporary storage instead of persistent storage which makes our cluster stateless. So we can now easily create, use it, and then destroy it. And the persistent data is in the cloud storage bucket. This maybe as far as we need to go to moving our Bigdata processing jobs up to the cloud. But, maybe we want to go a little bit farther. Well, the next step beyond that would be to actually rewrite the existing jobs or tackle brand new jobs by writing the code in Apache Beam and running them on serverless dataflow. But that's beyond the scope of this video. So the last part before I turn you loose on the lab is to discuss how we monitored the jobs running on cloud dataproc. After submitting a job, it can be monitored by the dataproc job sections of the web console. We can also monitor the cluster compute usage using the web UI either through the dataproc image or right down through the individual compute nodes through compute engines monitoring. Now, we can also monitor them using stack driver or Google's monitoring platform. So, now I'm going to set you loose on the second lab of this chapter, Leveraging Unstructured Data -Part Three. In this lab you're going to utilize a Google Cloud storage to store job input, output, and the application files, submit the job by the Web console, and submit another job through the cloud shell. Throughout the process your monitor your job process and view the results. So, go for it. Run the lab. And thanks for listening. This has been Grant Moyle on behalf of Google Cloud platform.
