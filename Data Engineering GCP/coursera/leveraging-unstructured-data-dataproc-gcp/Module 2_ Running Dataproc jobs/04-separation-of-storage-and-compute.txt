
Now, let's talk about the separation of storage and compute. Often, the first stage of moving from on-premise Hadoop and Spark is to make your existing jobs run as is. You don't want to change too many things at once. But once you get your data cluster processing your existing jobs, it's time to optimize them for our Google Cloud platform. Hadoop clusters typically rely on the Hadoop file system, or HDFS, which takes files and spreads them across the disks of the nodes. This allows for parallel read and write operations and also allows for larger disk sizes than you could create with a single disk. It also allows for the worker nodes to read/write files locally when operating. While this is advantageous for temporary data, if you utilize it for storage of persistent data either as your inputs to the cluster or the outputs after the job is done. If any of that is stored in the Hadoop file system, the cluster you can't just simply create and destroy when you need it. You really have to leave it running all the time. Well, that kind of defeats the purpose of utilizing Cloud platform. You might recall from a previous video, that the basis for Hadoop and the Hadoop file system were Google's papers that they published back in the early 2000s on the Google File System and MapReduce. Which the Open Source community took and built Hadoop, Hadoop File System and evolved into Spark. So talking about Hadoop and Spark, we often refer to YARN or Yet Another Resource Negotiator, for managing our cluster. And when we utilize the Hadoop File System for persistent storage, in addition to using it for temporary storage. You might keep your input files on that HDFS, which shards that data and spreads it across all the disks that make up the Hadoop File System. So all the disks on the compute nodes. And then when you execute your Hadoop job, they will read the input files, process and often put their output at the end of the job. Now, at the end of the job, since the files are sitting on the Hadoop File System, you going to have to go collect those. Well, the input data and the output data then kind of meets the category that that's persistent data, because you need it before and after your job is actually run. This is going to present an issue if we want to create the cluster on the fly. Remember, we can having it running in as little as 90 seconds, process our data and then get the output, then destroy the cluster, optimizing the use of our Cloud resources. So if you think about it, compute and storage, in a traditional cluster, are closely tied together or coupled together. When a compute node needs to be replaced, the shards of the file that are stored on those disks, must be replicated somewhere else. If we want to add new data, well, we have to store or append it to the files in the Hadoop file system. So the Hadoop file system must always be running. Or whenever we want to perform a file operation. And we may have to increase the size of the disks if our new data that we want to add is larger than we have allocated across Hadoop file system. So we have to pay to keep the nodes running all the time, which is fine on-premises because they're your physical hardware. But it goes against the cloud model.

We ingest our data from Hadoop file system. And then, we write it back out to the HDFS. So if I'm running a ten node Hadoop cluster, the data is sharded across the ten nodes. And let's assume that the four replication factor of three which is basically what the Hadoop file system does, it shards the data and places a copy of each shard on at least on around three notes. Well, we have to have three times the amount of storage allocated, so I want to have 10 terabytes. I actually need to allocate 30 terabytes plus I need to have some room for growth.

When the nodes replace the system as to ensure the replication factor is kept at that same level. And since the data prop machines are based on compute engine nodes, the discs assigned to the Hadoop file system are built based on allocation not usage. So there's a lot of downsides to the traditional HDFS model. Now, it's not going to completely go away, but it is going to go away for the purpose of our persistent input and our persistent output. What we would really like to see is storage based on our usage, not allocation that goes along with a good cloud theme. And the ability to quickly increase or potentially decrease the use of that resource, and have our costs adjust accordingly. Hm, doesn't Google have a storage often for holding data with an elastic twist? You bet, Google Cloud Storage buckets. So Google Cloud storage sits on top of the evolution of the Google file system, and these days it's a product within google called Colossus. Now Colossus offers a global file system over a petabit scale network. I've heard that petabit term a lot lately from Google, both in storage, you know they're putting a petabyte of new data into YouTube every day. And also in the form of the back end of their network. Now they didn't go out and build pedabit network cards or anything like that, but when they actually looked at their overall network backplane that connects the compute engine with your storage, they can pump pedabits worth of data around there per second.

But you don't see Colossus as a product option when you go up to Google Cloud platform. Well, you're not going to. That's an internal name for the platform that basically has replaced the Google File system, and is basically running behind the scenes on a lot of Google technologies, including Google Cloud storage buckets. So, if we can somehow incorporate cloud storage buckets into our either inputs or outputs for our data-plot cluster, we could replace the Hadoop file system dependency with Google Cloud storage So Google Cloud Storage is just one of the serverless options within Google Cloud Platform. The benefits of the serverless options like App Engine, which we use for Web and application hosting, BigQuery, which we use for data warehousing and it's a direct, and we can query like it's a SQL database. Pub/Sub for enterprise middleware or message oriented middleware. Dataflow for pure elastic pipeline's based on Apache Beam, and then all the machine learning APIs. We've got all of those in the serverless space where you basically pay for what you use. Well with Hadoop, we're very still much in the driver seat. We have to specifically create the cluster to our specifications. So the number of nodes, how big the nodes are, etc. We submit our job, we monitor the job, and then when we're done we have to go and destroy the cluster. But the long-term goal might be to move to a pure serverless platform that just allows you to submit a job, writing it in Java or Python using Apache Beam, submit it to Google Cloud platform and letting Google handle all of the scale with that. Even better, we might want to feed int hat data in batch form from Google Cloud storage, stream it in via Pub/Sub, process it with Dataflow, and then store the results out to BigQuery or BigTable, and we have nothing to maintain. Now that would be sort of the end goal of data analysis on Google Cloud platform, but we can't necessarily get there right away. We've got to go through a series of steps. So if my customer has an existing Hadoop infrastructure, they run Spark jobs. Well I'm going to want to support those in Google Cloud platform first, before I go and try to recode them to a whole brand new platform. So think of it as a series of steps to move us up to the Cloud.
