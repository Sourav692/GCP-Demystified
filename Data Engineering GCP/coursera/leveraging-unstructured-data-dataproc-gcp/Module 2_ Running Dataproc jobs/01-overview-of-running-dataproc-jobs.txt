Welcome to Google Cloud Platform. This is Grant Moyle. In this video, we are going to look at running Dataproc jobs on Google Cloud Platform. Dataproc is Google's managed Hadoop infrastructure so it's a fabulous platform for running your open source parallel computing jobs. In a previous video we discussed how to create a stateless cluster in less than 90 seconds using Dataproc. In this video we're going to discuss how to run our jobs on Dataproc clusters, how to submit jobs via a high level APIs, and also how to optimize the use of Google Cloud Storage as a replacement for your persistent data instead of using Hadoop file system. Once a Dataproc cluster has been created and then started, you can SSH into the master node, or in the case of a highly available cluster, any of the master nodes. From there, you can interact directly with Hadoop, run Pig/Spark, or Hive drops. Since all of these are preloaded, you can also interact with the Hadoop file system directly, or HDFS, as we'll hear it referred to, as that has been provisioned on your master and persistent worker nodes. The only nodes that don't have HDFS will be the preemptible nodes because they could be preempted at any time, so it doesn't make sense to have their disks associated with the Hadoop file system. Now the best way to experience SSH in any other cluster is to do it yourself. So, I'm going to just turn you loose on a lab, and in this lab, you're going to create a data plot cluster using Google Cloud Shell. Create a Google Cloud Storage Bucket and clone a repository. From there, you connect into the master node via SSH and run a Python Spark read evaluate process loop interpreter. Then you'll run a Pig job that'll use data stored in the file system. And then you'll destroy the cluster since you'll all be done with it. So pause the video and go complete the lab titled Leveraging Unstructured Data Part 2 
