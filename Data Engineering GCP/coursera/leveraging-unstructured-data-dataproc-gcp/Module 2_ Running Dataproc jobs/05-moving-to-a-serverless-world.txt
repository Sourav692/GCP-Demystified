
So a serverless design is going to focus on getting the inside out of the data instead of the administration of the servers giving you a near infinite scale, because Cloud Storage, Pub/Sub, Dataflow, BigQuery, all of these runs on Google's infrastructure which can scale to really whatever you need. You also pay for what you use, not for what you allocate. This allows you to experiment with small sets of data, and once you're happy, you can go a very large scale very easily, giving you the speed and freedom as well as controlling your costs. So in a serverless world we process using Dataflow and we get our input/output from Google's Cloud Storage for files and basically BLOB storage, or binary large objects. We use BigQuery for traditional database queries based on SQL. We use Pub/Sub for streaming of our data and we can even use Cloud Bigtable for NoSQL. What's Bigtable, you ask? Well that's Google's managed HBase compatible database. It's a drop in replacement for HBase. What was HBase? HBase was a database that's often utilized in a Hadoop file system type infrastructure. Because the HBase database gets spread across the Hadoop file system and the database itself is queried using the Hadoop worker notes. So HBase, just like the Hadoop file system gets closely coupled in an on-premise Hadoop infrastructure because the database file is spread across that Hadoop file system. So, when we start to do something like Dataproc and where our goal is to decouple the storage in the data we can do that because we can move the Hadoop file system data that's used for input and output over to Cloud Storage and we can decouple HBase because we instead can introduce Bigtable. It's drop in replacement uses the same API, so we update a couple references to where we're storing the data and presto we can decouple that as well. If we look at the architecture of BigQuery, basically BigQuery is a lot like a parallel computing system. We have nodes, the workers that basically process your query. They're basically referred to as slots in the BigQuery terminology. And then we have the storage that BigQuery uses, a highly optimized storage platform that Google uses for holding the database structures in the corresponding column or formats. BigQuery separates its storage from the database engine. We could submit jobs and get results back very quickly because Google can throw thousands of slots against the data and it could scale for large scale databases even up to the petabyte scale, so it's perfect for a data warehouse. So getting back to Hadoop and Spark clusters, can I separate my storage and my compute once I move Dataproc up to Google Cloud Platform or once I move my Hadoop up to Dataproc on Google Cloud Platform? You bet. Spark, which runs on top of Hadoop, we just need to modify the jobs so they're going to use our persistent storage coming from Google Cloud Storage for the files, and databases like BigQuery or BigTable for our database needs. Now, our processes have to change a little bit. So I'd like to think of this as our second big step in moving on-premise Hadoop up to the cloud. The first step was bringing our on-premise Hadoop cluster into Dataproc, making sure our jobs run and using HDFS as is. The second step would be to rework our persistent storage model, to use Google Cloud Storage instead of HTFS for obtaining the input files. And then our end result is writing those back out to Cloud Storage. With Dataproc, this actually isn't very difficult, because the libraries for talking to Google Cloud Storage are already present on the workers and the master notes. We just replace references that say, hdfs:// which is how we describe a path in the Hadoop file system with gs:// paths. And now we're taking advantage of Google's petabit bisection bandwidth that exists within the compute and storage back plane of Google Cloud platform. So is Cloud Storage really that fast? You bet. Google spent years optimizing their internal networks. Google uses a lot of software defined networking. And basically builds their own routers and the software that runs on top of it. So, if we go out and buy a traditional router from say, Cisco, they provide us a firmware or the software running on the router and it's got to be flexible enough to meet a variety of customers needs. So there's code in there for SNMP and older network protocols and all this sort of stuff. Well, when Google goes to work on their network stack, they don't even bother to put that code into the stack if they're not going to use it. So their core code running on their routers, basically, is stripped down to only the features they need. So if they're not using something like SNMP, that doesn't get rolled into the code. If they're not, you know, supporting an old protocol like, heaven forbid, IPX/SPX or something like that, it's not in the code. So only the pieces of code need to be in there when it gets compiled up. So that improves performance, another false step. And also makes it very scalable, reliable, etc. Now, they put all this- on top of that, they put a highly optimized storage protocol that drives Google Cloud Storage and that sustain read and write operations are faster than ever.
