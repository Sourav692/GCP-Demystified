So you can SSH into the cluster and it can run by Spark. Why would you do that? Why would you want to SSH into the cluster to that particular machine and run a program on the interpreter? Normally, the way you interact with a cluster is when you use a notebook or you submit a job into the Spark cluster but there are times when it's advantageous to be able to simply drop into the shell of the machine. Drop into Spark so that you can do a quick exploration. You can try-- you could try a couple of lines of Spark code. You can basically see if something works, if the data is in place, how long something takes. For those kind of quick experimentation, it can be very helpful to be able to SSH into the cluster. Start the PySpark interpreter and try out your quick experiments. 
