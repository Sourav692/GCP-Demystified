
So, part 4 of this lab had us go ahead and create a Dataproc cluster using initialization action that went ahead and installed Google Cloud Datalab. It also ran a custom script that we might build upon. So, once we have that up and running, we played around a little bit with Cloud Datalab so that we could see how we could leverage Cloud Storage BigQuery and Spark and everything from within a DataLab work book. And in areas that could be parallelized it could actually pass those down to my underlying Hadoop workers that are underneath the cluster. So, let's take a look at the highlights from that.

So, the first part in the lab we had you go ahead and take a look at the initialization action. So, if you hadn't done this lab already we had you clone in the code

from the data analyst at GitHub. We went ahead and had you run the replace an upload which put a bunch of files up into the cloud storage. And then look at a particular script. Now I'm going to look at this graphically, instead of using the gsutil command because we uploaded it in this step to our bucket. So let's go look at Cloud platform. I'm going to go ahead and go to the Storage.

Here is my bucket, same name as my project name which we created earlier. Here's unstructured. And there's a file here called the Knit Script. So this is a generic template that we want to use if we wanted to add additional software to run on either our workers or the master or both. So this is bash.

We went ahead and did an apt-get-update. So we went ahead and updated components on the system. We installed python-pip, and we told pip to then go ahead and install or upgrade the google-api-python-client. So that's going to get loaded on all of the nodes, both the workers and the master. Because you can only pass one set of scripts through and there isn't a separate area where you pass the ones that are going to run only on the master, and nary where they are only running on the workers. Okay, but in order to make our script run only or portions of our script only run on the master, what we do is we pull the metadata. So the command user share Google get metadata value is built in the data prop machines and you can then ask for a specific attribute. So in this case we ask for the attributes Dataproc raw. And if that returned as master then we are obviously working on a master. Okay, and then it finished. Okay. Now, to do data lab we could add a whole bunch of code in here to do data lab or instead we could go leverage the data lab initialization scripts. So while you were building this cluster, and I've already built mine. But I'll go ahead and go through the motions here, so you can see. So I'm going to go back to Dataproc and I am going to create a cluster.

I'll call it my datalabcluster. I'm going to go ahead and say how many nodes I want.

And down here under the preemptible worker, another networking and other node branches, there's an initialization action. So down here, we are actually have you copying in this line which goes to a project as bucket called Dataproc initialization action. So that's the name of a bucket and it's open to the public to use. And then there's a directory called Datalab and one called data lab.sh. So I'm going to go down here to my initialization ex. I'm going to paste that in. So we've told it we want to run that script. Now, in a second I'll show you what's actually in that script. Now we also wanted to run maybe some customizations. So let's go ahead and call that one from our bucket that we just looked at, the init-script. So I'm going to paste that in here. But I do want to replace the bucket name with

the name of my bucket that I'm actually using. So I've got that handy on my clipboard here. So let's go ahead and put that in place.

Here. There we go. So we're going to call this script as well as this one. So one's going to install DataLab and the other's going to do our own custom and initializations. Now, that will now kick off the creation of a new cluster called my Lab. I'm going to click Create on that. What you would notice is this is going to take longer than just spinning up a generic cluster where you didn't have any parameters because it's going to perform extra steps on each node including the apt update. The apt get update because well, that was in our script back here, init script plus, it's going to do all the Data init lab initializations. So, how could I tell what is in actually this script here? Well, there's two ways. One would be to go to the GitHub that talks about the Dataproc initialization actions, and that link is referenced in your course notes. The other is you could actually just use the Cloud Shell.

Because it is just out there in as a cloud storage bucket.

And what I can do is go ahead and do, in fact, I've already done it, is I'm going to do a gsutil. I'm going to cat, so I'm going to type the file, gsdataproc initialization-actions/datalab/datalab.sh. I'm going to go ahead and send that through more.

And so, what we're going to see is okay, it's bash script, got a bunch of comments. It goes in, does a couple things, exports our environment, goes ahead and get some role equals. So it goes in, reads the metadata, what's the role? It goes in, gets the project ID, so that way it can dynamically adjust. Now it's looking to see, did we provide a Docker image attribute, and I don't think we have. And then it's looking to see what Spark packages were loaded. So it's looking to say, if it's on the master, then we want to do this block of code. We want to do an apt, we get update, we want app to install, and we want to install Docker. Okay, it's going to do Docker. And if there's no parameter has been passed we want to go ahead and use the Docker image. We're going to set docker image set to pull the generic cloud data lab, data lab docker image. Otherwise, if we passed a parameter we could do a different one. So that's okay, we just want the generic one. So it's going to pull that in, and then it's going to do a little bit of spark configuration and some data lab configuration and some Python environment variables and some Spark configuration whole bunch of things to make it work. So, we'll kind of take this all as okay it's going to work and that was what's actually running on our cluster. Now you'll see our cluster still forming because it's got to run all that extra code down, so give it a few more minute.

And if it's seems you're getting impatient with it, give it a few more minutes. Now I've in fact already got mine one spun up with this. This one here called cluster one. And so, I'm going to go ahead and access Datalab with this one. So, to get the IP address of the cluster, I'm going to jump over to

compute engine, cluster one. Here's my master, so let me grab the IP address of that. And now, I'm going to open up a browser. And remember those firewall rules we made earlier?

We allowed port 8080 through, and that's where Datalab's going to sit. So Datalab is running on that compute engine as a docker image. And then we connect to it by the web interface.

And it's going to start us off by looking at a filesystem that's actually located on the local disk of the master node. And there's some documentation. There's a pre-built notebooks. Later in the course or later in the program, if you're watching a whole series from the data engineering.

Courses, you play around with Datalab, and we have more notebooks and things like that to play with. But let's just create a simple one. So I'm going to click on Notebook, and it's going to start me off with a generic notebook that I can work with. Now, this generic notebook, and we can scroll down through the course lab, here we go. This, we want to do temperature conversion. I want to take Fahrenheit and I want to turn it into Celsius.

So I can paste that in here, and I can go ahead and say, Run.

And what it's doing is the notebook is basically getting setup. So the first code you run usually takes it, I don't know 15, 30 seconds to run, and then it gets an output. After that, your engine is usually warmed up and you're ready to take a bunch of more code. So here, we said temperature 212 and it's going to run that Fahrenheit- 32 times 5 divided by 9. So let's go ahead and say, well, outside, today, it's 95 degrees, and I'd like to know what that is in Celsius. So there we go, it's 35 degrees. Okay, cool, so ran that code for us.

Well, let's try running a BigQuery code. But before we do that, we need to initialize Python a little bit. So I'm going to put this code in here. So saying, I want to import pandas as pd, and so that's I'm now got pandas, which is a library for basically connecting data tables together. And then, I'm going to read pandas.io. I'm going to import gbq. So I'm going to import Google BigQeery. So when I run this buck of code, all it did was just set up our Python environment to recognize those two, and then we printed the text imports run. So now, here is an actually BigQuery we want to run, so let's go run this. So I'm going to to put this code in here. Oop, I'm going to need my project name, so I better change that. Luckily, I've always got it saved out here on the clipboard, so there's my project name.

Okay, and we're going to perform everything in here as a select query that this command here is actually going to execute. It's go to SQL, that's what goes in here, and the project ID is going to be our project. So when I run this, presto, it's running the query, and then the results. And it went ahead and pulled out our information for us. Now, it said it got 40 rows, but we're only showing 5. Well, that's because our code down here is to only show 5. Let's go ahead and show that 40. And when we run it, there's our rows of data. So we're interacting with BigQuery through this workbook. Now, what we can do in here is we can actually go with more than just code. I can actually go up here and I can say, I want to append some just markup. because I want to say, hey, This is my results, oops, don't think I put that in the right place. Let's go ahead and, yeah, actually, we should. This is my results. And- if you don't see 40 columns above- something went wrong! All right, so I got my areas where I put my code, but I can also have areas where I put my text and everything. And so basically, I can describe what's happening.

Okay, so we went ahead and ran. So the BigQuery ran, the results was put into what's called the Panda's DataFrame. We just had the five records, but I went ahead and tweaked that a little bit, so that ran. Now, I'm going to add another codeblock that's actually going to plot that data up because we have two columns, we have year and we have average weight. So let's go and put that into our codeblock here and let's tell it to run that. And look at that, it created me a chart of the data. Okay, that's cool. And we could go ahead and have our markup here where we could say, You should see a pretty normal distribution

of data blah blah blah, like a good scientist, put all of your details in there. Now, what we're able to do is we're actually, so we are able to perform a series of BigQuery operations. Now, we can also do other things. We could submit Spark and Python and really, anything. And if there's jobs that can leverage the underlying cluster, they will. Now, what we are going to want to do is we might want to save this notebook. And right now, let's just call it Untitled Notebook. So let's just call it, My First Notebook, and I go ahead and click OK. And now, that's got a name, and it does autosave as you're working on it. So you don't have to keep remembering to go and save it, like some of those other tools we use to write things up. Okay, so now, there are other notebooks in here. And I encourage you to do a little exploration. What you notice is My First Notebook opened up on a new tab when I said New Notebook. So you want to jump back to Google Cloud Datalab here, and look, My First Notebook is saved in that directory. Wow, that's kind of cool. And what you want to do is you can navigate through. In the docs directory, they have a whole bunch of really cool sample Datalab workbooks to work with. So there's docs, and then there's samples, and there's tutorials in here. And basically, it sort of shows the real power of Datalab. So if you've never worked with these before, this is really neat. If you're continuing on through the course, you'll encounter lots of these Datalab notebooks that we work with. Like here is some Anomaly Detection and Logs and Conversion Analysis. And as we get into TensorFlow and Machine Learning and things, we'll do a lot more with Datalab. Okay, well, when you're done, you can shut down and close your tab for your notebook. The other thing you want to do is when you close things out, and I have seen this with Datalab, giving me a little bit of issues. If it ever seems to just hang, check to make sure that you're not necessarily running some other, you don't have a whole bunch of running behind the scenes. It gets a little temperamental at times. So if I click on this icon over here, sorry, this one, so it must be running sessions. And hopefully, they'll tell me, I don't have any running right now.

See, it still thinks that My First Notebook is open even though I closed the tab. So I'm going to say Shutdown on that. So I have no notebooks actively running, so I can go run a new PySpark one.

So what we want to do for the last part of this is we went ahead and pulled over to clone from GitHub and we brought over this repository. So that's this icon right here.

I've gone here, and I said, all right, I would like to bring over from a repository. I would like to bring over some other workbooks to work with. So we'll give it a second. It's using Ungit, which will go ahead and sort of an embedded GitHub in here. And down here, I want to go ahead and Clone from. So I want to clone from https://github.com/GoogleCloudPlatform/tr- aining data analysis. And I want to go ahead and put these into just my base directory. So we'll have it, we'll just have it, it'll actually put it into a directory called training data analyst. So I'm going to clone the repository for that.

Okay, so it's just cloned from there. So now, I can close that window, I can close this one. And when I go back to Cloud Datalab and I go to the very top, to the Home,

My directory structure has datalab, good. So I will click on datalab. And then, We'll give it a second here. I guess I should have given my compute, my master node a little more computing power. We've got docs and notebooks, so let's look in notebooks.

And now we have training data and analyst. So I'm going to go into data training data analyst, I now will find all the course content. Not just for this course, but actually we've put all of the stuff in there, because we keep it all in one place. And in there I'm after the one for unstructured, so courses unstructured.

Now you could shortcut and you could actually put in the direct path up in the URL. It's just where navigating through and then we want the one called unstructured

And then we want the one called PySpark Test Solutions. So do we have one for PySpark, yep that's a Jupiter file or IPython as it was originally called, and when we bring this up, this is going to be a data lab with a little more stuff prebuilt into it.

So it's coming along, there we go. Okay, so we got a whole bunch of code blocks to run. So this first block of code, here we've got about four lines. And you'll notice how it's kind of singled off there. So when I click Run, that section's going to run.

Okay so it's running, remember the first time you run Notebook it can take a few seconds to run but once it turns blue you're done running that block of code. There was no output on this particular one, so it didn't send us anything, this was just sort of setting up our environment. This one here, we're going to go ahead and run this block and this is going to parallelize in operation to a range. I think it looks like create a range of 1,000. And then perform them, calculate the mean of that, which is 499.5. Then we're going to go ahead and run this block, which is our data as one, two, three, five, eight, 16, 32. And we want to parallelize, and we want to do a and b, and we want to add the two together. Okay and then the sum of that would be 231. Okay looks like we are playing with a little bit of a Fibonacci series there. Okay, so now from pyspark we want to import and we're going to do a little bit more, so we need our bucket name here. So we're probably going to have to go through, yep, they said make sure you replace your bucket name with any occurences. So I'm going to go copy my bucket name and replace it back in my sheet here. So it's going to grab a file called pets out of my cloud storage. It's going to select the count from the animals from there and then it's going to create some information. So let me Run that block of code.

So it's reading the file and then it's parallelizing it, just like we were playing earlier with the pig scripts and building out our cluster.

Okay, so that's working. Obviously taking a little bit longer, because it's leveraging the Hadoop infrastructure underneath. But now we've got, okay, frogs. We had two pigs, we've got that. We've got five and we've got six dogs. So the last one was to perform another operation here and to go ahead and do some calculations in there. So we'll click that.

And, actually sorry, not calculations, just some concatenation of some values together. Okay, so we concatenated various values to produce our output. And there we go. We've got DataLab up and running on Google Cloud Platform and we've seen that it can run some Python scripts. And we're going to assume that this was all running and it was on the underlying Hadoop.

Now, I did kick off my cluster. I kicked off a second cluster here. And it's probably up and running now. So let's verify that DataLab came up on that one. So the way I'd verify that is to just go to http: the 8080 address on the IP of the master nope and if you get a kind of error or nothing comes up right away. Give it a chance to finish building the cluster. First access to any sort of web server. Yeah we see it's actually up and running already, because it needed, I'd give it you know three or four minutes to do all that work versus normally about the 90 seconds of spinning up a cluster.

But there we go. We have data lab running, and in fact I have data lab running on two clusters.

So go ahead and return to the videos and continue on.

So let's say you want to process some BigQuery dataset in your Spark cluster. Wait a second, why would you ever want to do that? Remember that BigQuery is serverless so you can essentially say, "Here's MySQL statement I want you to run" and BigQuery will scale it out to thousands of nodes for a few seconds and come back to you with the answer. That is a whole lot more scalable than doing some similar thing with Spark SQL for example. So why would you have data that's in BigQuery and process it using Spark? Well, it's not really a good thing for doing SQL statements but maybe what you want to do is that you want to run a Spark machine learning program. And at that point because you want to use the Spark library on data that exists in the BigQuery, you need to have this connection between a BigQuery dataset and your Spark program.
