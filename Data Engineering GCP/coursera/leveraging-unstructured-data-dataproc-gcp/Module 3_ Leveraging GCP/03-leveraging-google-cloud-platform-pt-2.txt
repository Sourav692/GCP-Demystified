
So, to create the script let's look at an example. Here we're looking at a bash script that basically is going to perform an apt to get update. So to update the cluster nodes and then also do an apt get install and install a series of libraries that we need. So we're asking for Python Numpy, and Python Scipy, and Python Matplotlib, and also Python Pandas. So those four components are going to be installed as well as an apt get update. Now you'll notice the apt get update has a pipe pipe true on it and that's going to pass in the true value so that way it isn't waiting for user input. And you'll notice that the apt get installed, we have a minus Y and that basically tells the script that it should go ahead and confirm yes to any options. Otherwise it's likely to get stuck waiting for input and our initialization will have to eventually timeout on that and continue onward and we wouldn't necessarily want that. But what if I only want to install software on my master node? Well, Google Cloud platforms dataproc visualization uses the same script for both the master and worker nodes. They don't differentiate between the two that way if you need to install something on both nodes, you don't have to do it in two places. The downside to that is what you just want to do something on the master, you need to have your script figure out which one's the master or which ones are the master if you're running a multi node master. Now, the way we do this is through metadata. Now, you'll see how we're asking or resetting the value of roll equal to the metadata value attributes data pop proc role. So if the roll comes back as master, we're going to go and install VI or vim there. So we see the apt get minus y vim. So it's going to install that and confirm yes to any prompt that appears. We're not going to install anything specifically on the worker nodes but if we look at the very last line of this script, we're going to go out and put numpy and scipy and matplotlib and pandas on both nodes because they're likely to both need those set of libraries for when they're executing a job. Now, after we create the script or the program, we need to save it and then we need to upload it to Google Cloud Storage. So here we're seeing the gsutil copy command, we're taking the script we have locally called my_init.sh and we're going copy that up to a cloud storage bucket and into a folder or directory, and then the file itself. Now, what about if I want to initialize standard things like flanker, oozie? Do I really have to go write those scripts from scratch? Well no you don't. Google's actually got a git repository that contains the initialization scripts for a lot of other open source software that we run on Dataproc. But in addition to that they've actually created a public facing project where you have read-only access to the storage bucket and it actually has files in there as well. So let's first take a look at the git repository for this. So I'm going to click on this link here and we see that they've got up on GitHub, they have the scripts necessary to initialize datalab, flink, kafka is there. Although Kafka is only supported right now on the preview as of the currency you have to be in one of the preview configurations for your Hadoop, they got oozie in there, they've got tez, they've got stackdriver, they've got zeppelin, they got zookeeper, you can play around with a whole bunch of initializations. And in addition to that, if we go take a look, they've also put it as I said into a bucket. So let's take a look at what's actually sitting in that bucket. So I'm going to flip back and I'm going to go to my cloud flat platform project, which I have over here, and I'm going to open up Cloud Shell. So Cloud Shell is going to initialize here for me and we're going to go ahead and take a look. Let me grab that other screen so I have it somewhat up on my screen here. So we want to look at that bucket called gs://dataproc-initialization-actions. So if I want to see what's actually in there I'm just going to do a gsutil ls and we're in Cloud Storage, so we want gs://dataproc-initialization-actions and we will hit Enter. Now if we have permissions to that, presumably we should get back a listing and sure enough there it all is. We've got a listing of all of the directories and the files located there. So let's say we want to initialize Datalab. Well, let's take a look to see what's in the Datalab directory. And sure enough, there is a read me and there's also a datalab.sh. So let's go and change our command to cat. So, gsutil cat gs://dataproc-initialization-actions/datalab and let's go ahead and say we want to look at datalab.sh. And that there is a whole series of commands, so let's see what this is. This is a bash, it is way up there, it is pretty long. Yeah we're running a bash script here that Google created that goes ahead and runs a series of commands to bring down the docker image for Datalab and then initialize it and make it available to us. Wow, that's pretty impressive that all that is done by just pointing to an existing script. We don't have to figure out how to initialize that, Google's going to do that for us, and that's kind of a nice thing. And so that's the initialization action. So when we actually go to create the Dataproc cluster we can specify that we want to use gs://dataproc-initialization-actions/datalabs/datalabs.sh and we'll actually do that in the upcoming lab. Now this is what the actual Dataproc initialization looks like. So you might remember, we have created a Dataproc cluster from the command line before using the GCloud command, dataproc clusters create mycluster. And I want the initialization actions to pull from Google's cloud storage, and the name of the bucket and then what we actually want to perform. And then in this case we're actually adding another line where we're asking for an initialization-action-timeout of three minutes. So for some reason a step in that or the script or program takes more than three minutes it's going to be canceled. Now it's not going to destroy the cluster or anything like that, it's just not going to complete that step. But that's better than having it wait endlessly before it comes online. Now if you're going to use the web console, you can also specify the initialization action. And as you add initialization actions and hit Enter, it'll give you another blank line because you can actually support multiple initialization actions. Now, if you need to handle multiple initialization actions from the GCloud command, just put a comma after each reference to the initial initialization action. So we'll have one --initialization action parameter, and then you'll list all of the various files separated with commas and no extra spaces, just comma between each of the gs references. OK so initialization actions are basically going to give us optional execution scripts we want to run which are going to run on each node of the cluster. Now, we can install additional files, we can stage files, we can change the node configuration. Basically, anything you want it to do programmatically within the cluster. And Google does provide for a set of common initialization actions by the GitHub and the bucket. However, you also have the option that what if you needed to change a specific configuration property of the cluster. So, what we call cluster properties. So, let's say, there is a particular parameter you needed to add or update in core-site.xml. Well, there's nothing to say you couldn't do it through initialization action but to facilitate it and make it easier, Google allows you through cluster properties to set the file and its prefix, colon and then a property in what you want the value to be. And, this can be done through the Cloud SDK or the gcloud command line. Right now, it's not in the graphical user interface. But, that's just a matter of time before it finds itself into there I'm sure. So now it's time for you to give it a try. Head over to quick labs and complete the lab on Leveraging Unstructured Data: Part 4. This will have you create a new Dataproc cluster. It'll initialize Cloud Datalab on there, so that you have Datalab up and running. Now, you'll leverage the code to basically have it install that using the GitHub or that public bucket initialization script. Plus, you'll create your own initialization script to put some other components on there. Then, you'll work with a Jupiter notebook to create a Python and PySpark jobs that will utilize Cloud Storage. They'll run a BigQuery operation and run some Spark on your Dataproc cluster. Now, one quick thing is when you're utilizing your project for this lab if you haven't gone into BigQuery and initialized it, basically open BigQuery and confirm any prompts. You want to do that before you run the lab so that way the BigQuery commands will run successfully.


If you're familiar with the metadata server from Compute Engine, this is extremely similar. The one difference is that Dataproc knows about master nodes and worker nodes, and therefore, it knows when a new worker comes on. So, for example, if there's a new preemptible machine that comes along, all of the initialization actions that have to happen on the worker nodes will happen on those preemptible workers as well.

So in this lab, we'll put together the two concepts that we've looked at. We will create a dataproc cluster with an initialization action. The initialization action that we will use will be to install a Jupiter notebook datalab on the dataproc cluster. And now that we have datalab installed in the dataproc cluster, we'll go into data lab and we'll write some code, Spark code that actually processes the query data.
