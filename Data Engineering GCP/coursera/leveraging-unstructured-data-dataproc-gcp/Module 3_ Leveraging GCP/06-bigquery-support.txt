
So let's talk about BigQuery support and the integration with Hadoop and Spark when running on cloud dataproc. So, in the last lab, we're working with the Jupyter Notebook and where was the computational work actually happening? Well, it depends on which part of the Jupyter Notebook we're talking about. When you were running the BigQuery portion, the Jupyter Notebook was just submitting a query over to BigQuery, and that all happens on the Google side, and then the results were passed back to us. If we're running some of the NumPy or SciPy or the Pandas code, well, that would all execute on the master node just as Python code. And, if we were executing into the Spark, that was actually being passed off to the workers and the worker nodes were executing that. So, we're a little bit of work happening everywhere. So, how do I directly integrate BigQuery with Hadoop and Spark jobs? Because Hadoop and Spark jobs don't directly know how to talk to BigQuery. Well, the answer to that is to actually use Google Cloud Storage in the middle. So the idea is you could run a BigQuery job and have the export store the results to Google Cloud Storage and then you could bring it into Hadoop via the Google Cloud Storage. Likewise, our output from our worker nodes, we could store into Google Cloud Storage in a format that is suitable to import into BigQuery. So we kind of use that as the middleman and the go between the two. Now, is this anything specific? I mean, this is brand new that something that we didn't have to do before. Well, not really. Most of the time when we're using Python or Spark over an old traditional on-premise, we would use the Hadoop file system as that middle ground. But because that Hadoop file system basically is part of the cluster node, we want something that's happening external to the cluster because BigQuery is going to store its results to BigQuery. It really doesn't know how to work with Hadoop file system. And, likewise, we want the results to be available at the end of our Dataproc job so that we can clear out and destroy the Dataproc cluster when we're done with it. No reason to keep it around if we're not using it when we can initialize a new one in 90 seconds or less. So what does this look like? Well, the first thing, assume that we have put all the data from our big query results into cloud storage. That's outside of the data proc cluster, that's really easy to do. So we split the data out of big query. So now we have to set up a connector within our Hadoop framework to basically go bring from the input from the cloud storage. So you'll notice here we've got Spice marketing getting some initialization, we've got a bucket so we're identifying the buckets and it's going to hold the data, we're identifying the project we're using, and basically we're identifying the input directory that's going to be made up of the cloud storage bucket, and then the appropriate path to bring in the input. So we've got the project, the bucket and the input. We don't actually call the big query from here, because we assume big queries already done it, big queries dropped its data in an appropriate format into the bucket. Now the next thing we're going to do is, we're going to load that data into Hadoop. Now, we're going to load it in as a resilient distributed data set. So basically, it's JSON data we're bringing it in, and Hadoop's going to know how to interpret it. Now, we might see a reference here to a big query, but Hadoop isn't directly knowledgeable about what big queries doing, it's just seeing that, OK there's this class that's being brought in, pulling in our big query input format of our data. But it didn't actually execute the query. The query was done long before that. Now the spark code is going to be the same as usual, list is going to work with the data set that we just loaded in. So in this case we might do a word counts. You know, so we've got some functions in here to load the records and perform a word count and then a reduction. Now, because that's going to execute on Sparc, the output is going to be shorted into multiple pieces to be starting go to club storage. Why? Well because, SPARC is parallel processing it, you would not want them all. All the results trying to go back into the same file, because cloud storage is an immutable blob storage in that, each operation would replace the file that was there before. Well, if you're a pending data what an append operation looks like with something like cloud storage, is you have to read the data that is there, append it to the file in memory, then write it back out to cloud storage. And if you've got four or eight or more nodes all doing that simultaneously, you're going to wind up, basically, with too many cooks in the pot all changing the same file. So what you want is xargs, you want each worker to basically work with a different file, and you'll wind up with a series of files in cloud storage instead of just one. So you end up with like part zero of ten, part one of ten, Part two of ten, part three of ten, all the way up. And then what we can do is then leverage that information to the end. So we've got our output being started being put out to the output. And then, if we want it into big query at this point, so shorted to close the loop of the other end that we want to take our process data we want to import it to big query, we're just going to import that data in, and based on how we outputted it to the cloud storage, is the shards we can import it and basically apply a schema to it, and presto, we have the data now in big query. When we're done, we're going to want to do a little cleanup. We're going to want to clean up our files, because we now have the result in big query, we don't need the temporary files that we created along the way. Often it's easier to extract data in BigQuery, pull the data into the Spark cluster for further analysis, and then possibly put it back out to BigQuery if you can't do everything you want in BigQuery.



So the easiest way to interact with a BigQuery dataset from within Spark is to essentially use the BQ package in Python to execute the sequence statement on BigQuery. Take the result set that comes out and convert it to be a pandas data frame. And now, if you have a pandas data frame, you can then go ahead and operate upon it using Spark. So that's the most common, that's the best way to deal with the integration between BigQuery and Spark. But this only works if you're getting a subset of your entire table. If what you're getting is an aggregate and you're computing those aggregates, the sums and counts et cetera in BQ in BigQuery and you're taking the results of the query into a panda state of frame. But this doesn't really work on a complete BigQuery dataset, that's because a pandas data frame has to fit in memory. And, it's unlikely that your BigQuery dataset is going to be small enough to fit in memory.
