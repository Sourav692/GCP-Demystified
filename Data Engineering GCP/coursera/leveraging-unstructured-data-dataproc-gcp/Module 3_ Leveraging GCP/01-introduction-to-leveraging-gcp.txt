
So far, in this course, what we've been looking at is about migration from an on-premise Hadoop cluster to GCP. So we just said, take the code that already runs on your on-premise cluster that the Spark or Pig or Hive or whatever code there is. The one change what we're asking you to make is to change it where you're reading from HDFS. Don't read from HDFS, instead read from GCS and by replacing that read from reading from HDFS to replacing to GCS, all of a sudden things get much better economically. You don't need to have a cluster up all the time. You don't need to have a cluster that people are competing for. You don't need to over provision the cluster for those small periods of peak usage. You get much better economic benefits, you get ephemeral clusters by doing one simple thing; changing your input from HDFS to GCS. So that's a recommendation. If you're migrating from on-premises where you're doing Hadoop workloads to a public cloud, replace HDFS reads by GCS reads. So that you take advantage of the extremely high sustained read performance that GCS provides. And this allows you to create clusters that are job specific. So you create a cluster, you run the job and you delete the cluster. So that's basically what the last couple of chapters have been about. But now, migration has been done. You are on GCP. What else can you do? What we are going to now look at is how to leverage the power of GCP beyond the on-premises jobs and migrating them. So you've done that. You now have your on-premises jobs, you've migrated them to the Google Cloud and they're now running on the Google Cloud. Great. But now that you have jobs that are running on Google Cloud, let's not treat your Hadoop and Spark jobs in isolation. Let's look at how you can take advantage of the rest of the capabilities of the platform.
