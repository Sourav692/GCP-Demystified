So how did you do with the lab? In the lab, you would have created your Dataproc cluster and then connect it into using SSH. You'd have added some firewall rules that would have allowed you to access the Hadoop-specific web pages because Hadoop will build a web server listening on a non-standard port so that you can connect in and see the jobs running. And also - Hadoop file system also runs a web interface. And then last, you created a dataproc cluster using the command line interface. So let's take a look. We'll review the lab and I'll highlight some of the key sections. So there's the lab and here I have a Google Cloud Platform project. Okay. So we connected in using qwiklabs, all of that looks good. We're on our project. So we needed to create our dataproc cluster. So to create it using the web interface, I'm going to click the hamburger stack or the menu bar. We often call it the hamburger stack. And let's go down to dataproc. Now the first time we go to dataproc, it hasn't initialized the API, so we'll make sure we click that and it'll initialize it for us. Now, while we're looking at this, let's take a quick look to see if anything else is going on in our project. So this is a fairly new project. So while it's enabling that API, let's see if there's anything showing under storage. Okay. Right now, we don't have any buckets or anything like that in storage because you're not required to have that. Now we're going to talk later in the course about where you would start to begin using buckets over using HDFS, but right now there's nothing there and I'll be using this same project for the demos for this chapter in the next few. So we'll come back and keep an eye on that later. Okay. So we're ready to create the cluster. I'm going to create it. I'll leave the name as cluster-1. I'm gonna fire it up in central1-f. I'm going to shrink down my node, my master and my workers to the bare minimum here, just because I'm doing it for example purposes. Okay. So we asked you to create a cluster, use the CPUs; or we can set the disk space. So the disk space being used here is actually going to be used as the Hadoop file system. Name node; in the case of the master and the worker nodes are going to be part of the HDFS data nodes. So if you shrink that down, it'll be a pretty small amount. Here, what you are being charged is for how much you allocate, so that's why we're having to shrink that down a little bit. Okay. But that looks like a pretty straightforward cluster, so I'm going to create that. And hopefully in about 90 seconds, we should have a fully operational cluster. Now - so that's going to fire up and let's watch what it's actually doing while it's creating it. So we see here in the cluster manager, we see, oh - cluster Cloud storage staging bucket. Okay. So it's got to have a little bit of a staging bucket to work with things. So remember, we just looked at Cloud storage and there wasn't anything. Well, look at that, there's now a bucket because the first time we ran that we created a cluster, it needed a bucket to store some information. Okay. So we're going to go back to our Dataproc and our cluster is coming up. So if we actually go and start looking at virtual machines now - so let's go up to compute engine - we actually see it creating the compute engine instances. See, so we've got cluster-1, so the name of my cluster; -m for the master and then I've got my two workers. We see they've been given IP addresses, external IPs and all that sort of stuff, although we should only ever need to interact with the cluster master. They're built in central1-f. And you want to make sure that zone is where your data is going to be because otherwise, you don't want to incur egress charges pulling it. So, you know, I've got my cluster built in us-central1, but my data's sitting over in, say, Asia East 1; well, then I'm going to have egress charges as the data is pulled over from Asia East 1. So just make sure we got it. Looks like the nodes are up. So if we click back to our dataproc, we should now see - oh. The cluster's still going because it's still initializing some things. So I'm going to go ahead and just bring my window out just a little bit so we can see a little more. See? So it still says provisioning. And once that's done - because it's not just a matter of building the worker nodes and the master node, it's a matter of scripts running inside that that integrate them together so that my workers are following the master for what they need to do. Okay. So that should be done here in just a second. Let's try and do a refresh here. Nope, it's still going. Gotta be patient, but it should be in just a second here. In fact, if we click on this, we can see the actual detail. Oh, yep; CPU's on the cluster are up. Here's our VM instances - it's showing our instances are all there. And when we look at the configuration, we see that our cluster - where it is, how many nodes we have, et cetera. So let's look at our VM instances and we should be able to SSH into this by now. So I'm going to go ahead and click SSH, get that connected in. I'll put my window over here for a sec. And so we're going to want to see it. We see our cluster running. We click on the SSH, we can connect into it. Now, it's using the credentials of my user. So I'm logged in here with a qwiklabs account. So it's passing that credential through and it's using keyed-based authentication because my user account under Google Cloud platform has the keys and we can - the system recognizes my public-private key pair that's being used for the SSH connection. So I didn't have to remember a password or anything like that. So here we are into the node. So let's go ahead and do Python --version. And sure enough, we got Python 279 in there. That's good. I'm going to just copy and paste these guys over here so we can see what we have. So we've got Java 1.8.0_131. We've got Spark version 2.0.2. We have - let's see, we've got Pig 0.16. And hive - I just need to give an extra return - there it goes, it's thinking. So we've got the standard set of what we need for running big data, the traditional tools. Okay. Now, the traditional tools for managing a Hadoop cluster are things like either the command line interface or a web interface. So this last part - or the next part of the lab - is what you did, is you had to make sure that you could actually allow the host that you're sitting on to connect to some specific urls. So better find out my IP address. You could use IP4.me or what I like to do is just go to Google and say, what is my IP? And Google does a really good job of telling us our IPs. So I'm gonna take that and I'm gonna copy that and then I'm going to go back to my Google Cloud platform and I'm going to go to networking. Now from networking, what I have to do is configure the software-based firewall to allow my connections. So I'm going to go over to firewall rules and by default, ICMP is allowed through; it's allowing communication between the various subnets that make up the main network for Google Cloud platform projects; RDPs, so if I had any Windows machines, I can connect in; and SSH. But there's no rules here for web or, in our case, we need very specific ports because 8088 500070 and 8080 are commonly used by Hadoop. So I'm going to go ahead and create a firewall rule. I'm going to go and give this a name, we'll call it our default-allow-dataproc-access. Don't need a description. The default network, we're going to just set normally. Now, the screen might have been a little bit different than what you saw because the interface is always evolving or you might be part of an AB testing where they're trying different features out within the interface. So I wanted to allow ingress. I wanted to allow the connection. And I'm going to go ahead and just say all instances on the network here instead of tagging each individual server. Now that's a little beyond the scope what tarags and all that means. Take a look at some of the CP 0200 videos on the operations of Google Cloud Platform, very virtual machine networking, that sort of stuff oriented. So I'm gonna say targets, all instances on the network. I want to allow a specific IP range, so I'm going to allow the IP address of where I'm located. No filter needed. And I'm going to go ahead and want to put in TCP 8080. I'm going to want TCP 50070 and I want to do TCP 8088. Oh, sorry, I got those backwards from what we had there. Let's go ahead and do a create. So that rule now says if I'm at this IP address, I'm allowed to connect to these ports on any host that's being exposed to the outside world. And we'll give it a second. Come on, Google. There we go. Okay. So that's my source; that's where I'm going to. Now, what we're going to do is go back to our dataproc cluster. And the dataproc cluster has - the master node is the public IP address that we want to access. We typically would not access the individual workers. So I'm going to go and click on my cluster and when I go over to VM instances, there's my master; there's my two workers. Oh, but I don't get the IP address from here. Hmm, that's annoying. Can I get it from here? No, we're not seeing the IP address there. From the overview, we're not seeing it; ah, but remember, it was a compute engine instance. So I should be able to go via compute engine. There's my compute engine interface and I've got my IP address and there's the external IP address of my cluster. So I'm going to grab that. I'm going to open a new tab in my browser and we're just gonna try to go HTTP to that address. Now this is going to fail because we didn't open the port to 80 and there's no web server running there; Hadoop is running on some non-standard port, so I'm going to stop my browser from trying to resolve that. I'm going to go to HTTP, put in that IP address and I'm going to try 8088. Now when I go to 8088, this is the main console for Hadoop if you don't want to use the command line interface. So what it did is fired up a simple web server and it's got information about my cluster itself. So that's what we're after here. We've connected into it; we see the user interface and we can explore the Hadoop cluster. Now, someone who's used to working with Hadoop is going to go ahead and know exactly what to do with this. If you're not used to Hadoop, well, that's beyond the scope of this course, but you'll find plenty of tutorials and other things out there on the web on how to actually do - you know, create new jobs and all that. We'll do some of that, so you can keep this interface, you know, handy if you need it. Now the other one we want to look at is port 50070 and that's the port for the Hadoop file system. And so this basically gives us an idea - so we can actually see some utilities, how is the disks all laid out and everything like that. The live nodes, the dead nodes, the distributed file system, the name node and the storage. And we can actually even browse it here. So here's our data nodes. So we've got two, they've each got 500 gigs worth of disk space. We've got any issues that are related to failures. We can snapshot. We've got startup processes. We've even got some utilities to let us browse what's actually on the Hadoop file system. And right now, there's not too much on there. Now, this - all the stuff you can do through these interfaces can be done from the local machine as well, inside the master node. You'd use the Hadoop command. So let me show you that really quick, just for fun. Let's - we should have our window here. So I'm going to do Hadoop and I'm going to do - I want to work with the file system, so I'm going to do hadoop -fs and I'm going to do -ls and I want to do slash. And what it's doing is - see? That's the same directory we're seeing here. So I'm going to go ahead and let's, you know, take a look. There's a couple of folders; there's a tmp folder here. And look at that, there's a Hadoop-yarn and a hive directory in there. So I go ahead and ask for - /tmp. I'm seeing the same thing. This is just a web interface of this. That's all you were doing. Okay. So the last part was to take a look at using the command line interface - so from Google Cloud shell. So not from the cluster node itself because - well, I wouldn't have had the cluster if I want a command line to create the cluster. So let's go back to compute engine and I'm going to click on the Google Cloud shell here. So when I open Cloud shell and start it up, I've now got the option where I could basically create a cluster using a command line. So I'm going to take this line here, I'm going to copy that to my clipboard and - soon as my Cloud shell is provisioned. So since I haven't used it yet in this project, it's provisioning me five gigs worth of storage under this user, as well as a virtual machine that kind of gets fired up - used. If I don't use it for a while, it'll destroy it and they'll create me a brand new one next time I connect. But I'm just going to go ahead and pass this, so I'm calling gCloud dataproc clusters. I want to create my second cluster in the zone, us-central1-a, that'll work. Master type is going to be an n1-standard-1 and the boot disk is going to be 50 gig. I want two workers; I want them to be n1-standard-1's and I want them to be 50 gigs worth of disk space as well. Now, kicking that off, it's going ahead and performing the operation. So let's go take a quick peek at the user interface here through the web console and see if we can see it performing some activities. Sure enough. Look, my second cluster is being built; it's provisioning it. And if I was to pop over and take a look at my compute engine; nothing just yet, but in just a sec, we should see our second cluster start to create some nodes. So it'll create a master and then the number of workers we need. All right. Let's look at the next couple of steps. They're just telling us - okay. Oh, just wait 'til it gets created. Now we just need to be patient, provided we're not seeing any errors yet. It just takes a few minutes for things to come online. And what we should see is see our second cluster with two workers there. Now, if you really wanted to, you could also go through and build a third cluster, you know? And here, you filled in the form but we didn't click go yet because basically, we wanted to see what the command line would actually look like for what we want to do. And then clean up at the end. You can delete any clusters that you created. Oh, come on, interface. I'm going to go ahead and say refresh this. And I suspect we're going to see some nodes here. Yes, some UI update. Yep. In fact, the nodes were creating. We just didn't happen to see them because the screen hadn't refreshed that particular part of the browser. Well, great. Looks like we have a dataproc cluster running that we created with the command line. And - oh, still coming online a little bit. I'm going to go ahead and choose the Wizard one more time and see - you know, so if I say this - my third cluster - and this time, I'm going to say well, I want to have four nodes. I'm still going to keep them with one, that way my project quota isn't exceeded. And then down here, notice how the equivalent rest or the equivalent command line, that's the command, just like we pasted in from the workbook. You don't have to, you know, keep this workbook handy and keep that line copy and pasted, you could always prototype it up with the interface and then just copy and paste what you're after. All right. So we've now created two, even three clusters and we're all set. So I will just clear up my clusters here 'cause we're going to create new ones in the next lab. And we'll go ahead and return to the lecture. 
