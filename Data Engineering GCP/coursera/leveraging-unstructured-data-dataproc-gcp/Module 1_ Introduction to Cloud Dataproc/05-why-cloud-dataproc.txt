So why Cloud Dataproc? Many users have been using Hadoop and the Hadoop file system as well as Pig, Hive and Spark for many years now running their own clusters and using them for processing large parallel jobs. So the question is, will you ever have a PetaByte of data? Well, let's look at some scale of that. It's a stack of floppy disks higher than 12 Empire State Buildings. It's 27 years of downloading data over 4G. It's a 100 US Libraries of Congress which maintain copies of all books published in the USA. And every tweet ever Twittered 50 times over. Well, it's probably closer to 49 times these days. It's been a few months since we probably produced this live. But a Petabyte has also two micrograms of DNA. One day's worth of videos uploaded into YouTube. And 200 servers logging 50 entries per second for three years or 600 servers making 50 entries per second for one year. So how do you process such large amounts of data? You can scale up by making a bigger machine but how much bigger can you make a single machine? Or you can scale out which lets you scale as large as the parallel nature of the problem you can handle by adding more computers to it. So let's say I have a job where I need to take 50,000 images that we have stored in a Google Cloud Storage bucket and I want to put, say, six different versions of that image, each with different dimensions, for say a web server, you know, clip art, that sort of thing. I want it at a bunch of different resolutions. Now we could try to make one big monolith or monolithic server to do this. So more CPU cores all tied up and, or make it faster in attempt to process more. But how big can we make it? We can only go so far. But what if we could take five hundred computers and have each one process a hundred images or 5,000 computers and each process 10 images or the absolute extreme, 50,000 computers each processing one image. Well, a lot of this was the problem that Google faced back in the 2000's because they had a massive parallel computing problem. How do they take copies of all the web pages on the internet and then break them down into the keywords so they can make up a search engine? They could build some large computer, buy expensive storage arrays and try to do it with expensive computing resources or they could invent their own parallel architecture to store and process this information. Well, which way did they choose? Well, they built their own parallel processing system and they called that map reduce as in we need to map the data to a model and then we need to reduce it. They also created a distributed file system called Google file system or GFS so that they could store all these files that make up web pages and not have to have one big system that holds those that could bottleneck. So in the mid-2000's after doing all of this stuff, using map reduce and Google file system, they released a couple of papers to the world that showed how they did it. And from that, we now have Apache Hadoop and the Hadoop file system, so thanks Google for that. Now the job of a map reduce is to split the data and map it to a structure and then reduce the structure to a usable result set, often putting it into a database or other structured format. So these days, many on-premise applications for big data are built using the open source stack based on Hadoop and the Hadoop file system. And then higher level applications which abstract the Hadoop HDFS portion of it, like Pig for data transformation, Hive for data querying on a large scale, and Spark a general purpose open source cluster computing framework. Spark is really popular these days and has database components, machine learning libraries etc. I'd recommend doing some research on spark.apache.org. If you've never heard a spark or haven't done too much with it. It's beyond the scope of our talk today. So I'm going to set up an environment in your mind to let you think about it for a minute, about running a Hadoop cluster. Imagine a 100 node Hadoop cluster. So that would have 100 workers that basically process the job. It's not uncommon to find a cluster this scale in a medium to large business, and then use it to run parallel computing jobs. Maybe it's a bank balancing their book each night. Deposits vs loans, ensuring they're meeting the regulatory requirements. Or maybe it's a large retailer comparing sales per store, there popular items stores, producing store item pairings, processing all the receipts from the last day, week, month, year etc. Or looking for new insight into their existing data. If they use their Hadoop cluster 24 hours a day, seven days a week. Then they probably need a bigger cluster. They need to add more nodes because who knows? They're basically bottlenecked. Now, if they only use their cluster 10 hours a day then they're wasting 14 hours a day of computing power. They should be taking that into account with our ongoing computing costs. It's like an airline seat that once the door closes, that seat's lost that there's no revenue that can be gained from that seat. Or a hotel room that goes empty for a night. Well, if wasn't used that night, that's lost revenue. And so basically if you're not using a cluster 100 percent of the time you're wasting computing resources and if you are using the cluster 24 hours a day, seven days a week you need a bigger cluster. Either way the on-premise Hadoop isn't going to operate as efficiently and hopefully as cost efficiently as utilizing Google Cloud Platform with Google Dataproc.
