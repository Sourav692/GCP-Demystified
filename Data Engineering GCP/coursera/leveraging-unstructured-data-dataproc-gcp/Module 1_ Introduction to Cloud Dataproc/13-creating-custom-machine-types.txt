So hopefully, you've enjoyed creating your first two dataproc clusters. The first one using the graphical interface and the second one using the gcloud command utility. Now, the last part of this video that we're going to talk about is controlling the size of the compute engine nodes that you're using. In our examples, we used one CPU and 3.75 gigs of RAM. And depending on the jobs that you're running, you may need to adjust either the amount of memory or the number of CPU's per node. The Cloud Dataproc hardware architecture is basically we're leveraging Google Cloud for the networking aspect. We're using Google Cloud IAM or Identity and Access Management for security. And then the Dataproc cluster is actually creating compute engine notes for the master, for the persistent workers, as well as the preemptable workers. It's going to be creating, within the cluster, the Hadoop File System. Not showing in this diagram, but should be, we should also see Google Cloud Storage in there. Because that should, ideally, be the place that we're bringing in our persistent data, and at the end we're storing our data to. Okay. So if I want to create a custom machine type where I want to define a specific amount of CPU, and a specific amount of memory. Then, what I use is I use instead of the N-one standard one, I use the term custom. And then there's a dash and then there's a number of CPUs, and then there's a dash and the amount of memory to do. Wow. So Google allows me to actually build the machine to my exact specifications. Yeah pretty much. Now there's a little bit of a caveat with that. In that you may not want to do that because if you're really close to one of the standard building blocks for Google Cloud platform it's going to be cheaper. And let me show you that here really quick. And I'm also going to show you how you can get the right value to pass to creating your machine type. So let's flip back to our cluster. And we're going to go to compute engine. To get this. So I'm going to go over through the menu stack and I'm going to go back up to compute engine. And I'm going to go ahead and create an instance. And when I go here, I see here is the machine type. One CPU 3.75 Of RAM. Oh, I've still got that preemptibles, it's remembered that from last time so let me turn that off. And that way we can also get an approximate pricing. So we say, "I don't want this to preemptible." So my worker node, assuming it's using a standard image. So this is giving me a price estimate If I was going to run my dataproc clause for an entire month would be $24.67 per month plus whatever network egress I would have. So, one CPU and if I increase this to two CPUs, we see here's my option two CPUs 7.5 Gig of RAM. And I see my option for four CPU and 15 gigs of RAM. And so on all the way up. Here we see the N one high memory which is two with 13 gigs of RAM. Double what we have. And then if we scroll down even further we can see here are the high CPUs which lower the memory per CPU. But, Google's also got an option here for us to customize. So I can go in here and say well I would really like my worker nodes to have 14 gigs of RAM. So I want four CPUs with 14 gigs. But notice the warning down the bottom it says, you know what? You can save two dollars and fifty two cents per month by going with a straight N one standard four which is actually going to give me 15 gigs of RAM. And that's because the standard blocks have a lower price than when you customize because basically there's a calculator that Google uses. The number of cores times the core price plus the amount of memory times the memory price. And when you're right around a standard building block it's definitely going to be cheaper to go for the standard one. But if I needed something like 10 gigs of RAM with that. Then there's definitely an option for that. So four CPUs 10 gigs of RAM. Now, how do I figure out what exactly is that for the parameter to pass to GCloud create? Well, I'm glad you asked. We're going to scroll down the bottom. And what I can do is, I can ask Google to show me the command line. Now, when they show me the command line for this, this is the command line that would create this compute engine instance. But we're going to use it to pick up the identifier for that custom build we just set. And in fact if we look right here. Custom four dash 10240. So it's a custom VM rather than a standard, it's got four CPUs and I want 10240 memory. And that's what we would plug in to this value back here when we do Gcloud data proc cluster create. We could plug in custom four 10240. Giving me what I need. So, that's what this slides kind of showing you. I just like to show it to you live sometimes. It's more fun for me too. But basically, we went through just creating compute engine instance. We chose the customize option, we specified what we want and then we could steal it out from either the command line or we could have also pulled it from the restful call as well because it's in the blob of JSON data that would be submitted that way. Okay. Well that concludes our video on our first section of Google Cloud DataProc. This is Grant Moyle on behalf of Google Cloud platform. 
