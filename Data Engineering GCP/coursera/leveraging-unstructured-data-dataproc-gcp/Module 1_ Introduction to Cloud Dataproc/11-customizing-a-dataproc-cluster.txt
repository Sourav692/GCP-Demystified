
So when creating a Dataproc cluster, there are some additional options that can be selected. So let's jump over and I'll pull up the wizard for creating another cluster and I'll discuss these few extra entries for you while we see it in Google Cloud Platform. So, I'm going to go back to Clusters and I'm going to say Create another cluster. So there was the name and the zone, and the master nodes, and the cluster node that we talked about. There was the disk, here's the worker nodes. And when I click this drop down here, there's the preemptable node option. There's a cloud storage staging bucket, which it'll automatically create if you don't choose one, it'll just create a unique identifier for it. Here's our networking, so we can decide what networks are we actually going to put it on. Now networks are beyond the scope of this video but there's plenty of resources up within Google Cloud Platform to discuss the network configuration. A default project only has one network, so this is connected on the network in the particular zone we're at. What I'm really excited to tell you about is this image version option. So if we drop this down, we actually see that Google is offering at current time five different images. A 0.1, 0.2, 1.0, 1.1 and a preview. So I'm going to click on the question mark here, and click on Learn More.

So it's going to tell me these are the Cloud Dataproc Version List. So what version of software will be on the Dataproc cluster when I create it? So the default is obviously the latest version, version 1.1, and that's going to have Apache Spark 2.02, Apache Hadoop 2.73, Apache Pig 0.16, which is pretty much the latest version, and Apache Hive 2.11. It's also going to have the connectors for Google Cloud Storage so that Hadoop can talk to Google Cloud Storage. And remember, that's going to be one of our eventual goals, is to not leverage HDFS because that makes our clusters not stateless anymore and instead begin using cloud storage for the importing and exporting of our data, our persistent data. And then also a big query connector for Hadoop so that it can directly call big query. So, if 1.1's got the latest versions of that, and in fact in this case, it was last updated in April of 2017. Now version 1.0 has slightly older versions of Spark and Pig and Hive. Now, there are also the 0.1 and the 0.2 version which at this point are unsupported because they go way back, and they've got really older versions of the tools. And then at the very bottom, we see that there's actually a preview version. And this has the latest version of Spark, the latest version of Hadoop and also current versions of all the other tools. So, that's what we're actually choosing when we're building our cluster

using the graphical interface. And we can also pass those as parameters during creation. Now the only other option there we see is initialization action. And we're going to discuss that in a later video in this series on Dataproc, is just basically a script we want to have run on the workers and the master nodes when they boot up. So that way if we need any custom installation to occur. And then the project access. And this is basically just like when you create a compute node, what automatic access is the software running on the nodes going to be able to leverage? So the worker nodes, are they going to have API access to all of Google Cloud services that are present in the project?

Now, just like everything within Google Cloud Platform, most of the time, the things you can do in the web console can also be done from the gcloud command or the gcloud SDK. And they can also often be integrated in with custom software because we can call the restful services directly. So there's plenty of ways to build and manage your Dataproc clusters

So what does it look like if I was to use the gcloud SDK command? Well, I want gcloud, I want dataproc, and I want clusters, and I want to create, and it's going to be called my-second-cluster. I'm going to put it in the zone us-central1-a, I want the master machine type to be an n1-standard-1, so one CPU with a standard ratio of memory which is 3.75 gigs. The master boot disk size is going to be 50 gigs. I want two workers. I want those workers to be an n1-standard-1, so they're also going to be one CPU with 3.75 gigs of RAM, and their boot disk is going to be 50 gigs in size. Now, are there other options there? Yeah, we don't have an option for preemptible instances, so the default's going to be used, which is none. If we wanted that, we just have to know the appropriate --option for it. Now, how do you get that? Well, you can always run gcloud dataproc --help to get details on that level. gcloud dataproc clusters --help to get details on that. And if you want the really specific commands on the creation command, you just do that gcloud dataproc clusters create --help. And that'll give you all the documentation. And if that's not enough, head up to Google Cloud Platform's documentation on the website, or like I do, search gcp dataproc, and you'll come up with the entire documentation set on Dataproc.

So I'm now gotta turn you lose on the lab, and we're going to have you create a couple of Dataproc clusters. The first one you're going to do is through the web console, and then we're going to have you SSH into it and just verify it looks good. Now, Hadoop does offer web page management. And so, by default you wouldn't be able to get those webpages just from your on-premise workstation because the firewall is going to be blocking it. They're running on non-standard ports, so like port 8080, 8088 and 50070. So we're going to have you make a firewall rule change that allows your browser on your laptop or your desktop that you're using to do this, to actually connect into the resources of that Dataproc cluster. And then last but not least, we're going to have you create, manage, and delete a Dataproc cluster from the command line interface.

So, go ahead and pause the video and go tackle Leveraging Unstructured Data Part 1, the lab to create your first and your second Dataproc cluster.
