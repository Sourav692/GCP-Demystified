Now, when you actually build your Dataproc cluster, you're given the option of three different cluster configurations. The first one is a single node for everything. And that's what's called "pseudo mode" in traditional Hadoop structures. Now, that's just for experimentation, that's for working with really small datasets and everything happens on one machine. The more common one is the standard which uses one master and then two or more worker nodes. And then the third is a high availability which actually leverages and/or creates three masters plus all the workers. So that way if a master fails, you've still got two other masters that have the data, and they can keep the cluster operation. You haven't lost everything. If you go with a standard, with one master only and something happens to the master basically anything that wasn't stored outside of that cluster is going to be lost. Now, we can still use the Hadoop file system. You know, in fact when we were provisioning our disks were telling it "OK. How big do you want the Hadoop file system to be? So if I choose for worker nodes and I give each one 500 gigabytes, well, presumably that's about two terabytes worth of disk space. But we have to take into account the replication scale so we probably have something more like 600-700 gigabytes of space. Because the default replication scale on the Hadoop file system I believe is three copies. Now, we've got a note here saying "Don't use Hadoop file system". Well, why not? Well, that's a great step bringing your on-premise. Up to dataproc. But the problem is any persistent data that's stored in the Hadoop file system so that's where you're loading your input files or you're collecting your output files from. It's going to prohibit our ability to literally create the cluster, use it and then destroy it. We're going to have them directly manage the persistent data we store on the Hadoop file system. The better answer, and we're going to look at this in a later video, is to actually change our Hadoop jobs to directly use Cloud Storage instead of the Hadoop file system. Because it's going to give us the capabilities to do our persistent data to bring it in from Google Cloud Storage, run on the cluster and then store data back out. And so, the cluster itself remained stateless. We don't have to preserve the state of the cluster between each job. One of the really powerful capabilities of Google Cloud platform is the ability to use preemptible worker nodes. So what are preemptible worker nodes? Well, these are basically nodes that Google offers at a significant discount of around 20 to 30 percent of the full price. Now, preemptible nodes are not unique to Dataproc. You can actually go through compute engine and ask for a preemptible node. And if Google has availability on the computing resources in that zone, they will go ahead and give it to you. It's sort of like buying a last minute plane ticket when the airline has a bunch of seats open, or a last minute hotel room when the hotel room was going to go empty. You're sort of in the driver's seat on that pricing, you know, because hey, it's going to be lost revenue if they don't use it. So rather than have some sort of weird auction or other sort of bidding war, Google just basically has a flat discount that they offer on it based on purely the computing charges. And it amounts to basically 30 cents on the dollar. Now, I'm going to take a minute and I'm going to go look at compute engine to show you a little bit of details about preemptibles. And then I'm going to adjust the cluster to use them. So let's do a quick demo here because we can only explain so much. So I'm going to shift over to my Dataproc cluster. And here's my Dataproc cluster but I'm going to go over to compute engine. First. So when I go to compute engine. And I'm going to say create a brand new computer instance. So here I'm just asking for instance one. It's going to run a Debian Linux. And we see over here that the cost for one CPU and 3.75 Ghz of RAM in the U.S. one central or U.S. Central one C is 24.67 per month. Now, this is not a premium image, this is a free operating system image. So there's no OS charge for that. And so, when I scroll down the bottom I see there's a dropdown that says "management", "disks", "networking" and "SSH keys". So if I click on "management" and scroll down a little bit, we see there's an option for preemptability. So I'm going to turn that on. And look what happens to the price. It's now down to 7.70 per month. So a significant cost over the $25 we saw before. Now, the idea is though you're using computing resources that Google has unallocated at that time. Now, what if somebody comes along and suddenly needs that computing power and Google could get full price for it? Well, they will preempt you. Now, preemptability basically says they send you a soft shut down and you have 30 seconds to finish off whatever you're working on, and then the system will be shut down. Now, another thing is you can only run it for 24 hours. That's the most they let a preemptible instance run for before they'll shut it down. Well, that could be, you know, not ideal for running something like a Web server or things like that. But when it comes to Dataproc that is absolutely perfect because while Hadoop is built to handle the increase and decrease of nodes. Nodes can join the cluster and they can also be taken out of the cluster as needed. So let's now go over to our Dataproc cluster. And let's see where the options for us to add preemptible nodes are. So here's my cluster. My cluster. I'm going to go over to configuration and we see that it says "We don't have any preemptible worker nodes right now". Well, we can solve that. Let's edit that and I'm going to ask for for preemptible worker nodes. Now, I don't get to choose specific type of compute because they're going to match the same number of CPUs and memory as my worker nodes. And also, they're not going to have any primary disk assigned to them because they can't be part of the Hadoop file system. And that's normally or that's primarily what the disk is for. Why not? Well, since the preemptible could be taken away at any time, we don't want it to hold any data, even if it's temporary, as part of the cluster. So let's go ahead and click "save". And let's take a look at the VM instances. So over here the clusters we've submitted the command and in just a minute we should see our Dataproc. If they're available, we'll go ahead and create us four more preemptible workers. Oh, OK. There they are. We had a little yellow warning there but now they're coming up. So I've now got the capability to add these preemptibles, and I can also define them when I create my cluster. And basically, they're going to get "let me get my job done faster because I've got more workers". Plus, they're going to let me run it for lower cost because I'm going to pay less for those workers. So let's say I have a job I need to run and it takes two hours If I use four nodes. OK. That's sort of the timeline. And my deadline is I have to produce the results in two hours. So I needed to find my cluster with four worker nodes right off the bat because there's no guarantee I can get preemptibles. And so, I based it on one master node and two workers. And I don't get any preemptibles. I can't get my job done in the two hours. But based on what I've, you know, calculated things to be one master, four nodes gets my job done in two hours. And in this case I asked for four more workers so presumably we can get those for preemptible workers. We can now get the job done in one hour. Now, what's my cost savings to this? Well, I only had to run the main workers for an hour so half the price for the primary workers. And I got 30 cents on the dollar for the preemptibles. So I've got my job being done faster, assuming those preemtibles come online, plus I'm saving money. Now, can we take this to another extreme? You bet. If you need to run a job and you don't have a tight deadline for when it needs to be done, you just need it to be processed. There's nothing to say that you couldn't build one master, two regular workers because that's the minimum number we need. And then I could build 10 or even 100 preemptible workers. And as preemptible workers can come online, they'll process. Now, a little word of advice on preemptibles. The larger you ask for the machine to be, the less likely you're going to get preemptible workers. Or the more likely they are going to be taken away once they're up and running. Why is that? Well, if you asked for a 16 or a 32 node preemptible worker, that's over half of a physical machine in Google's data center. And more often than not somebody is going to come around who wants a big machine and they're going to need to preempt you to offer them that capability. But if you ask for smaller workers so your job like my image processing where I wanted to process 50,000 images in, you know, and create six different sizes, a bunch of preemptible workers would be fabulous because it can run in parallel and process more and more of my results. So I really can design around preemptible workers when time is not a factor. And when time is a factor what I have to do is calculate the minimum number of workers necessary to get my job done in the timeline, and then supplement that with preemptible workers. All right. So let's shift back to our side deck here. So we we basically we can get our jobs done faster and for less cost. And during set up is just under the Advanced Options, you just say how many nodes you want. You don't have to do anything special to that preemptibles, they're going to match the same configuration as the worker nodes. 
