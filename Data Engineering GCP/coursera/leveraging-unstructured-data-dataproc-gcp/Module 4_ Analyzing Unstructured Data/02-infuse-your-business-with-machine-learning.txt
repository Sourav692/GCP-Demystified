
Now Google Cloud Platform has lots of options for machine learning, starting with APIs that do what you need. There's no need to train the system, just pass them an image, or sub text, or audio, and they'll come back with the response with the data processed for you in a structured form. For those organizations that need more than the prebuilt models, there is TensorFlow, which is an open source for seeing learning library, and Google's Machine Learning Engine and Cloud ML, and that whole packaging there. Those are going to be covered in a different video. What we're going to focus on in this one is the APIs that use machine learning. So let's start with something simple we can easily understand. The Google Translate API. So I'm going to bring up Google Cloud Platform here.

So I have a project open and I'm going to go up to the menu. Then I'm going to click on or make sure my menu's enabled, then I'm going to click on API Manager and I'm going to select Library. So this is a library of all the API's that Google makes available for Google Cloud platform projects. Now you'll notice a section right here called Google Cloud Machine Learning, looks like a computer brain there. And they have the Vision API, the natural language, the speech, the translation, and then they also expose the Machine Learning Engine APIs, and that's where you do your own models. But we're going to begin by taking a look at the translation API. So the Google Translation API looks like it's already enabled in this project, because up there's an option to disable it. And I haven't used it with this particular project, so there's no data about its usage and things like that. So what I am going to do is over here on the right I am going to click on Try this API and API explorers. Now, so Google's APIs are exposed via RESTful. So basically what happens is the web application or your internal application or whatever makes an https request to Google endpoint. They submit the data and then they receive back a block of data, much like you use a website except it's computer to computer communication. And they're going to pass back blocks of data rather than pretty pictures and graphics and all that sort of stuff. Okay, so we see that this actually, this API has five different functions available to it. And it actually has language detection to detect the text within your request, and there's two variations on that. There's two variations on translating text. And then one to sort of show you the list or to allow you to query for the list of language pairs that can be translated. Well, we're going to take a look at the one called Language Translation List, so I'm going to select on that.

Now the two parameters in red are the only ones that are required. So let's try a common phrase we might want to translate it into another language, where is the bathroom?

And we're going to translate that into Spanish. Now there is several other parameters that you could pass here, but only the Input Text to Translate and the Target Language are required. So I'm going to go ahead and say Authorize and Execute. Now, I need to basically give Google the okay to execute this, so I'm just going to say Authorize and Execute because we're in the testing or the training area of Google Translate and the APIs. So I'm just going to choose my account here. We'll allow it, and there we go. Now what happened was we sent this request. We did a get Https://translation.googleapis.com/langua- ge/translate/v2. So, that's the URL it went to and submitted the parameters q equals where is the bathroom and target equals es. Now, if we were using this in production, we would also have to provide an API key so Google knew to build this back to our project. Now that seems simple enough, it's a request and down here is the response we received back. Donde esta el bano? Wow, that would definitely get me find me the nearest restroom. Let's try another language, let's put in French. So I'll go and execute that. We sent the value this time we asked for the target equals FR and back came, okay, my French is pretty bad. But I suspect that's going to find us the nearest restroom. Well, let's see, can we do German?

There we go, looks like German, I think. And let's throw, just for fun, let's put some Hindi in here. That's definitely going to find me the restroom! Sorry, my Hindi is not that good, but my colleagues who do speak and read Hindi, they tell me, yup, Grant, that would get you close enough. So we're in good shape and we see how we use this Translate API. Now this isn't the only way we use it. This one be very much an application that basically needs to assemble a URL, submit it to the translation.googleapi.com website as a web type service or a RESTful service, and gets back a block of data along that same pathway. It's very firewall friendy, it's just an https request, and presto, it does what we need. Now this is going to be one of the easy ones. And that's why I started with this because we can handle text into translation. But what if we basically needed to start playing with something like vision? Well, the Vision API, that's going to be a little trickier because we gotta somehow pass it a graphic. Now, if we pass it a graphic, Stop, let's step back for a sec. [APPLAUSE] Now let's talk about using something like the Vision API. It's going to be a bit trickier, but the concepts are the same. Scrap that.

Now let's talk about using something like the Vision API. It's going to be a bit trickier, but the concepts are the same. In the case of Vision, we're going to upload the image to Google Cloud Storage bucket, and then submit our request pointing to that image. Presto, Google will give us the data. Well, what sort of data? Well, let's find out. I'm not actually going to use the API explorer, although if we go back and take a look at the various APIs we could leverage, let's go look at Google Cloud Vision API. And we see they only expose one RESTful service called for this. So we're going to click on this and they want us to basically pass in some fields in the request body. Yeah, that's going to need one of my developers to work with. But trust me, if you give that to one of your developers and tell them the appropriate parameters to pass, they're going to be able to work with it. Now, what I'm going to use instead, is I'm going to go up and I'm going to search for GCP Vision API.

Now I always, whenever I'm searching for something on Google Cloud Platform, I always put GCP in front. That way I know I'm getting pretty close to the top of the list. And in fact, look, the first hit coming back is the Vision API.

Now, so the Vision API, this is the website to basically tell us how it works and etc, but they actually give us an interactive area here that we actually drag some icons for. So I've got Try the API. Let me slide this out of the way a little bit. I'm going to minimize this, and looks like another page, and I got some graphics here. So I've got both my kids. So my daughter, for Halloween, was a zombie. So let's see how Google does with her zombie picture. Yeah, didn't my wife do a great job on the makeup? Well, we actually confused Google a little bit here. Because what we did, is, when it went to do face detection, it was able to determine the eyes and the nose and all that sort of stuff, so we got good coordinates, and that's data being returned. But we didn't get any joy or sorrow, it can't tell any good emotion from this. Okay, well that is fine. Let's look at labels. Wow, Google can tell us a lot about this image. They know there's hair and they're giving that a 96%. They know there's a face, they know there is a person, there's a cheek, there's a nose, they know it's a girl. Wow, they actually figured out there was some beauty in there. Well, my wife didn't do a good enough job on the makeup then because zombies I don't think are supposed to be beautiful, but my daughter is, so we'll we'll call it that. And we'll say that Google figured out that it wasn't real.

They found a smile, okay, they found head and blonde and portrait and mouth. Hey, look at this. They came out with a 65% that's it's likely to be a child. Well that's really interesting, because how does it know? Well, that's part of machine learning. It's seen enough pictures of children and it's seen enough pictures of adults or not children, and it basically has a model. Each one of these has a machine learning model behind it to detect. And when you upload a photo into this, it's running that process through looking for all the values that basically meet a certain threshold, to categorize or classify this. Okay, so let's look at one more, can't just use one of my kids, let's use another one. So here's my son's Halloween picture. He went as Captain America, and it found joy in it. It found his smile and from all the properties they see, he got a very likely that he was happy or joyful. Okay, so what labels did it find? Well here it found a face, male. Wow, how did it know he was a boy? There's not a whole lot there describing that. But okay, figured out it was a toddler. Well, he was four in the picture. He's got a smile, it's a child. Organ? Not sure what that is, but maybe it's picking up on something in the background. It looks like a little skull cup or something. Eating? Okay, it's picking up some of the stuff on the counter behind. And it found eyes. Now, that's not all. Google, if we take a look at web, they've kind of started to grab a bunch of other things here. Wow, I have no idea where they came up with some of these things. They came up with YouTube and Father and Toddler and Violin.

Got some really interesting web entities. It basically looked for pages with matched images. Wow, I'm going to have to check some of those out. So these are all sites where I think it found a match to that image.

And if we go to Text, they found OO. They found the eyes in the glass pack here, okay. The Document, they sort of have broken it down into any text they did find. So yes, you can use the Vision API for text processing. We look at the properties, it figured out the dominant colors and where we're likely to want to crop things to best get. And then the really cool piece, is a Safe Search. So it started to say it's unlikely, but we actually scored two bars on the Adult and two bars on the Spoof. Now, three or four would be a pretty good sign that we're looking at adult content, or spoofed content, or medical content, or violence content. So when we look at this, actually five here. Possible, likely and very likely, sorry. Now the last part is to look at the JSON data. So here is a block of data that's usable for our programmers. So when they submit an image they can come back with all the blocks of data and then utilize that information. So we've taken unstructured data in the form of a photograph and turned it into some form of structure. So we're looking at my kids here. We we're looking at them with a Vision API, so let's talk about kids and web usage at school. Those of you who have kids in elementary or middle school, do you worry about what they're surfing on on the web at school? I sure do. So what if we could have a filter in place that uses Google's Cloud Vision API to look at the safe sender information? So here's a possible workflow in an application. So when a child opens a website it goes to the proxy server and the proxy server then requests the image from the website. If it's never seen that image before it submits the image to Google Cloud Platform's Vision API and comes back and looks at the safe search flags.

If it flags either a possible, a Likely or Very Likely, or some specific threshold, maybe we do Likely or Very Likely as our flags, it's going to go ahead and block that image if it's got adult or violence or medical content in it. Now, that's pretty cool, but what, we don't want to have to do this with every single image. Well, if it downloads an image it has already seen, it's a proxy server after all, we could basically take the image that's downloaded, run a hash on it. So maybe we use a 256-bit hashing function, and we'll store information into a database. So if it gets an image it's seen before, the 256-bit hash will match and we look at the role in the database, it says we've already categorized this one as violent. If it hasn't seen it before, it performs a Vision API check on the image, comes back with the safe search data. Creates a hash of the image and stores that information into a database that can drive the website. Well that's pretty cool, and that doesn't sound like a massive undertaking as far as software development goes. Because we're not trying to program all of the, we're not programming all of the pieces in the middle, we're leveraging the APIs to do what we need it to do. All right, so, we didn't have to build the model, Google already did that. And that's the key about the machine-learning based APIs. Google has basically run the millions upon millions of images through Google's machine-learning algorithms to basically come up with all of our labels and categorizations and things like that. So what would the code look like to actually do Vision API? So here we see that machine learning APIs are accessed through that RESTful API. So we don't ourselves have to know much about machine learning, although it's helpful. We see here that the image is stored in Google cloud storage, gs://cloud-training-demos/vision/sign2.jpg. That's this image right up here that we're seeing, that sign right there. Now, we're going ahead in our, we're just going to submit that and we're going to ask for Features, we're asking for text detection, and it's going to go ahead provide back the results. So we submit the image by putting in a cloud storage and then calling the API. So we're going to call it Envision API version one with a developer key. And so that way they know to bill it to us, we get back a JSON response, and presto. We have now used the machine learning APIs Now, what does this let us do? Well, unstructured data is everywhere, images, audio, video, free form text. We put them through a machine learning API call. And we come out with structured data in the form of places or labels or people or events or texts, etc. And it sure makes the data a lot easier to use. The speech API is just as easy to use as the Cloud Vision API or the translation API. Speech can be submitted either synchronicity, so basically you pass it a file and you get back the text, or asynchronicity which can either be used in an interactive system where as it receives the data, it provides you back the translated text. There's actually a great demo up on Google Speech API site, where you can flip on your microphone and talk, and Google in near real time will translate that back as to what you're saying. So take the speech and turn it into text. Now, taking speech and turning it into text is an amazing thing because there's so many possibilities for it, such as, well, api.ai. Api,ai basically has a conversational user experience platform for the creation of interactive bots of voice driven systems. You say something and the system will then perform the task that it identifies you want to do. Now, often this stuff is used on websites, but more often than not it's utilized on interactive phone-type platforms. And so, we're so used to interactive menus. Please say the name of your frequent flyer account. Please say what city you'd like to go to, etc., so we have those already. But this just allows for the much more rapid creation of those, both in an audio world and also in a web world, where it could be at fully interactive via website.

Now, some other use of Various APIs, is Google actually leverages the vision API. So for conference rooms, they have something called Meeting Nanny and in monitors of a conference room is actually in use. Now, if you schedule a conference room at Google, and then you don't use that conference room, the meeting organizer basically gets a follow-up saying, yay, you booked a conference room, but you didn't use it. If it happens frequently, like, let's say you scheduled a reoccurring meeting, invited the conference room but you never used it, or you rarely used it, that meeting room might get uninvited from your meeting. You might say that if they just use motion detection, because it should be something you could use for something like this, well, it is not really machine learning. And you're probably right. Because motion detection is just taking one image and another and seeing what changed. But what if we incorporate facial recognition in here? If we put facial recognition in the picture and basically identify is it actually a person using that conference room, well, now we're definitely got a machine learning enabled application. Now, in the particular case of Meeting Nanny, Google basically will send a pub/sub notification between six and eight minutes after a meeting starts with either, is there people present in the room, or if they're not, so they can count the room as being occupied. And then, they use that to follow up on the information, and usage of the conference room. Another, Ocado, they use natural language processing to route customer service emails better. Now, never heard of Ocado? Well, they're the world's largest online grocery supermarket, and they've got a goal of being one of the best in customer service. Being able to identify the type of comment that somebody submits in an email or a website, or take us in the form of feedback or refund or payment issues or redelivery requests. It's important to getting it to the correct department right away and acting on it much faster. So, there's a great use of the natural language processing, and that's what that NLP acronym is there. Now, Wootric collects both numeric data and qualitative data from feedback surveys, etc. Well, we know computers can handle numeric or quantitative values pretty easily. But what about qualitative where somebody many typed in free form text? Computers were never great at that. But with natural language, API, we can basically solve the problem by taking the free form text, turning it into text but not only that, actually getting the gist of what the person is asking for, or what the person's feedback was in particular. So we can take that and get some structure out of it, and not just the form of the text, but in form of sentiment and so on. So Wootric uses the NLP APIs and custom machine learning models to get sentiment, to get the topic and possibly, even the support personnel because if there's names mentioned in the text, well, we know who they're talking about in the particular customer service representative or others. And we can associate that feedback with that particular rep or maybe it's common on a delivery person, and he said, "you are my delivery person", and they stated the person's name. Well, we can gather that information as well.
