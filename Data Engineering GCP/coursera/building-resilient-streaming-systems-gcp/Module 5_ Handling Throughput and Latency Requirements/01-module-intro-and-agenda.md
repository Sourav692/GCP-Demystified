So far in this course, we have looked at streaming data, how to build resilient streaming pipelines, we looked at how to create variable in variable volume ingest, we looked at how to process data that could be late or unordered using Dataflow, and then we looked at how to do queries and data even as it's streaming in using BigQuery, and displaying that data with data studio. But what we haven't yet looked at is other options as far as the sync is concerned. BigQuery is a very good general-purpose solution, something that would work in most cases that you're worried about, but every once in a while you will come across a situation where the latency of BigQuery is going to be problematic. In BigQuery, the data that's streaming in is available in a matter of seconds and sometimes you will want lower latency than that. You'll want your information to be available in a matter of milliseconds, for example, or microseconds. You may also have latency issues where the throughput of BigQuery, which is about 100,000 records a second, may not be enough, and you may want to deal with higher throughput. And so, what we're going to be looking at in this final chapter is how to handle such throughput and latency requirements. When BigQuery is not enough, where do you go? We will talk about Cloud Spanner and we'll talk about Bigtable. These are going to be two of our options that we could consider, and then we'll spend a lot of time looking at Bigtable. We'll look at how to design for Bigtable. Specifically, how to design schemas, how to design the row key for Bigtable. We'll look at how to ingest data into a Bigtable. We'll do a lab that essentially takes Dataflow pipeline and that is currently streaming into BigQuery, and modifies it so that it is streaming the average speeds into BigQuery but it's streaming the current conditions which is 30 times more data. The current conditions, we will stream it into Bigtable. And then finally, we'll look at some performance considerations. 
