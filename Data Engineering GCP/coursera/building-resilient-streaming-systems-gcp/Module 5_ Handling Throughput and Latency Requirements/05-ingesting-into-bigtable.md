## Ingesting into Bigtable
So now that we've looked at the design of Bigtable let's look at how to ingest data into Bigtable. In order to ingest data into Bigtable the first thing is we need to create a Bigtable cluster. And the way you create a Bigtable cluster is that you can go to the Web user interface and you can create a cluster. You can use gcloud to create the cluster. So in this case we're just showing you gcloud. So gcloud Bigtable instances create, create the instance and then basically specify what your instance type is. So again, here we're going to be specifying what your workers are. Do you want N one, no, eight CPU, how much memory, etc. and then you can basically provide your cluster storage type, right? That by default it's SSD disks because they are faster but you can choose. But once you have your cluster you can always add nodes, you can remove nodes and you can do this without any downtime. So you have a cluster you've created with three nodes and you get to add two more nodes and the reason you can do this is that the cluster nodes themselves don't hold the data, they only hold metadata. So all that happens when you add new clusters is that some metadata gets copied over. It's cheap. So you can add nodes, you can remove nodes without having any downtime. Once you have your big table instance you can create your table and this I'm some showing to you in Python. You use a Bigtable client and you say get me the instance and then create a table and then, you know, and then you start working with a table. More commonly though, rather than using Python as we are doing here where we are basically creating a table and specifying the column IDs, specifying the value and committing it, more commonly you would do this not from Python but from data flow. So if you're using data flow then you would essentially be using BigtableIO in order to commit to it. So if you want to stream data into Bigtable using data flow you would basically get or create the table and remember that in BigQuery what we did was that we took our data and we converted them into table rows and then we streamed those table rows into the query. In the case of Bigtable what you are going to convert your object into is is that you're going to convert them into these things called mutations, the changes that you're going to make and then you're going to write that list of mutations to Bigtable. So to create the table essentially you're going to authenticate against GCP, right? Having authenticate against GCP you will go ahead and you'll create your table, okay? So this [inaudible] creates a table and then you can go ahead and create a mutation. So in this case we are basically creating a mutation where we're setting the value of the latitude to be some latitude value. And then we're basically we going to do this for every column that we're going to write. We will create a row key and the row keys in this case are highway, the direction, the lane number, the sensor key and the reverse time stamp. Notice how I'm doing the reverse time stamp. I'm basically taking the actual time stamp and I'm subtracting it from the maximum possible value of a long. So I'm getting the number of seconds since 1970, number of milliseconds since 1970 and subtracting it from the maximum value of a long and that gives you a reverse time stamp. I'm making that my key. So highway direction, lane sensor ID and the reverse time stamp. And then I'm creating a list of mutations and for every value that I need to write out, latitude, longitude, center, etc. I'm basically adding a mutation and then I'm basically writing the key and the list of mutations. So that's basically my mutation object and then once I have my mutation object I basically do a BigtableIO.write to this table. To read from Bigtable, again, you can do it very programmatically using the HBase API, you can use the Hbase client. You can even use Bigquery to query off Bigtable.
## lab-overview
So, how do you stream it to Bigtable? So, in this lab, what we're going to do is that we're going to complete this pipeline that we created. So in our pipeline, what we're going to do is we're going to basically take all the accidents that we've found, send them to pop sub, take the average speeds, write them to BigQuery, but take the current conditions data and write them into Bigtable. So again, this current conditions is 30 times the output of the average data. So there's more data, Bigtable's probably a better place for high throughput sensor data. So that's what we're going to do. We're going to write the raw data into Bigtable, the aggregate data into BigQuery and information that the rest of the system might need to be alerted about, we're going to write that into a pop sub. So that we can use it to launch things like Cloud functions and so on and send alerts et cetera. 
