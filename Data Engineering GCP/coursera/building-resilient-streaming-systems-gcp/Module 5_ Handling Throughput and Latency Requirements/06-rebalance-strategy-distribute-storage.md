## Performance considerations
So let's finish off this course by talking about some performance considerations when using Bigtable. So the thing to realize is that Bigtable essentially configures itself. There's been a lot of engineering effort that's been carried out at the Bigtable in the many years that it's been used at Google so that it can react automatically to the way in which you are using it. But the key thing is, that Bigtable learns over time and in order to do that you have to leave Bigtable up and running in a typical scenario, in a typical way that you will use it. So the best way to figure out how Bigtable is going to perform is to have it actually work on the load that you're performing because the way it performs in the first few seconds is not the way that it's going to keep performing. So there is an amount of learning that Bigtable does. So Bigtable essentially looks at access patterns, the way that you're accessing it, figures out how many nodes it needs to keep running, figures out where the tablets need to be, how the tablets need to be split up, which machines need to process what data? All of these kinds of things, it learns and looks at the access pattern and reconfigures itself so it learns this access patterns, figures out of that, "Oh OK. So there's processing being carried out. First node is basically getting called a lot. The other two nodes are not actually getting called as much." And it essentially goes ahead and rebalances the data, moves the metadata for B and C over to these other two nodes so that the reads and writes get better distributed. So we did talk about design of the row key in such a way as to distribute reads and writes. That's still important. You still are want to do it but you should realize that to some extent it doesn't need to be perfect and you don't need to be perfectly balanced to a large extent. Bigtable will figure out and will take care of some amount of hotspots. Not all of it. So you do want to design your row key well but then you can also as a second backup rely on the rebalancing within Bigtable to take care of some of the artifacts that happen in the real world. So the way that Bigtable does is the first rebalance strategy is that it will redistribute the reads. So if it finds that more than 25 percent of the reads are happening on a single overloaded node it's basically going to go ahead and try to make those reads better distributed so that other nodes also get to share that pain. 

## Rebalance strategy: distribute storage

Also, part of the rebalancing strategy of Bigtable is that the storage itself will also get redistributed. So remember that we talked about the nodes don't actually store the data, they only store the metadata. The data itself is still stored in cloud storage. So this kind of redistribution of the data is not as expensive as you would think. Bigtable is not really copying data around, it's only copying pointers to the data.
0:31
Under typical workloads, Bigtable is going to give you a very predictable performance. The whole idea here is predictability, so you're going to get very consistent, very standard performance. So something that you should kind of expect is that you should expect about 10,000 queries per second at about a 6 millisecond latency. Read and write, if you're using SSD discs. And 10,000 QPS write at about a 50 millisecond latency. So larger latency if you are writing to an HDD, a hard disk drive. And reads would be about 500 queries per second at 200 millisecond latency. So again, now read write is going to be even slower. And so SSDs are obviously going to be much faster than HDDs. But this is the kind of read and write performance that you should be able to expect.
1:31
So how do you improve the performance of Bigtable? The first thing, we talked about this extensively, is that you should look at your schema. And you want to design your schema such that your reads and writes are as distributed as possible. So that there's not much of a data skew. Second thing is to realize that Bigtable learns over time. So don't be in an urge to keep changing your schema, keep changing your data. Let Bigtable run for a while before you start making changes, right. So you want to let it warm up a bit, how long? Well, try to basically let it warm up for, rule of thumb, a couple of hours. And when you're testing Bigtable, make sure that you're not testing on extremely small amounts of data. Try to make sure that you have at least about 300 gigs of data. Again, Bigtable is about large data sets, high throughput. It's not meant for very small data sets. And then, as the previous slide talked about, SSD is going to be faster than hard disk drive. And you've got to realize that the number of nodes that you have are linearly related to performance. So if three nodes can't keep up with your throughput, make it five nodes, right? So things are going to be linearly scaled. And this is pretty obvious, but make sure that your clients and your Bigtable are in the same zone. So launch Bigtable in the zone from which it will be accessed.
3:08
Now, we mentioned that the throughput can be controlled by the node count. So, you can basically get pretty much whatever queries per second you want, by simply increasing the number of nodes. So for example if you have a 300 node cluster, it would be able to get 3 million queries per second. So you can get whatever throughput you want, with an appropriate cost.
3:35
And with that we come to the end of the course. So let's do a quick summary of all of the things that we have done in this track.
