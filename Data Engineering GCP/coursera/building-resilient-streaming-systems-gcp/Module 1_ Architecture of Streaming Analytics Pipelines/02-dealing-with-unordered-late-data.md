## Latency is to be expected
So the solution on GCP, the way to get a durable, highly available messaging system, is to use Pub/Sub. Pub/Sub is serverless, as we'll see later, you don't need to start any clusters. Just publish messages to a topic in Pub/Sub, or subscribe to a topic in Pub/Sub, and that's it. So challenge number two, if you have two sensors sending data, smaller packets are going to arrive before larger packets often in most networks, even if it's from the same sensor.
And of course, if you have two sensors, the network paths to the two sensors could be very different.
So the data from sensor A will keep coming in with much lower latency than data from sensor B.
And what this means is that when you look at the totality of all the messages, they're all going to be all jumbled up. They're not going to be in perfect sequence, so you take this data and you do some computation.
Maybe you compute the average, maybe you compute the max. And then some data that was supposed to have arrived, but didn't, because of the different network path, because of some problem, it arrives late. And that max that you computed, it's wrong, what are you going to do? So what do you do about late arriving records?
So that maximum that you computed, it was a speculative result, but now you need to correct it. Because you've got new data that should have been included, how do you deal with it?

## Latency happens for a variety of reasons
Well, you have to operate assuming that latency will happen.
There are two schools of thought here. One says let's harden our systems, let's design our systems, let's build our systems so that there's absolutely no error whatsoever.
This is very unrealistic, it's not going to work.
The second school of thought says, let's design so that errors are going to happen, but we will be resilient to those errors.
We will take those errors into account and we'll assume that errors are normal, and we'll deal with them.
So latency is going to happen. It's a fact of life.
It could happen during transmit, because of network delays, ingest delays, write latencies, ingest failures, variety of reasons why when the messages are being sent you get latency. Or latency could happen during ingest.
Or latency could happen during the processing, maybe you're processing, you don't have enough workers so you're starving them of resources or there's a back log that's built up. So whatever reason, whether it's during transmission, whether it's during ingest, whether it's during processing, latency happens.
So we need to assume that latency is going to happen, and we have to come up with ways to address this latency.

## Beam/Dataflow model provides exactly once processing of events
So the Beam/Data flow model provides for exactly once processing of events. So it lets you write clean modular code that maps directly onto these four questions. And these questions are relevant in any data processing pipeline where messages can arrive out of order. Number one, what results are you calculating? For calculating the max, that's a P transform in data flow and you know how to compute the max. So that's you do that in your P transform. The second is where in event time are you computing? When are the results actually materialized? When they're saved and they're sent out? And finally, if a new message comes in, that's in it. How should you refine it? How should you change an already computed result? So if you can answer these four questions; what results are calculated? Where in event time should we calculate those results? When in processing time should you save out those results? And how do you change already computed results? How do you refine it? Then you can build a correct stream processing pipeline. So these powerful out of order processing constructs, watermarks, triggers, they all give you the ability to maintain eventual correctness of results in a pipeline. But at the same time they give you low latency speculation. This ability to publish a maximum before all the data are in. And they give you the ability to refine the results after the fact when you get new data. And you also get an easy way to basically cap the lifetime of data within your system. You can say for example that if a late record comes in three hours late, I'm just going to ignore it. Up to three hours, I'll refine my results. So all of these kinds of things are possible with a Dataflow pipeline and we'll talk about those things. 
