# Challenges in stream processing
So let's start off by talking about some of the challenges associated with stream processing, in particular about carrying out this sort of continuous processing. And we will use this to motivate some of our discussions around the concepts of windows and triggers etc. So when you look at stream processing, you have a few challenges. Number one, in the case of our traffic data, the traffic data is just going to keep growing. So we will have more sensors, they may start producing data more frequently, so size is going to grow. We need it to be scalable. So our traffic data as the data keep increasing and because we have distributed sensors - we have a sensor on every road, every lane, they're all over the place - so we need it to be fault tolerant. We need to handle the case that sensors go down, there's a problem with the networks or the sensors are sending you a burst of data because it suddenly came online and those kinds of things. We also want - think about your programming model. We need to be able to do things like, let's compare the traffic over the previous hour with last Friday's traffic at the same time. So at this point is this stream processing or batch processing? Last Friday's data you can think of it as batch. It's all done. We already have the data. But traffic over the past hour, well, that's unbounded data and we ought to do this comparison because we want to say is this unusual for this time of day. And any kind of processing where you need both batch and stream, you obviously want to do it in the same pipeline. So you want a programming model that lets you handle batch and stream within the same pipeline. And finally, of course underlying all of this is what happens if the data from the sensor is late. There are these sensors that are distributed all over the city of San Diego; the networks are not going to be identical; the sensors are not going to be all up at the same time, so we need to be resilient. We need to do our stream processing in a very resilient way to handle all of these problems.
## Need to process variable amounts of data that will grow over time
So the first problem is that that amount of data is going to be growing over time, we know that, right? Because most data grows over time, but at the same time we also know the traffic patterns change. There is more traffic at rush hour than there is at midnight.

So you basically have different amounts of traffic patterns and you have more sensors that are arriving. So how do you deal with this variable amount of data?

One way is to decide, what my work load is going to be. This sort of a curve, and, because my peak usage is of, requires say 18 workers, so I will provision a cluster of 20 workers. Well, that's a fixed cluster. And a 20 node cluster is a waste at a time when you may need right at the troughs of this curve, you may need only five machines, five workers in that cluster. So a fixed cluster is a waste of resources. At the same time you can say I'll go ahead and take the average and give you only ten workers. Because if you have ten workers then you have underprovisioned clusters half the time and those underprovisioned clusters are going to give you serious scalability issues.

So you will need to do something else. We need to auto scale our clusters.

The second thing that data flow gives us is this idea of windowing, right? We want to basically say where an event time. So again, the key thing is an event time, there's not just a question of what time did you receive the data. But what event time, right, is the event data corresponding to 8 o'clock or 9 o'clock, right, the when it was produced. So we're answering this question of where in event time do we need to compute this aggregation. When I say total number of stock market trades. I'm interested in the total number of stock market trades that happened at 8 o'clock, not the total number of stock market trades that I happened to receive at 8 o'clock, right? There's a big difference here.

We want to take this network delays out of this equation and when we compute our aggregates, we want it to be in terms of event time and that's what windowing is. So windowing is going to give us the unbounded data stream, and it's going to divide that unbounded data stream into finite chunks of data. Now you could do this with fixed windows right? You could say for example that all the data corresponding to trades that happened between 8 o'clock and 9 o'clock, all the trades that happened between 9 o'clock and 10 o'clock and so on. So those are fixed windows.

A more interesting way to do it is to say that I'm going to look at hourly trades, but I'm going to update my result every five minutes. That's a sliding window. So you have an hour of data but you're updating it every five minutes, okay? So that's a sliding window. Another type of window is also possible. And the whole point of this is to say that one size doesn't fit everybody. You need choices here in terms of what kind of windowing you want to do. So the third type of window you might want, is maybe you are a web application. You're building a web application and in your web application, you want to do you window based on user activity, a session. And you want to say the user logs in, the user logs out in between how many times did they do X?

Or how many web pages did they visit? So you're basically having a stream of click data. And you're processing it, but your window is not based on time. It's now based on user activity. You're defining a session, and you're defining as a session, the key here could be each user. You're basically saying for that user, how many clicks did they do.

Or it could be based on domain. So within this domain, overall users where each windows is defined as the time that that particular user logged in and that particular user logged out. What did they do?

So again, the way you do your windows is going to be different, but inevitably the way you define your windows is going to be in term of event time. It's going to be in terms of the time at which the thing in question happened, not the time at which you happen to be processing it. So we want to make this distinction between event time and processing time. Windowing is about event time.

## The Beam unified model is very powerful and handles different processing paradigms
So the Beam unified model is extremely powerful because it helps you handle different types of processing paradigms. So you have your data coming in and you might decide that you want to process your data as batch.

So you're going to basically take your data, you're going to treat it as being bounded, it's all there and you want to process batch data. Beam will let you do that. You want to take your data and you want to process them in micro batches, you want to just split them up and you don't want to deal with this idea of event time or those processing time, you just want to deal to this processing time. No problem, you can still do that. But more importantly and very interestingly, Beam will let you also do stream processing in terms of event time. And Beam gives you this flexibility and it lets you mix and match. So as we will talk about a situation where you want to do some things in batch and some things in stream, and you want to do them in the same pipeline. And that's still possible.

In addition, when you're building your pipeline, Beam let's you choose between high latency and low latency. Between this part of my pipeline needs to be batched, right. It needs to be, it can be high latency but I want it to be very accurate. Or this part of my pipeline can be speculative, I want it to be low latency, just publish a sum, but keep updating the sum.

Also, Beam lets you process different types of data, structured data, semi-structured data, object data, let you run queries. So the Beam unified model is extremely powerful. I know it can be a little unusual, you probably haven't seen it, but I strongly encourage you to learn Beam, because it's a hammer that solves so many problems, so many issues. And it does it especially if you run it on Cloud Dataflow, it does it in an auto-scaling, fully managed way. So it's a very powerful way to do things.

## For example, batch and window in same pipeline
So as an example of why you might want to do batch processing and stream processing in the same pipeline. Shameless plug here, so this pipeline is out of my book. That's first coming from O'Reilly on the data science on Google Cloud platform. So, the idea behind this pipeline is that you want to predict the delay of a flight, right? Is this flight that's taking off? Is it going to be delayed or not? And the arrival delay of a flight that you can kind of use a couple of things to predict it, to help you predict it. One of them is- my flight is taking off 15 minutes late, but this 15 minutes late that's how long I spent on the taxiway. Is this unusual or is this common? And this turns out to depend on the airport that you're at. So for example, if you're taking off from LaGuardia Airport in New York it's very, very, very common for your flights that leave in the evening to sit on the runway for 30 minutes. So you want to compare the time that you spent on the runway with the typical time that you spend on the runway at LaGuardia. So this is where you're computing the over batch, over historical data. So you're over the historical data, you're computing the typical departure delay of flights from LaGuardia Airport. On the other hand, you are also interested in things like weather delays. And the idea is that if you're going to be landing into Chicago and there is weather going on in Chicago, all the flights that are coming into Chicago are going to be delayed and therefore you are also likely to be delayed. But this time, this is the arrival delay that's happening currently in Chicago. So this is the arrival delay over the last few minutes that's happening in Chicago. So now we're talking about computing an average delay over streaming data. So we have an average delay over historical data, over global data which is batch. We're talking about an average delay at the arrival airport, which is over the current time. So that streaming data. And when we are basically trying to say, "let's go ahead and figure out if this flight is going to be delayed", you're interested in both the departure delay at your airport compared to the departure delays that's typical at this airport and the arrival delay at the arrival airport that is completely over just the current time. So you need to do batch and window the same pipeline. This is one example of it. But this is a surprisingly common thing. And most people don't think about it because they believe it's not possible. Well the data flow with the Apache beam, it is possible. So you want to think in terms of doing both batch processing and stream processing within the same pipeline.

## Dataflow resources are deployed on demand, per job, and work is constantly rebalanced across resources
So the other aspect about dataflow when you are running Apache Beam pipelines is that dataflow gives you auto scaling. So your work constantly gets rebalanced across compute and storage resources. So yeah, data comes in, and there is an optimization, and auto scaling, and rebalancing of parts inside Dataflow. Dataflow is also doing watermarking, it's figuring out which machines are overloaded, and it's healing them. It's monitoring it, it's collecting the logs, and it's sending all of these to you. So what Dataflow gives you is an execution framework for the beam pipeline. So you can write a beam pipeline and run it anywhere. But it's a good idea to run it at a place where you know that all of your pipelines are going to be executed very well. 

So bottom line, then, stream processing is a lot easier with Dataflow. Because the size matters, and Dataflow does auto scaling and does Mapreduce, it lets you handle variable querying volumes. Because you need fault-tolerance, it's important that resources get deployed on demand, and that work gets redistributed among the workers. New workers get added, if necessary. And the programming model that Apache Beam gives you, it gives you this ability to deal with both batch and string. And finally, we can deal with all of the issues associated with unbounded data by giving you a good, scaleable implementation of Windows triggers, of incremental processing, etc. So we look at how we do this in Apache Beam code.
