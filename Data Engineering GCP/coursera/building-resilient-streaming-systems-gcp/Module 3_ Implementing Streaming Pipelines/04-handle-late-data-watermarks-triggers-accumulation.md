## Handle late data: watermarks, triggers, accumulation
So the way we change it from the default of erring on the side of accuracy, is we have different ways of dealing with late data. Watermarks, triggers, accumulations. So let's talk about those.

So the idea here is that there are two concepts. You have the event time, which is bound to the source of the event. For example, the time at which something was published into Pub/Sub. And then you have the processing time. The processing time is relative to whoever is processing this event. This machine, the workers on which Dataflow is running. So in between the publication and the processing, there's going to be the network, and that's going to introduce some latency. So the processing time is greater than the event time.

It should be greater, but sometimes the clocks aren't synchronized, and so on. But if the clocks are all synchronized and it's perfect, then the processing time is always greater than the event time, because there's always a non-zero latency associated with this.

So another way to think about this, is that in the ideal situation, as the events get happened, they're immediately processed. So in the ideal situation, the event time and the processing time are exactly the same.

In reality though, events that happen at 8 o'clock don't get processed at 8 o'clock.

They might get processed at 8:03, and that 3 minutes is the latency. Or another way to think about this, at 8:03 you are processing events that happened at 8 o'clock. So you can think of it as a three minute skewed that's happened.

So this queue, this difference between the ideal and the actual event time that you're processing, you need to track it. Or actually, you don't need to track it, Dataflow needs to track it. So Dataflow tracks how far behind it is from event time.

And that is what the watermark is. So the watermark is Dataflow tracking how far behind the processing time is from the event time.
## Heuristic/guarantee of completeness (Watermark)
So the watermark is a heuristic that's computed from the stream. So as the stream's coming in, remember that every time something gets pushed into Pub/Sub, Pub/Sub is going to say, this got pushed into me, at 8 o'clock. And then data flow says, right now it's 8:03, and therefore it looks like, based on this message, this queue is three minutes.

Hopefully it's not three minutes, let's say it's three milliseconds, so we have a skew of three milliseconds. And then it looks at the next thing, and it's a skew of something else. And so it basically averages the skew over a period of time, it learns the heuristic. But it can also, if it's something like Pub/Sub, you can also define the watermark to be a guarantee of how complete it is. We know that we can have received all of the messages, because another thing that data flow knows is how Pub/Sub assigns its message IDs. So it knows that even though when it receives a particular message that it has received all the messages before this particular one or not. So it could be a heuristic, it could be a guarantee. Depending on the system involved, but the watermark is essentially how complete this message is. So when we go into this time-shuffled window, so you have an unbounded stream, and you're doing a time shuffle, that's your window. And it's going in, and these are all of the messages between the eighth minute and the ninth minute, for example. And these are all of these things that event time is between eight and nine, and those go in. So all the eight stuff is in the window between eight and nine, so that's the basic idea. And data flow then tracks whether that window is complete or not, and the check mark idea is it's complete. And that the idea is that it's still receiving some events for the event time between 10:00 and 11:00. But it knows that, based on historical information, that time window's unlikely to be complete, and it needs to wait before it closes that window. So the idea here is that data flow is learning this watermark, this typical skew associated with messages. How long the messages take before I receive them. So am I ready to compute it now or not, is basically that heuristic. The data flow is basically learning as it's doing it.
## Windows + Watermarks + Triggers collectively help you handle data arriving late and out-of-order
So, with windows you're basically saying where in the event time you want to process. The watermark is as heuristic of, "Have I received all the data that I need to go ahead and compute it?" And the trigger, which we haven't talked about, is how when you do this computation. Collectively, they help you handle that data that could be on order, that could be late. So we look at four different cases here. We look at how you do it in a classic batch, what it would mean to do a windowed batch or microbatch, but then we will move on to how you deal with streaming and how you deal with different ways of doing triggering. Bottom line, again, we've talked about windowing. And window is where in the event time are you actually processing. Are you processing data between eight and nine? What you're computing is basically a transform so we are doing our censored data, all of the extraction associated with it, those are all your T transforms. And then, when does it happen? In processing time, that's your watermark because a watermark is essentially carrying out this, "how long do I need to keep the window open that I believe I'll receive it." And the trigger is, "what do I do about late data?" Because any heuristic is a heuristic. There could be data that come in after you close the window so the Watermark helps Dataflow decide when it's a good time to close the window.
## How Dataflow handles streaming data while balancing tradeoffs
So dataflow handles streaming data with balancing a few tradeoffs. The windowing model lets you basically deal with eventtime windows. The triggering model helps you basically bind the output time of the result to how the pipeline is operating. So you can basically say, "this is the way I want triggering to happen." We'll look at examples of this. And finally, an incremental processing model that says, "how do I go back and recompute means and averages and sums etc. to make them correct if necessary?" And dataflow gives you a scalable implementation of all of these models. So scenario number one, no triggers, no big windows, you just want to process all of the data. So now you're taking all of the events, no matter when they happened and you are essentially processing at once. And so you're saying, take my input and apply sum, and you basically get the entire sum. And so you would get the sum of all of these numbers and you would get the number 51. That's batch. No windows, no triggers, no nothing, just comput the sum over all of the data. Second scenario, you take the windows and you say, "I want to go ahead and compute the sum, but I want to compute the sum over fixed windows of two minutes." So all the data that were produced between 12:00 and 12:02, notice that again this is the eventtime data that were published between 12:00 and 12:02, compute the sum, data that were published between 12:02 and 12:04, compute the sum, data that were published between 12:04 and 12:06, publish the sum, data that were published between 12:06 and 12:08, publish the sum. So in this case then it is going to be, go ahead. It's this batch, so all the data are present. So now we get an accurate sum over fixed windows of two minutes. Now, what happens if you have streaming? If you have streaming, then you have this problem that whenever you close the window at 12:02, you're not sure that you actually do have all of the data at 12:02. So what you're now telling data flow is- go learn a watermark and close the window at the watermark. This is the default anyway, right. Triggering is at the watermark. But you can explicitly say that. You can say, "go ahead and break this up into windows of two minutes and trigger at the watermark." So then, dataflow's thing at 12:06. I believe that I have all the data that I will have at 12:02. So all the data that were published by 12:02, I believe I will have it at 12:06, so I'll close the window at 12:06 and I'll emit the sum five. Then the next window 12:04, I believe that I have it all complete at 12:07. A little after 12:07. So this is something that's learned. It's learned that this particular thing arrived four minutes late. And based on that it's basically closing the window four minutes later saying, "you want 12:02 window? I think at 12:06 I should have received it", and closes the window. The 12:04 window, it crosses at like 12:07 and it computes the sum, and that 12:06 window, it closes at something like 12:08, and it computes the sum. And the 12:08 window, it closes at something like 12:10 and it computes the sum. So that's essentially the triggering. You're triggering only once at the watermark. What are we doing about late records? Nothing. Because we did not trigger on late records. By default the triggering is at the watermark and also whenever we receive a late record. In this case we've replaced that default by saying, "trigger only at the watermark. Throw away all the late records." That's essentially what we're doing here when we said triggering a watermark.
## Watermark is the system's notion of when all data in a certain window can be expected to have arrived
So the watermark is basically a systems notion of when it thinks that all the data that should be in a particular window, when does it think it has all arrived. So you can think of it as the oldest unprocessed event, how far behind the system is. So watermark's basically about where an event time to compute.

And the triggers are relative to the watermark typically. The trigger is at the watermark do it.

But also do it one minute after the watermark. Also do it two minutes after the watermark. Also do it whenever you get pend late records.

So you can trigger based on count. You can trigger based on time, you can trigger based, but you would typically trigger at the watermark or one minute before the watermark, one minute after the watermark. Something relative to the watermark, and something based on count.

So this watermark remember, is based on the arrival time in PubSub.

So that's the default. If you want to use a custom time stamp, maybe your message producer has a clock. And it has a time stamp in your message, then you have to tell PubSub that you don't want it to use its time, right? Or you want to tell Dataflow, you don't want it to use PubSub's time you want it to use some other attribute in PubSub. Again, as far as PubSub is concerned, all messages are opaque. So you can't basically go around processing the event. Instead you set an attribute. And in this case, I'm setting an attribute, I'm calling it my time. You can call it whatever you want, and you basically specify that time In this format, right? The format is important. So you specify the time in that format and then you tell Dataflow which of the attributes of the message is that timestamp attribute. So if you specify a timestamp attribute, it'll stop using PubSub's publication time and instead it'll start using the time that you specify.

So you can either go based on PubSub or you can go based on your own publication time.
## Streaming with speculative + late data
But let's go back, so let's make it a more complex triggering. So you can trigger at the watermark, but you can also trigger, right, withEarlyFirings. So you're basically saying go ahead and trigger a minute early, okay?

So you're going to trigger one minute before the watermark, and whenever there is a late record. Okay, so late firings AtCount(1). So if I said AtCount(10), then I would trigger whenever I get 10 late records. It would keep adding, the data flow will keep track of the late records, and when those reach ten, then it will trigger, right? So you can basically specify the time here. So WithEarlyFirings, obviously means, that earlier than the watermark. So one minute before the watermark, close the window. And then, every record that arrives after that, AtCount (1), trigger, close the window, refine the result. But the allowed lateness is only 30. So if I got a record that is more than 30 minutes late, throw it away. I don't care anymore.

So that's a more complex way of doing it. And in this case then, so for example, right, you basically get a record on time and it computes that. And then you get this one, right, that's early. So that's your early firing. And then you compute something on time. And then you'll get a late record and you compute it again. So you basically have three results that get computed for that particular window, right? Basically, so three refinements that happen for that particular window, and that's perfectly okay.

And, finally, right, if you want to do a session window, you would basically do sessions with a gap duration of two minutes. So the idea is, all of the events that happen, we're going to treat them all as one session. Until there are two events that happen at least two minutes apart, and then that becomes the next session. So you trigger at the watermark, with an early firing of a minute.
## In your DoFn, can get information about Window, Triggers
In your DoFn, you can get information about the window itself. Normally, we've just done a process element with a process context, but you can add an extra window parameter to your process element function and if you do that just your data flow will basically push to you information about the window itself. So you can say, "Go ahead and tell me the max time stamp within this window," for example, and you will get information about the window. So with that let's go ahead and do the lab, on processing the data. In the previous lab, we published the data into pops up. In this lab, what we're going to do is that we're going to process the data using Apache Beam and using Cloud Dataflow. Here, what we're doing is that we're extracting the data, we're windowing it, we are grouping by sensor, and we're computing the average. So we're basically finding the average speed for every sensor along highway. 
