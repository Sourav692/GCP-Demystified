## Build a stream processing pipeline for live traffic data
So what you're going to do next is that we're going to look at how to build a stream processing pipeline for live traffic data. So we'll use that as the example but we'll use this to look at what stream processing in beam looks like. So the scenario here is that on the freeway in San Diego we have a bunch of sensors. These sensors are placed one per lane and you have these sensors at periodic intervals along the highway. So these are your traffic sensors and they're measuring and sending you information about the traffic that's passing over that sensor. So in order to process this data we're going to create a streaming pipeline. Creating a streaming pipeline is just like creating any other pipeline in data flow. We looked at this in the course on server less data analysis. Right? So where we created a dataflow pipeline. So we go ahead and create the pipeline. It was a pipeline that create. But these options, the one change is that you set streaming to be true. That's it. So is that the streaming to be true and dataflow knows that right now you're not doing batch, you're actually computing stream. But I mean within the streaming pipeline, there can be things that are global aggregations and those will be batch. So for example, let's say that we are taking our traffic events. We are getting the messages extracting the messages and writing them into big Query. So the source is Pub/Sub, the sync is BigQuery. How does that look? Well, essentially you're saying PubsubIo.readStrings from it, from the topic and that's your first P transform. That's going to give you a set of strings. Right? And you're going to take those strings and that string, you're going to basically convert it, extract the information and make a lane info object. So at this point now you get a p collection of lane info. Now, you take those P collection of Lane info and calling the current conditions and then we can convert them into a table row for big Query and we can say BigQueryIO.writeTableRows. So this part is similar to every beam pipeline that we wrote. It's pretty identical. The only difference was that we had set streaming to be true in options. And rather than read from text, normally if were reading from textIO. But this time we'll now be reading from PubsubIO. So that's pretty much the difference.
## Dataflow is a great way to work with Pub/Sub
So Dataflow is a great way to work with Pub/Sub. Pub/Sub, remember, is a low-latency global delivery service. It's a message bus. But there are a couple of issues. It doesn't guarantee order of messages.

It doesn't guarantee order of messages, and because it only gives you at-least-once delivery guarantee, it is possible that you have repeated delivery. That the same message gets delivered a couple of times. Stream processing in Dataflow is going to account for this. Number one, when you're computing aggregates, it's going to work with out-of-order messages. Because you're shuffling, not by the time at which you've received it, but by the time at which the message was published. And so it works automatically with out-of-order messages. And also, remember that whenever we published into Pub/Sub, we had a message ID?

Dataflow knows about this message ID. So it keeps track of which message IDs it has already processed. So if Pub/Sub happens to redeliver a message, no problem.

Dataflow is essentially going to use that internal Pub/Sub ID to deduplicate it, remove it, and end up processing the messages only once. So Dataflow knows about Pub/Sub. And because it knows about Pub/Sub it, together with Pub/Sub, can give you exactly once processing guarantees.
## Can enforce only-once handling in Dataflow even if your publisher might retry publishes
So, you can enforce the only-once handling in dataflow automatically. Right. It's just that's the way it works. But, this only works about messages that are going to Pub/ Sub. What happens if the person publishing two Pub/Sub, the person publishing to Pub/Sub could publish multiple times. Right. So, this could happen for example if you are getting data from a sensor, the sensor knows nothing about Pub/Sub. So, it's just sending you requests. And sometimes, a sensor goes down, it goes back up it comes back up and it doesn't remember whether it has sent a particular message to you or not. So, it resends those messages. Now, as far as Pub/Sub is concerned, you got a new message. So, it's going to give it a new message ID. So, this is not going to work in terms of duplication. Because, Pub/Sub thinks that these are two independent messages. So, the bottom line is, if the person publishing to Pub/Sub could publish multiple times, then when you publish it to Pub/Sub add a unique label to your messages. So remember, that when you publishing to Pub/Sub we can specify attributes. So, specify an attribute. And in this case, my attribute is going to be called myID. It can obviously be called anything. And then, when you're reading from Pub/Sub in dataflow, we tell dataflow that there is an ideal label and that's the ideal label. So now, data flow instead of keying off the internal Pub/Sub ID, will now key off on this attribute instead. And, make sure that any particular ID gets processed only once.
## To compute average speed on streaming data, we need to bound the computation within time-windows
So back again. So we now know how to take our data and just stream it right out. Okay that's element by element processing. As we said, that's not that interesting. What if we need to do aggregations?

Let's say we want to compute the average speed.

If you want to compute the average speed on an unbounded data stream, it makes no sense.

What is the average over something that has no beginning or end?

You need to think in terms of an average of a bound, within a time window. You have to say, the average of all the traffic that happened in the highway, over the last five minutes, all right? Or between, 7 o'clock and 7:05, so you would need to provide this time window and the time window that you're providing is an event time. The time when it happened, right? So in this case, we're basically saying currentConditions apply. And what are we doing? We're basically taking our currentConditions, which is our element by element stream, and applying a time window to it. And this time window that we're applying is a sliding window. It is all five minutes every 60 seconds. So we are sliding a five minute window over the time stream.

By itself, just specifying a window on the event doesn't do anything to it, right. It's just you're specifying here's a window. The key thing happens when you do an aggregation. So after you apply the window, somewhere later on in the pipeline you're going to do an aggregate. So in this case the aggregate that we're doing is a mean per key. So you need to do some sort of a group by key.

And now this mean that you're doing is going to be a mean of only the events that happened in that five minute window. So that's the key thing. So you apply your window, and then you apply the aggregate. So in order to compute the average speed on streaming data, we need to bound this computation within a time window. So you have bound it within a window and then you compute whatever it is that you want to compute. In this case we want to compute an average per sensor, and because we want to compute an average per sensor we group it by sensor. So we basically say, get me the sensor key, get me the speed, I'll put a key value pair of the key and the speed and we do a mean.perKey. So we're going to get the mean per sensor, because this key in this case is the sensor.
## Did we use triggers? What did we do with late data?
But wait a second, what happens to late data? 

So we have a window that's a five-minute window, it's doing it every 30 seconds, or every 60 seconds. But what happens if some data comes in three minutes late, and we've already closed the window, we've computed the mean, and we've sent it on. So by default, the trigger happens when you have what is called a watermark, and we'll talk about what a watermark is. So it will happen once, at the watermark, and I'll talk about what a watermark is later. And the mean gets written out, but then whatever watermark you have, you may have some late data after that watermark. And if you get late data, this mean is going to get recomputed. So every time you get a new record, you're going to recompute this mean, and going to stream it out again. So, data flow is going to handle this for us, from the default trigger setting. The way you remember this is, a default trigger errs on the side of accuracy. We want to make sure that your result is correct, so we close the window, we write the mean out, we say here is a mean, that's a speculative result. A new thing comes in that needs to be part of this, we'll recompute, and we'll write it back out again.

Obviously you want to be able to change this, and you can, but let's talk a little bit about how you change this. And the way you change this from, do it the most accurate way, to make some tradeoffs. Is really to understand the idea of what a watermark is and what a trigger is.
