# Module Intro and Agenda
So far we've looked at two of the challenges that we started out with. One was to be able to deal with variable data volumes during ingest, and we talked about Pub/Sub as a way to handle that. The second challenge was to be able to do continuous processing of the data as it's streaming in. And we looked at cloud data flow as a way to do that, and the concepts of a watermark, and of a trigger, and of a window to control how to do aggregations and how to make these computations. The third aspect is that even as the data are coming in we want to be able to do ad hoc analysis, we want to be able to power dashboards, and so that's what we will look at next. So now we are looking at the BigQuery part of the GCP reference diagram.

So we will look in this chapter at streaming analytics and dashboards. Now this is one of the things that might look kind of trivial, in some sense, but it turns out to be extremely difficult to do. So, before I joined Google, I use to build real time weather processing systems. So everything that we did was real time data. And so we built the system that was a messaging system, that would get the data from radars all over the country and carry out computations on that radar data as the data came in. So we got new data from a radar about seven times a second. That data was voluminous. We had to deal with it. So in essence we built something that looked like Pub/Sub. We had a messaging system. We built something that looks like data flow in the sense that we had the concept of streaming computations. What we couldn't do, what was really hard, was this latter aspect of being able to do ad hoc queries and power things like dashboards, without having this intermediate step. And this is what we did, we would basically take the computations that we computed and save them to disk. And then all of our tools would read the data off disk.

The problem with that, of course, is latency. We would tend to flush things to disk once every five minutes, and so we dealt in our real time, quote unquote real time, weather processing system we dealt with data that were five minutes old. So that was our trade off, but you don't need to make that trade off. And that's basically what we're talking about here, in terms of being able to do ad hoc analytics and dashboards even as the data are streaming in.
## What is BigQuery?
So what's BigQuery? We've looked at BigQuery several times in this course. So it's essentially a fully managed data warehouse. You don't launch a cluster. You just have your data. It separates out computer and storage so whenever you want to query the data, you just submit a SQL query and the query happens. It deals with data up to the scale of petabytes, but the querying is very convenient. It's SQL. The data, as is true with everything on GCP, it's encrypted at rest, and it's encrypted on the wire. This is true of Pub/Sub. It's true of BigQuery, as well. So the data encrypted. They are durable. BigQuery is highly available. In addition, you only pay for what you use. So you basically get thousands of machines that come up and carry out your SQL query for you, but you only pay for what you use. But the last aspect is what you're going to be concerned about. BigQuery also provides streaming ingest to unbounded data sets. So it allows you to stream data into BigQuery to the streaming buffer and carry out SQL even as the data are streaming in. So best practices then is that you combine Dataflow and BigQuery. So you do your processing, your routine processing, the things that you always have to do over and over again, you have to do on every piece of data that comes in, you do that in Dataflow. And then you have Dataflow write out tables in BigQuery. Those tables allow you to carry out data-driven decisions, do ad hoc queries, and power dashboards. So you're basically taking advantage of BigQuery for long-term storage, right. So because Dataflow is a processing engine, Pub/Sub is an ingest messaging bus. It's a way to decouple publishers and subscribers. It's not a permanent storage. BigQuery gives you long-term storage at about the same cost as Cloud Storage. So it's very cheap. And finally, in BigQuery, you want to basically create views that basically provide very common query support. So the kinds of queries that people are going to do, you take those tables, you write your SQL statements to provide views to them, and you give those views away so that people can use it to power their dashboards.
## Streaming data into BigQuery
So when you stream data into BigQuery, what kind of rates can you handle? Well you can handle streaming up to about 100,000 rows per table per second, okay?

So essentially what you're doing is that you're basically calling a REST API to do it and that's what data flow is doing, data flow is calling the REST API. So this is not magical. So you can do this even outside of data flow as long as you make the call to the REST API. It works for standard tables. It also works for partition tables. So if you have tables partitioned by date, you can stream into those as well. And the absolute cool thing, is that you can query streaming data as it arrives. And normally, what happens is BigQuery will go ahead and de-duplicate records. If for whatever reason, you insert the same record twice, it basically de-duplicates it. But this is on a best-effort basis. So if you want to be absolutely sure, when you insert, you basically provide an insertId for every row. And that way you can de-duplicate it, okay? But normally, you don't bother, it's fine. The data are inserted by data flow into BigQuery. So once you have your data into BigQuery, let's say within the table current conditions, so what can you do with it? Well, you can query it. So what are we doing here? We have this inner query, select sensorID and MAX of timestamp AS timestamp, from this table GROUP by sensorId. So in essence now, this inner query gives us the latest timestamp for every sensor. So we know what was the last received data from every sensor. And then we basically say, for each of those sensors, for each of those timestamps. Give me all of the data. So now we get all of the data from the sensor, all of the most current data from the sensor. And this is what you will do to power your near real time dashboard, because you want the latest data to power it.
## Data Studio lets you build dashboards and reports
This dashboard itself, you could build in a variety of dashboard tools, will be showing you data studio. Data studio is a dashboard tool by Google, it's free so it's an easy convenient thing to show but there are a variety of other dashboard tools out there that you could use. Data studio lets you build a dashboard that lets you build reports and it works a lot like Google Docs, in the sense that you can create a dashboard, and you can share it with people, and you can collaborate on a dashboard, so it's really really really good for those kinds of collaboration and scenarios. You can do a variety of charts. The charts could be as simple as bar charts or charts of different types of maps. Now, we have a variety of charts out there, we will look at some of them in the lab. Now, data studio can create all these graphs but it can also, in terms of what powers is input, it basically can connect to a variety of different GCP data sources. It can connect to Cloud SQL, it can connect to Bigtable, to BigQuery etc. We can connect from data studio to a variety of different GCP sources and that list keeps expanding so I'm not going to say these are the ones, but go ahead and when you open up data studio you will find the list of sources that it can connect to. In particular though, it offers a BigQuery connector, and in the BigQuery connector you can read from a table or you can run a custom query, and you can use that to populate it. You can build charts, graphs, you can do maps. You can do a variety of things. The thing that we are going to do is that we're going to basically get traffic data from a sensor, and what we want to do is that we want to monitor each of the lanes, and we want to stay in a particular highway, if any one lane, the traffic over that lane is much slower than the other lanes in that highway, it is possible that there is an accident or a blockage in that lane, so we want to basically issue an alert. This is something that we have to do all the time. We have to basically monitor the highways and we want to look at every lane, and if that lane is slower than the other lanes in the highway, we want to issue an alert. Where should we do this? Should we do this processing in Dataflow or should we do this in BigQuery? Think about it for a second. Where would you do the processing to figure out if one lane is slower than others? I hope you said Dataflow. The reason you want to do it in Dataflow, is that you want to do this all the time. BigQuery is good for ad hoc. If you want to basically ask, is there a lane blockage right now? That's BigQuery. It's an ad hoc thing, somebody walks into your office and says, "Hey! Can you look at the data and show me whether there's blockage or not." That's a dashboard, that's a BigQuery. That's fine. But if it's something that you want to do over and over and over again, you want to continuously do it, so any kind of continuous analysis on data is better done in Dataflow. We're going to be monitoring and we're going to be basically finding that that lane, for example, is slowed down. In order to do that, what we're going to do is that we're going to read our sensor data from Pub/Sub, and then we're going to window it, and compute the average speed at a particular location. The average speed at a particular location is all of these sensors at this particular location, the average is all of these. And then we will find a slowdown. And a slowdown will essentially say, "Is that going to compute the average of all of the sensors at this location? And if my speed is slower than everybody else, then there is a slowdown on my lane." And that's basically what that find-slowdowns is going to do.

## Pipeline to detect accidents
So this is basically going to be our pipeline to detect accidents. We're going to get messages from Pub/Sub. We're going to extract out the data, that's going to be the information associated with the sensor on a lane. We're going to basically take that lane data, and send it directly to the Detect Accidents. This is the current speed at that lane.

Meanwhile, we're going to basically go ahead and compute the average speed of all of the sensors at that High V location. So because we're going to be computing an average and then average doesn't make sense in an unbounded data stream just by itself. It's not an average, historically, over the entire data set, because there is no entire data set. Instead, we're going to compute the average at this location over the past few minutes. So that's basically why we apply a time window, we compute the average. And now our Detect Accidents is going to take the average at that location, and the actual speed of that lane. That's going to compare it and it's going to basically write out all the accidents to Pub/Sub, say. In addition, it's going to take the average speeds and write the average speeds to BigQuery. So this is basically going to be our complete pipeline. 

How does that work? Well, we go ahead and find our slowdowns and we write them out.

So let's go ahead and look at how to do this. So what you would do in this lab is that we will complete our data flow code so that we are actually finding accidents. We'll also learn how to build a dashboard so we can look at data that is streaming into Bit Query rather than typing SQL queries we'll actually create nice, impactful graphs.

So, if you look at a streaming pipeline to process traffic events, we'll perform transformations in the data that averages to detect anomalies, to detect which lane is slower than the rest and store all the results in BigQuery for future analysis. Actually, we'll store even the accident data, we'll store at to BigQuery.
