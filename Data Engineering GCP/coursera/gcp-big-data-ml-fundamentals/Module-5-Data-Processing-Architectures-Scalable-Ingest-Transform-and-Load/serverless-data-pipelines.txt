0:00
The other architecture that we want to talk about is this whole idea of building data pipelines.
0:07
Dataflow, Cloud Dataflow, is the execution framework for Apache Beam pipelines. Apache Beam is an open-source API that let's you define a data pipeline. So this API here that we are showing you is Apache Beam. And you're basically creating a pipeline, reading some text, and the text that we're reading is from Cloud Storage. So we're reading it line by line and for every line, we're applying Filter1. The result of that goes to Group1, the result of that goes to Filter2, the result of that goes to Transform1. And then the result gets written out to another Cloud Storage. The neat thing is, Filter1 is wrapped into a ParDo, parallel do. And what this means is that every line that's read from the input source basically is processed by Filter1. But this is done in parallel across tens of machines, hundreds of machines, as many machines as you need. And you don't need to create these machines beforehand. Dataflow manages the provisioning of these machines, the auto-scaling if necessary, of your pipelines. Such that Filter1 just happens at scale, completely distributed and then everything comes streaming back into Group1. And then the results of Group1 again get done in parallel using Filter2. And then Transform1, and then it basically goes into this sync. Now, each of these things, Filter1, Group1, Filter2, Transform1, these are all classes that you write in Java. You can do Beam in Java or you can do it in Python. I'm showing you Java here, but you can do it in Python. And the code that you write, the Beam pipeline is an open source API. And you have other executors besides Google Cloud Dataflow. So you can execute it on fling, on Spark, and a variety of other things. So with Dataflow, what you get is a completely NoOps data pipeline. So you have a job, you submit it to the Cloud, and that job gets executed. In this case, all of this processing happens on some text input, and some text output gets written out.
2:23
A very neat thing about Dataflow is that it can have that intermediate processing be identical. Filter1, Group1, Filter2, Transform1. But you can swap out the input. Rather than reading from text, in this case, I'm now reading from Pub/Sub. So I'm now reading messages that come in into a particular topic. And remember that I'm doing grouping. Grouping is an aggregation operation, so doing grouping where? If you're accounting or a sum, you need to basically do these things
2:57
within a particular window. In this case I'm saying the window that I'm going to do all of these things on is over a period of 60 minutes. So I'm applying a 60 minute window and although I don't have a .every here, the default of .every is 1 minute. So I'm applying a 60 minute window every minute. And every minute, I'm basically writing out a new message to the output topic. That consists of filtered, grouped, Filter2 transformed data. That's a new message that now goes to Pub/SubIO. I can change the second PubSubIO, the output sync, to be BigQueryIO. And then I'll be writing to BigQuery, I'll be writing table rows to BigQuery. And then now other dashboards etc., can basically run BigQuery queries on the streaming data as it comes in. And that's the way that you generally get real-time insight. The cool thing is the processing that you do. Whether the data comes from Pub/Sub or the data comes from Cloud Storage, the processing that you do in Dataflow remains the same.
4:08
So Dataflow helps you do ingest, helps you do transformations. Help you do load, so it can do filtering, you can do grouping, and you can do windowing. So Dataflow is where we see a lot of data pipelines migrating. Because you really want to be able to process historical data and real-time data in and identical way. That is the only way that you'll be able to build a machine learning pipeline. For example, that is trained on historical data that operates on real time arriving data. And Dataflow is essential glue in order to be able to do this kind of data processing. 
