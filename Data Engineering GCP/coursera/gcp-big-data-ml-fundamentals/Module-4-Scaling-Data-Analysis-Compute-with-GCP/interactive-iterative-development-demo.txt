0:01
So, in this section, again we're talking about transformational cases. The next transformational case we want to talk about is about notebook development.
0:11
As a data scientist, one of the major changes that happened in the way I work with.
0:17
No, but data is with Datalab. I think we recorded this in person already, so we should use that here.
0:28
So, let's, the way you work with Datalab, is that you have a web page, that's your Datalab webpage. And in this webpage, you can basically write code in Python, and once you write the code in Python, you can run that code by either hitting Shift Enter, or by clicking the Run button at the top of the menu. You can look at the output, you can go back, change the code, run the cell, change the code, run the cell and at that point some time you're basically satisfied with the way the code works you can go in and you can write some commentary right? And the commentary can be mark down format, it can have titles, it can have bold emphasis and all of those kinds of things. It can have links, images, you write your commentary your documentation, and then because it's just a web page you share it and other people can come in. And they can execute your notebook and they can make changes to your code and rerun your notebook, it becomes an extremely good collaborative environment.
1:29
But one issue still exists, and the issue is where does the web server run? Remember that this is the client client site, the web page is a client but you still need the web server And if you run the Datalab or iPython or Jupiter Web Server on your own laptop, then as soon as you close down your laptop, nobody can access your notebook. So, you really want to have this thing running on a cloud computer, and Datalab Is a way to run it on GCP. To work with this then, you have to think in terms of two things, where does a server run and where does a client run. So, one option is to run both locally. And this is if you're doing local development. Your own CPU, you store your Notebooks on disk And you access your notebook using localhost:8081/ for example, that's your port. And data log comes in a docker container, so you're basically running a docker image locally. So, that's good as long as you are the only one who needs to access this notebook. The second option, this is good if you have multiple people going to access a notebook, is to run the Docker container on Compute Engine.
2:45
So you basically get a Google compute engine instant up, and going and on that computer gen instance you run that Docker Container and this way then whenever you need to connect to the notebook Use an ssh tunnel CloudShell will let you do this. So, you'll use an ssh tunnel where CloudShell to connect to this GCE
3:07
instance and then inside your browser you're working with it, but remember that everything that you're running the code itself is getting executed on the computer engine instance.
3:18
A third way to do this is to basically have it again on a computer engine instance, but access it through a gateway, so you basically have a proxy set up and this involves a little bit more in terms of setting up your browser in such a way that you are not going through cloud shut. For the purposes of this class, the CloudShell approach is what we will use. But if you are going to be doing this a lot. If you are a data scientist, you do quite a bit of data science work. You might want to explore the third option, because remember that CloudShell is an ephemeral VM. And every 60 minutes or so it'll get recycled. And then you'd have to go create the SSH tunnel again. It's pretty easy to create an SSH tunnel, it's just a single script that you run, but it is still something that you might want to avoid doing, it can get pretty frustrating. So, if you're going to be doing this a lot, and if you're going to be doing this for more than 60 minutes, let's say if you're going to be doing this for more than 60 minutes, you might want to look at the third option. But for the purposes of this class, we will use the second option because it's very simple, very easy to get going. We already have CloudShell. It's easy to create a Compute Engine VM. So, let's go ahead and do that. So, that code lab, that link, gives you directions for doing all three of these things. So, let's click on this link. And you will see that it takes you to option one. One option, two or option three. Option one from G crowd SDK, option two from crowd shell and option three from local machine. These are actually reversed from the order in which we covered them but this is basically the order of recommendations if you will, okay. So, this is the one that we recommend, this is the one that's an okay thing, This is the one that you would have to be able to install software on your machine, etc. So, this cord lab is organized from the best to the not so best. So, let's go ahead and, in this class we'll basically use option number two. To run Datalab from Cloud Shell. If you want, you can follow along with me, pause the video if necessary so you can catch up, and we can go on. So, here what I'm going to be doing is, number one, I'm going to open up Cloud Shell. And, if necessary, I'm going to clone the following Git Repository. I already have the Git Repo, So that I can copy this. And so we need to open up cloud shelf. So, how do we open cloud shelf? The way you open cloud shelf okay is to base go to cancel this okay
6:10
The way you open up CloudShell is to go to console.cloud.google.com. And this takes you to the JSP web console.
6:23
And you can click on the Activate CloudShell. If you've been using Quick Labs to do your labs, then we will already have a data lab that's up an going for you. So, you can just look at this demo as how you would do it outside of the Quick Labs environment.
6:43
So, once we have Cloud Share up we can basically go ahead and do get clone In this case I don't need to get cloned because they already have it. So, there is the training data analyst already exists. So, the next step is to go to the datalab directory says cd training-data-analyst/datalab/cloudshell. So let's go into that directory. And I'm here. And basically, I see a few scripts that exist. The script that I need to do is to create the virtual. The web server, if you will, for the notebook. So, I'll do ./creatvm.sh. and at this point it is creating US central 1a and in one standard NBM. If you want to change the zone or you wanted to change the type there is a file. Let's just look at the file. I just opened a new cloud session And inside datalab, there is an instance details that I search and you can change then the zone or you can change. Change the machine type here. But us-central1 and n1-standard-1 are pretty reasonable. So, I'll go ahead and at this point notice that the datalabvm has been created, that's the name of my VM. It's in this zone, it's n1-standard-1. This is the IP address of the VM. This is going to be useful if you're setting up a different proxy that's the option, there are other options that exist, but for now, maybe we can basically we will connect to this through cloudshell. You can look at this VM by going to compute engine and if you look at the instances that are currently running there is the data lab VM that's currently running We can associate into this VM, we can do staff on that VM if you ever need to, right. So, you can associate into the VM and you can see what's running. And what should be running is docker, so if we go into this VM,
8:56
Docker, unfortunately, runs only as a root. So, after we go into this VM, I'm going to sudo su, so that I'm now root, and I can do docker ps. And this tells me that gcr.io/google_containers is running, but not yet datalab. So, if we can just wait for Datalab to get started.
9:27
A couple more minutes.
9:44
So, now, when we do Docker PS, let me just clear this so it's obvious, that we do Docker PS now. Notice that Data Lab Cloud Training has also started. So, now, that Data Lab has started, we would be able to go into and connect to it. So, to connect to it, be in the same directory that you did create VM from, there is another script called start tunnel. So, go ahead and do ./start.tunnel, and this let's us tunnel into this vm from cloud shell and then click on this arrow to do a Web preview. Change the port to be 8081. So again, after you start the tunnel, move to this
10:31
thing that says Web preview And change the port to be port 80 81 and at this point you are now into Datalab. So, let's go ahead and look at how to use Datalab. So, I'll accept the conditions and now that we're into Datalab, let's go ahead and create a new notebook.
10:52
So, this is the new notebook, this is the kind of thing that I'd be working in. And it's Python, so I can do for example x = 2 and y = x + 2. print y, and then I can hit Run.
11:10
And well it's four. Surprise, two plus two is four. Great, but I can also go up here, I can add a mark down. I can move the mark down up, right? Let's see what <pre> 2 + 2 </pre> is. And then I can hit run. And you see that, let's see what 2 plus 2 is. That's basically text. I can go up here. I can add a title.
11:41
Fancy arithmetic and hit run and there we go. Right? There's Fancy arithmetic as a title With some code and this is basically a notebook that we can save. I'll save in CheckPoint. All right, this notebook is called Untitled Notebook, so I can go down here, I can click on this thing, right? And I can do a variety of things with it. There are other notebooks that already exist. So, actually not that. Other notebooks are already exist. So, if you go to Docs, there are the right sample notebooks. So let's go to the tutorials, let's go to the tutorials. And look at tutorial on how to work with BigQuery, or how to work with, let's look at how to work with the storage.
12:40
So storage there is storage campaigns iPad and notebook. And there it is, right here is an already fully pledged to notebook with text and with output and so on that has already been written and we can click on this thing and do Shift + Enter and we are running these things one at a time. All right? That's our storage list. So, that's all of the storage buckets that I have. I can say here, also that, so also notice that I'm just clicking on this and I'm hitting run. In fact, I was hitting shift/enter. But, that's shortcut to run.
13:18
So let's, for example here just to show you some code that we could change. We're basically saying that okay, bucket is this, object is this, let's change this to be real bucket is that, and run this, and you notice that it's real bucket. So this is a way that you can share a notebook with someone else, they can change your code Rerun it so that is kind of the advantages that data lab gives you 
