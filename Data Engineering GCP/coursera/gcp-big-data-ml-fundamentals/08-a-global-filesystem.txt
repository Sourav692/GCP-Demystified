In the previous section we looked at how to create a compute engine. Let's continue our journey into the low level infrastructure of GCP by now looking at storage.

Why are we talking about cloud storage? Well, one of the reasons that we are talking about cloud storage is that this is the way that you stage any input into a relational database which is Cloud SQL, into BigQuery which is a data warehouse, or into Dataproc which is a Hadoop cluster. So if you want to stage the data into GCP, you first have to get the data into cloud storage. So cloud storage is blob storage. So you are basically storing raw data in any format, directly onto cloud storage. In order to get there though, normally what you might want to do is to get this data from somewhere else. It may be in your data center, it could be on instruments out in the field, it could be logs that are being created. Tends to be that you're basically ingesting extracting this data. You're doing some processing, and that processing could happen on a compute engine. So you could have a compute engine VM and you're basically doing a whole bunch of processing. And if you need to do processing, you need very fast seeks, reads and writes of the data. And a good way to do that is to basically store that data on disk. The problem is that any disk, although you have persistent disks, let's ignore that for now. Typical disks are associated with the compute engine that they're attached to. When the compute engine goes away, the disk also goes away.

So in order to keep data persistent, the standard practice is to take this data and not store it on a persistent disk, not store it on a disk, because those tend to be expensive, but instead to store it on cloud storage. Cloud storage is persistent storage, it's durable, it's replicated. It can be made globally available if you need to. And you can use it to stage the data onto other GCP products. So often the very first step of the data life cycle is to get your data onto cloud storage.

So how do you get your data onto cloud storage?

The simplest way is to use a command line tool called gsutil. It comes with the G cloud SDK, so you can install the G cloud SDK and once you have G Cloud SDK installed, you will have the gsutil command line. So whichever machine you're going to be uploading the data from, install G Cloud, get gsutil, and then say gsutil copy, that's a cp, gsutil cp sales*.csv. That's a file that I'm copying. Where am I copying to actually files that I'm copying, where am I copying these files to?

Well, in this case I'm copying them to Google Storage, that's GS, and I'm giving them a full destructure /data, and I'm putting all of the sales files in /data. But /dataware, and that's where the concept of a bucket comes in. Loosely, you can think of a bucket as like a domain name, right. Or if you're on the Internet you have different machines, have different machine addresses, and then each of them has their own file system which is your web pages. Well, buckets are very similar to that. Acme sales in this case, plays the part of a domain name. It's this thing that makes it unique, that's your bucket. And you can create any number of buckets that you want and the bucket name that you provide has to be unique.

Most commonly, what people do is to make their bucket name have some relationship with their corporate domain name.

So you may have addresses that match the company name. But if you're basically going to be using addresses that match a company name, well GCS is going to basically say, well are you really this company, right? And so you need to prove that you own it, typically by changing a D name field etc. Similar to the way that you prove that you own a domain. So prove that you own the domain, and you get a bucket name that matches the name of the domain. In the case of a classroom like this, we want a unique bucket name. But we're not going to go through the bother of proving our identity, etc. So we'll just try to come up with a bucket name that happens to be unique.

So you can basically copy files, gsutil-cp. Besides that, you can also do remove, rm. mv is move, so that is essentially copy it and then delete it locally. You can do ls to list, so you can do listing of things on the cloud. The thing to realize is that this URL here, gs://acme-sales/data/ even though I explained it as like a folder structure, that's purely convenience, right. All that it really is, here's a name to blob. And this name is just a string. However, people tend to think of file systems as being hierarchical, and so you might tend to think of GCS as also being hierarchical. And that's kind of where ls comes in. You can basically go ahead and look at the hierarchy. But remember that this hierarchy is additional semantics that your placing on top of a pure key value store. You can also do mb to make a bucket, rb to remove a bucket, rsync which is a Unix utility, so this is an emulation of that Unix utility. Where you can basically have a mirror of something that's local up on the cloud, and then whenever you run rsync again, it's just going to look at the files that have changed and upload only those files. ACL is an access control list, that's the way you change permissions.

Even though I'm retalking about using gsutil as a command line tool, you don't have to use it as a command line. Because all that that command line tool actually does is that it invokes a web service, a REST API. So you can make that exact same REST API call yourself, so that's one way to do it.

The other way is that you could go to web console, the same way that we've created the compute engine. We could basically go to the, instead of going to the compute engine part of the user interface, we can go to the storage part of the user interface and you could use that. But because it's a REST API you can also use Python or you can use Java or you can use your favorite language, any language that can talk HTTP, which is pretty much any language. You can basically interact with cloud storage using the REST API.

And incidentally this is a very common theme.

Things that you can do in the command line you can do with the REST API, and because you can do it with the REST API, you can also do them from any language that you want. And anything that you do from a graphical user interface, also uses the same REST API. So in the previous lab, we created a compute engine and we created it by using the web user interface. But we could have also done it using a command line, and the way you do it with a command line is that you would have said G Cloud instants create, right, and that would have allowed you to create a computer Ged instance.

So talking more about cloud storage, in addition to using GS utility is a one shot, take this data, copy it to the cloud there. You can also set up at transfer service. The transfer service could be one time or could be recurring. So you could say I'm going to basically take all the data from here and transfer it over, and as new data shows up I'm going to keep transferring it. And the source of your transfer could be your local machine, it could be a local data center, something that's on premise, it could also be AWS with S3 brackets, you can transfer them and you could keep this transfer service going.

Now as we mentioned, the whole idea of cloud storage is that you use it as a staging area. So you can input it into Cloud SQL, into BigQuery, into Dataproc, into variety of the different analysis tools and data bases. You could also use that to take the data from cloud storage and move it to a local SSD disk on a compute engine VM, so that if you're going to be reading some data routinely all the time, you might want to move it from cloud storage into local.

Once you have your data on cloud storage you can control access to it at that object level.

But lots of times you will have objects that are related and they will be in the same quote unquote, folder structure or in a bucket level. So you can control access at that bucket level but every bucket belongs to a project, and a project is essentially the way you do billing, etc, in GCP. So essentially, when you create a bucket in a project, you are basically saying, which billing account is going to be responsible for paying for the storage, right.

So you can control access at that project level. You can say people who are editors on this project can also add and remove files from this particular bucket. But crucially one of the access control things that you can do is that you can actually make access control to all authenticated users. In other words, anybody who's logged in with a Google account, you can provide access to them.

The other way that you could do it is you could provide access to all users. And by providing access to all users, what you're essentially saying is that they don't even need to be logged in, they can just come and get the data that you have. And they could get that data using just an HTTP URL. Why would you want to do that? Well, remember what I talked to you about GCP. I said that once you put something on cloud storage, that thing is durable, it's persistent, it's also edge cached, it has multiple copies made of it. So in other words, this is an easy way to get a content delivery network going for your data. Just take your data, your static data, put it on GCS, and give people the URL to that GCS location. And Google Cloud takes care of doing the edge-caching, and replication, and reliability, and durability, and all of those kinds of things.

So, if you're going to be putting a data on the cloud, right. And actually, even if you are creating instances, computing the instances to process data on the cloud. You should have a good idea about what zone, what region, you want to be doing the event. So what is a zone? What is a region? Well, it's a geographic construct. You can think of a zone as like a data center. So you want to choose the closest zone region to where all of your users are. And the reason you do that is that you want to reduce latency, right. So if almost all of your users are in the central US, you want to use it, you want to choose the data center in Iowa, and you would use us-central to close the zone, to close the region just to reduce our latency.

But the problem with having everything in one zone is that what happens if that zone goes down, okay? If a tornado hits Iowa and the data center in Iowa is not able to be accessed, your application cannot be accessed either. So in order to limit service disruptions, you might want to have multiple zones within the same region. So for example, you might run your application in both zone b and zone f with zone f acting as a back up to zone b, for example. So distributing your absent data across zones is a way to reduce service disruptions.

This is good if all of your users are in Europe, or if all of your users are in the US. So you would use us-central if they're all in the US, you will use europe-west if they're all in the EU.

But what if you have a global application?

You have an application, we have some users in Japan, and some users in Europe, and some users in the US. If that's the case, then you need to distribute your apps and data not just within a region but across regions. And the reason you do that is to basically make your applications globally available. So bottom line, control your latency by choosing the closest zone or region.

Use multiple zones in a region to minimize the impact of service disruptions.

And use multiple regions to provide global access to your application.
