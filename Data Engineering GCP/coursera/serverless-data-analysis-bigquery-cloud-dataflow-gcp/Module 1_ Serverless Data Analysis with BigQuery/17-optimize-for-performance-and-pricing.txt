So the final part of it that we want to talk about is performance and pricing of BigQuery. So, in terms of performance, we're looking at optimizing queries. And a query is better if it's faster, if it does less work. So what constitutes work for a query? Work can be one of four things. It's the amount of data that the query is reading the input and output of the query. It could be the shuffle because any query will have a plan and the amount of data that basically gets passed from one stage to the next stage is the shuffle. And the lower the amount of data that's passed from one stage to the next, the faster the query is going to be. You also have to worry about whether anything requires materialization. If we can just hold it in memory between stages and keep going, such a query is going to be faster than a query that requires intermediate outputs or final outputs to be materialized for the bytes to be written out. And finally, you also have to think in terms of the CPU overhead associated with the functions that you're calling. Some functions, like sum or count, are probably very cheap. Other functions, such as sine or cosine, are a little more expensive. So, depending on the CPU overhead of the functions, some queries will take longer than others. So, how do you improve the performance of a query? First thing is because BigQuery is a columnar database. You want to limit the number of columns that you're processing. So, don't write a SELECT * unless you actually need every field. 

Excess columns are just going to involve a lot more input/output, a lot more

materialization, more use of the network, it's going to be slower. So don't project unnecessary columns in your query. Select only the fields that you actually want. Second row, because we want to limit the amount of data that's passed from one stage to the next stage, you want to carry out filtering or WHERE clauses as early as possible. So try to do your WHERE clauses as early as possible to reduce the amount of data that's parsed between stages of your query. Again, the more rows you're processing, the more data you're processing and therefore the slower your query is going to be.

When you are joining, consider the order in which you're merging the data. The guideline is that you want to do the biggest joins first, and do smaller joins later. So essentially, what you want to do is reduce the amount of data that need to travel through your query. And you can do that by doing your big joins first. Also, whenever you're doing a GROUP BY, you should think about how much data is actually getting grouped per key because GROUP BY is per key. So, for example, in the programming example, we grouped it by language, right? So, it now depends on how many files are there, how many commits are there per language – that's the cardinality of the group. The lower the cardinality, the faster the query is going to be. The higher the cardinality, the slower it's going to be. So, if you have a language, like say JavaScript which has lots and lots and lots of commits, that key, that JavaScript key is gonna be slow. If on the other hand you have a key of Haskell – and Haskell is a language that's not used that often, it's a low-cardinality key, and so anything associated with that key is gonna be a lot faster. But at the same time, if you have lots and lots of keys, so you have lots of GROUP BYs, then you basically are gonna be faced with a lot more shuffling and you will have a lot of tail latency, and what you mean by tail latency here is that you will have some keys that don't actually have lots of data associated with them. So, in other words, to understand the performance of a query that involves a GROUP BY, you should have a pretty good idea of how many rows each of the GROUP BYs is actually processing. So it's a good idea to think about the number of rows, and if you look at our query earlier, we were counting the number of commits, that is the number of rows. So that gives us an idea of the amount of data that each of those keys is gonna be associated with. And some keys you could avoid processing with a where clause. You could say, for example, process only those keys that have more than a 100 commits. So you could have having num_commits greater than 100, and that's one way to reduce tail latency. When you're choosing functions, you should think about what the CPU overhead of those functions are, and every function is going to be different. But there are a few guidelines. First of all, built-in functions are going to be the fastest. So if you have a sequel function that exists, you should try to use that. JavaScript UDFs will tend to be the slowest. A function that you write, that you define with Sequel is going to be somewhere in between. So you should try to use a built-in function when you can, write your own UDF in sequel, if you can, and as a last thing where you have no other option, then write a JavaScript UDF. Also, consider differences between functions that are similar to each other. For example, there are a variety of APPROX functions that are supported by BigQuery. An APPROX quintile will be faster than a regular quintile. An APPROX count will be faster than a regular count. So if an approximate result is good enough and approximate in this instance typically means that it's within one percent of the actual number. If that's good enough for your purposes then you should probably just use the APPROX. When you're doing your ordering, you want to apply an order only on the outermost query. Doing a order on and inner query makes absolutely no sense, usually, right. Because you want to do the sorting on the smallest amount of data that you can get away with. And that tends to be your outermost query. So, you want to filter first, and then you want to order.
