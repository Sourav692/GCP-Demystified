So why are we considering loading data into BigQuery? Primarily because BigQuery is the end of many roads in GCP. So wherever you get your data from, whether you're ingesting and capturing it from App Engine or through log files. Or you're coming in through Google Analytics, or you're receiving data in real time from a sensor out in the field with Pub/Sub. Inevitably, at some point, your data makes its way into BigQuery. BigQuery acts as your storage. It also acts as the data warehousing mechanism whereby you can carry out your SQL queries on your data. So you'd be ingesting your data. You'd be processing it with Dataflow, with Dataproc. You may be storing it on Cloud Storage. But very commonly, if the data are tabular in nature, you'll be storing it in BigQuery. So that you can do your analysis in BigQuery and then use all of the connected tools to visualize your data and to share your data around. So it's important to look at how to get data into BigQuery, because that's something that you'll be doing quite a bit Now you can load data into big query using command line interface called BQ. It comes with the Gcloud SDK. You can use the web user interface or you can use an API, a python API, a data flow API, pretty much all of the tools in Google cloud. We'll be able to talk to the query and we'll be able to write their data into the query. So rather than us just talking about how to do it, let's go ahead and do it. So go ahead and do the next lab, which shows you how to load and export data to and from the query using the command line tool BQ and using the web user interface. 

So we want to load our data into Big Query and in order to load our data in Big Query, the data I have is airport.csv. It's there in my local file, I want to load it into Big Query into a table in Big Query. The thing to realize is that you cannot directly load your data into a table in Big Query because all tables are part of datasets. So first thing to do is to create a dataset in Big Query. So I'll go to the project that I want to create the dataset in. Recall that the project basically serves as the billing functionality. And also the owners of the project will have rights on the dataset that you create. So we'll go to this project, I'll click on the right arrow and I'll say Create a New Dataset. And I want to give my dataset a name. This dataset name needs to be unique within my project. So let me just go ahead and call it demo_flights. And I'll say that I don't want it to expire, and then I hit OK. And at that point, I have my dataset called demo_flights, but it doesn't have anything in it. It has no tables in it yet, so I want to create a table in it. I'll go ahead and click the plus button, and I basically get the option to create a table. I want to create it from CSV files. I'm going to upload the file. So I'll basically say, I want to choose the file, it'll be in Downloads, airport.csv there we go. So there's airport.csv and I've chosen it my destination table name, let me just call it airports,

I can call it whatever I want, right, I'll call it airports. And I will ask to basically add the fields, so what are the fields in my table? It turns out the fields in this airport.csv file are,

So it turns out the fields in the CSV file, the first one is called IATA.

And it's a string. And we'll say that it's nullable, and I'll add the field, and at this point, rather than just type all of these over and over again, I can basically go into Edit as Text, there's IATA, it's a string. The next one is,

So I have others which are,

So I have the IATA string. I have the other columns, airport, which is also a string, city, which is a string, state, which is another string, country, which is a string. I also have latitude and longitude. So I'll go back to Edit as Fields. Add a field called latitude which is a float. And I'll go ahead and edit this as text again, so just as I have latitude as a float, the other thing I have is longitude which is also a float. And at that point, I have,

And at that point, I basically have all of my fields defined. My field delimiter is a comma, we don't need to skip any rows.

Actually we need to skip a single row because this fight has a header and we don't want any errors. So let's go ahead and create the table.

A second later, we basically see that the load has completed and my demo_flights has a field called airportsi, I must have typed an i in addition by mistake. So I have my table, I can do a preview of the table and see that the table has an IATA field, has an airport which seems to be correct. City, state, country with a latitude and a longitude. We can go ahead and try to query this table. So let me change this to be standard SQL and hide per options. And query the table again. And let's say that I want to basically find,

Airport, city, and state, where latitude is greater than 55 and longitude is less than -120.

And we learn that there're 18 such airports in the United States, seems pretty reasonable to me.

So that's one way to load data, right? You can use the web user interface pretty conveniently load data into it by following the prompts. The other way to lure data is to use the command line tool, BQ. And rather than install the G Cloud SDK on my local laptop, which I can. I'll go ahead and use Cloud Shell because Cloud Shell already comes with Big Q installed on it.

So I go to the Cloud Console, and select, My Cloud Shell instance. I'd select My Cloud Shell instance and on the Cloud Shell instance, let me just go ahead and make a directory called Temp.

Move into the temp and in there go ahead and download, I'm downloading the data using curl, download the JSON file. And at this point, I have JSON file that has the schema in it so I don't need to type up the schema. So once I have that schema, I will... So once I have the schema, I'm ready to use. So once I have the schema, I'm ready to use the command line tool BQ. So I want to use bq load, the source format is a newline delimited JSON from my project ID and I need to provide the name of my dataset and remember that I created a dataset named in my case, demo underscore flights. So we go to our dataset, demo flights and we specify the location of the data files themselves and the location of the JSON file that describes the data.

And in addition to the dataset, We need to create something, a table. Let's call this airports2 so that we don't conflict with the airports table that we already have and hit return, and just as before the data are going to get ingested. So this is another way to get your data into BigQuery, is to run the BQ command to load your data. So you can use a web console, you can use BQ, you can also use a python API. In the fundamentals course, we did this when we did...ok so you can use a python API, you can use dataflow, we'll look at different ways of doing this through this, throughout the course. But for now, let's look, these are two ways that we can do it. You could use the web UI and you could use the BQ command line tool.
