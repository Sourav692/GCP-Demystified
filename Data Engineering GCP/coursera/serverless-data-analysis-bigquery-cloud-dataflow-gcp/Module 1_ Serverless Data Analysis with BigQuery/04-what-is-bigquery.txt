So, let's start by looking at serverless data analysis using structured query language, or SQL.

So what we're going to do is, first we'll talk about what BigQuery is. BigQuery is the data warehouse that petabyte scale data warehouse on Google Cloud. And then we look at how to write queries, how to write functions in BigQuery. And go ahead and do a lab that takes what we've learned and demonstrates it in the form of sample code. And then we look at how to load and export data. Load data into BigQuery, export data from BigQuery, from tables in BigQuery into some other formats such as a CSV file. And we'll also look at some advanced capabilities that BigQuery allows beyond straight forward structured query language. How to do nesting, how to do repeated fields, etc. And then we finally finish off with looking at performance and pricing. And all along as we go through these modules, we will go ahead and we'll talk about what these. So as we go through these modules we'll talk about the basic concepts, but not just the basic concepts, we'll also then go ahead and look at those basic concepts in a realistic application. We'll go ahead and do a lab with it. A word of caution, all of these labs are going to be very structured. And, basically, they'll step you through writing a query, loading data, exporting data, etc. What I would recommend that you do after you do this course, is to go ahead and try to do it on your own dataset, to do something that you are interested in. So the labs here are primarily about showing you how it's done, but you will actually learn a lot more if you try to do it by yourself. So don't do the same examples as we are doing here, but perhaps pick some other examples. There are a variety of public data sets on BigQueries, I'll show you some of those as we go along. And then you can go ahead and try doing the exact same things that we're doing, but on that other query dataset. When we talk about BigQuery, BigQuery is part of the second generation of Big Data at Google. And what do we mean by that? Well, Big Data at Google started with the Google file system, GFS, and MapReduce. MapReduce is basically the idea that if you want to process very large datasets, you can't do them on one machine, so you take those datasets and you shard them, you split them up into small pieces and you put those small pieces on a distributed cluster of machines. And then when you need to do any kind of analysis on that data, you go to those machines and each one of those machines that has a shard of the complete dataset, you carry out a subset of the operations. So you basically parallelize what are called the map operations, so that each map operation on each compute node only processes its shard of data, and then you take the results from all of those map operations and you combine them in some way on a different set of machines, compute nodes that are called reduce operations. So these reduce operations aggregate the results out of all of the map nodes, and then that's essentially your result. So with MapReduce, it essentially means that you have this one prior step. The step is that you have to take all of your data and you have to shard it, you have to split it and store it on multiple machines. Now, that doesn't scale very well because now you've mixed up storage and compute. The amount of machines that you need to do your compute is gonna drive how you're gonna split your data â€“ and that's not a great thing because every time you need to do a compute job, you first need to basically figure out where the data are stored. And so that's what led to the second generation of Big Data at Google, starting with things like Dremel, things like PubSub. So these are ways in which you can autoscale and carry out your operations, and the system, the infrastructure figures out how many machines to do these operations on. So when you look at things like Dremel and PubSub and Flume, you're essentially looking at completely serverless, autoscaled ways of doing data analysis. Now, Dremel, that was used within Google, is basically what on Google Cloud is available as BigQuery. In fact, it's the same engineering team that builds Dremel that also builds BigQuery. So it's exactly the same product, one of which is internal and the other isn't. So what's the point of BigQuery? The reason that you use BigQuery is that it allows you to query very large datasets in seconds. How large? Well, the dataset could be as big as a few petabytes. And you want to basically do it, your query. You can run this query. So here's an example of a query, and we can run this query from a cold start. We have our data, the data are stored on the Google Cloud, and whenever we want to do an ad hoc query, we just write the query and we run it. We don't need to provision clusters. We don't need to create indexes. We don't need to do anything other than have the data on Google Cloud in a denormalized form in the form of BigQuery. Now, you can have your data, although I say denormalized form, you can have it as a regular tables. It's just that it's a lot more effective and efficient if your data are denormalized. So let's show an example of what this looks like. So I'll just take this exact same query. So I'll take the exact same inquiry and go to bigquery.cloud.google.com. That takes us to the Web console for BigQuery. We can also get there by your typical console. So we can also go to console.cloud.google.com and, once you're in console.cloud.google.com, you should be able to click on what I call the hamburger menu, right, on the on the left hand side. And by clicking on that menu you should be able to go down to the big data sections and there is BigQuery, and that also takes you to be bigquery.cloud.google.com. So that's essentially the same thing. That's what I just got here. And once you are there, go ahead and click on compose query, and let's say that this is a query that we want to run. I'll make sure that I'm using standard sequel. So I unselected the "use legacy SQL" part of that query. So at this point now I basically am querying this data set. It's a Medicare dataset of, so big query public data Medicare and I'm saying go ahead and find for every state. So I'm grouping by State. So for every State, I'm basically finding the total number of claims, as it in millions. I'm dividing it by one e power of six. So let's go ahead and run the query and remember that this is now millions of claims that we're analyzing. And in 2.5 seconds, we were able to basically process, this is not a huge dataset. A quarter of a gig of data. And we basically get the claim for every State. And just to show you that this is the kind of thing that it's very ad hoc. Let's go ahead and rather than order by total claim millions, so we know that California had the greatest ones. Let's go ahead and change it. And the way we can change it is that we can go find in this table. So that's in BigQuery public data. There is the medicare, so there is in Medicare Part D prescriptor and we see the schema here and we see that along with the schema, there is a drug name. So let's go ahead and find instead of just getting it by State, let's go ahead and do it by drug. And now this query runs, in again about 2.5 seconds, we learned that the drug Lisinopril is the most claimed drug in Medicare. And you'll notice that again I didn't have to index anything. Right. I just, the data are there and I were able to do an ad hoc query on it, and we can do a query on it extremely fast. And we've paid for essentially 550 megs of data. Right. That's it. You pay for the amount of data that you process in BigQuery and that's pretty much it. Okay so this is the, you know... In essence we've paid for the compute nodes that we have used for 2.4 seconds but because that's kind of hard to get our head around, BigQuery charges us by the amount of data processed. And so we've just, we just pay for the amount of data that we process, which is essentially based on the number of columns that we process. And in this case there are two columns that are processing, the drug name and the total claim count.
