So BigQuery is a huge part of all the kinds of data analysis that you do on Google Cloud. So here, for example, is a typical architecture that you might do for Data Analytics. In this case, this is a gaming architecture, but this is a very common theme, a pattern, if you will for data analytics on to the cloud. So in this case, with games, the idea is that basically every time somebody plays a game on a mobile device, real time events gets created, this person has crossed this level. This person has clicked on this thing, or this person has made in app up purchase, this person has called this amount that this person has conquered this whatever, right. So each of those events then gets sent to a gate server, which is running on app engine in this case, and app engine is basically taking those events and basically putting them into Cloud Pub/Sub. Pub/Sub is an autoscaling message queue. So it all goes into the topic in Pub/Sub, right? And then the messages and pub/sub are consumed in real time by data flow. And data flow, again, is an autoscaling data processing pipeline, and so it basically processing these messages. If there are more messages, there are more machines to process them. If there are fewer messages, there are fewer machines to process them. And then from data flow, a streaming pipeline, data flow is probably creating aggregates that saying that, over this period of time, this is what this user has done. So it's condensing the data that's being created from these mobile apps, maybe per user, per time slot, per region, per domain, whatever is doing this aggregates, and it is writing the aggregate into Big Query. 

Now cool, now you can basically take those Big Query aggregates and do stuff with them. But how do you know what kinds of things to do with them? The way you know what kinds of things to do with them often. When you build tools is you have to go look at historical data.

Now, we want to build historical data traditionally, the data pipeline for historical data is very different from the data pipeline to process real time data.

But not so in pub/sub, because cloud data flow lets you process both real time data and batch data with the same code.

So in this case historical data is loaded from Cloud storage

into Dataflow, and a batch pipeline that is probably running once a day, for example, is doing essentially the same kinds of things and is writing them into BigQuery, creating a data warehouse. And so regardless of whether the data comes in through a streaming pipeline in real time, or a batch pipeline based on historical data, it's all in BigQuery and BigQuery, as we talked about, is connected to a variety of business intelligence tools. So you can basically analyze it, if you're a data scientist, you can analyze the data lab. If you are a business kind of person, you want business types of charts, you can analyze them with tools like tablo and Q-Liv. You can basically take it from big query and use it to populate a Google sheet. As we talked about, you can take big query table, your data set, your query, and you can basically send links to those to your coworkers. And that's basically, the way you tend to do analytics and in Google Cloud. So BigQuery's a huge part of it. So how do you work with BigQuery, right? The way you work with anything on Google Cloud is from a project. A project, of course, is what sets billing, right? It's basically knows where do we send the check to when you run a query on BigQuery. When you load the dataset into BigQuery, where did you send the storage bill to? It goes into the project that you've created the dataset in. So, the project provides billing information. It contains users â€“ there are people who belong to a project, they have owner rights to anything that's created in the project. So typically you don't want to have one owner, you will have a few owners so that if that one person leaves, other people can use that project, et cetera, so you have multiple owners in a project. And then in the project, you basically create a dataset. A dataset is basically a collection of tables, right? A dataset is a collection of tables belonging to an organization, and you basically do access control on a dataset basis, not on a table basis, because you'd want to join tables together typically within a dataset. So you don't do access control on a single table, you do access control on a dataset that consists of multiple tables. And then whenever you want to do a query, that's a job, and the query's imports, exports, copies, those are all jobs that actually interact with this dataset in some way. So the access control is through this data set. So a data set contains tables, will also contains views. A view is a live view of a table. So essentially you can think of a view as a query, right? And that query returns out result set and then you can write a query to process that view, Just like you're writing something on a table. So you can use views as a cool way to actually restrict control to your data set. Because a view, basically, is a select, right? It's a select, and you can select specific rows and specific columns out of your data set, out of your table. And you can, basically, make that a view in a separate dataset, and that view, because it's part of a new dataset, it can have its new access control. And that access control might be a lot more limited. You could say, here are columns, and here are rows, that people using this view don't have access to, because the view has done a select and restricted it.

Tables are basically collections of columns. The way I'm saying this is very important. A table is a collection of columns. And the reason we say a collection of columns is because BigQuery's a columnar database.

Views themselves of course are virtual tables, they're defined by a SQL query. And tables, even though we normally think of tables as being part of BigQuery itself, BigQuery is a really a query engine that separates storage and compute. So BigQuery's a query engine. BigQuery also comes with its own native storage format. But the BigQuery query engine can work with tables that are external to BigQuery. So a table could be on cloud storage, the table could work on a Google Sheet.

And then finally a job is something that works on the dataset. It is potentially long running. So it could take a few seconds, it could take a few hours. And you can cancel a job. And then when you cancel a job, what happens to cost? Well, you get charged for the amount of data that you have processed so far. So remember they said, BigQuery, the storage is columnar? Well, no, it is columnar. A typical relational database, you tend to think of it as rows of data, records of data. And you basically use it that way because you want to support transactional updates. Well, BigQuery lets you update it. You have ways of deleting rows and updating rows, etc. But that's not the primary use case for BigQuery. BigQuery is not a transactional storage. Well, the way BigQuery works is that every column Is stored in a separate file. That file is encrypted, it's replicated, it's compressed. And because it's stored as separate files, we don't need in this instance you don't keys, you don't need partitions, right? You can use partitions to basically save on cost. We'll talk about that later. But BigQuery, essentially, works column-by-column basis, so one of the ways that you can save money with BigQuery is to, basically, limit the columns that you run your queries on. You basically get charged for the number of columns of data that you're processing.

And BigQuery is primarily meant for immutable, very large datasets.
