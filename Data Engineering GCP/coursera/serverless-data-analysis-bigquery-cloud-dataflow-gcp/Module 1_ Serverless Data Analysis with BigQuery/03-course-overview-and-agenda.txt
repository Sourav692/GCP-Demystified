Welcome to our class on Serverless Data Analysis. In this course, you will learn how to carry out no-ops data warehousing using Bigquery and pipeline processing using Cloud Dataflow. So we're going to be talking about Google Bigquery and Cloud Dataflow. Google Bigquery is a data warehouse that you interact with primarily through SQL and Cloud Dataflow is a data processing pipeline system that you can program against in either Python or Java. So this class is meant for people who build data pipelines, people build data analytics. So in order to do this class, you need to have knowledge of SQL because you're going to be interacting with Bigquery and you need to know either Python or Java because we're going to be looking at Dataflow. So our agenda here is pretty straightforward. We will first look at Bigquery which is the no-ops, and no-ops in this context essentially means that there is no infrastructure for you to manage. So, no operations. So we'll be looking at Bigquery which is the no-ops data warehousing and analytics system. So it's a place that you can store your data, analyze the data, and export your data from. We will look at how to do queries, functions, how to load and export data, and we'll also look at some advanced features like nested and repeated fields, how to do window functions and how to do user defined functions. And along the way we'll do a few labs on queries and functions, on loading and exporting data, and on demos. And I'll do a few demos. Then we'll move on to talking about Cloud Dataflow which is also no-ops in the sense that you don't need to manage any infrastructure. But here, you're going to be writing programs that process data, and the kind of processing that you tend to do can either be on batch data or it can be on streaming data. And as we will see, in Dataflow, you get to write the exact same code. It works on both batch and stream. So if you need reliable scalable data processing on GCP, the best option is Cloud Dataflow. So we'll look at Dataflow. We'll talk about what a data pipeline is. We will talk about how you could do MapReduce which is the most common type of big data algorithms that you might be doing today. We look at how you would do those kinds of algorithms in Dataflow. And with that, get the benefit that you can work with real time data and historical data in exactly the same way. We will look at what are called side inputs. For the most part, when you're writing a data pipeline, you'll have this one massive data set that you're processing as it comes in and you're writing things out. But usually, along with that big data set, you will have other smaller data that you need to get and join with. Those smaller data sets are called side inputs. So you look at how to deal with side inputs in Dataflow. Then we will also look at streaming because again, one of the key advantages of Dataflow is the idea that you can process batch data and streaming data the same way. So, having looked at examples of used cases that used batch data, we will then look at how to do streaming data and Dataflow. And the code itself would be very similar to the way you deal with batch, with a few extra concepts that come along because you're dealing with but infinite data, unbounded data. In terms of labs, we will look at how to create a simple pipeline, how to write MapReduce jobs, how to incorporate side inputs and along the way, we'll also do some demos. Quick introduction to myself, my name is Valliappa Lakshmanan. Everyone calls me Lak. I'm at the professional services org of Google. I have a Ph.D. in Electrical Engineering. I worked for a long time on weather research, sub-building data pipelines for doing real time weather prediction and real time weather diagnosis, so I built a lot of machine learning models for that. And my goal here is to talk to you about how you could deal with very large data sets, both from a declarative way with Bigquery and the programmatic way with Dataflow, such that you don't need to spin up any servers or manage any infrastructure. You can essentially write your logic. Your logic can be sequel statements or they can be Python or Java programs and you can get your logic executed on the cloud against massive data set, small data set against data sets and not have to manage infrastructure. It can be extremely liberating as a data scientist and as a data engineer if you can get to focus on your data. And the two key products and GCP that let you focus on your core logic and leave the infrastructure management to the cloud provider, the two key products are Bigquery and Dataflow. And that's what we are going to be looking at in this course. 
