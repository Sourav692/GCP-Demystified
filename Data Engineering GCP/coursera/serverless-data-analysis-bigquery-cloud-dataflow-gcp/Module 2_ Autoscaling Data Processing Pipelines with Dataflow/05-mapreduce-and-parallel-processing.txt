Now that we've looked at how to write a basic pipeline in Dataflow, let's look at how to write a more useful pipeline, something that actually does useful things with it. And one of the common things that we do in Big Data is to take a map produced approach. The idea behind map produced is that if you want to process a very large data set, you break up the data set into pieces such that each compute node processes data that's local to it. So for example, if we need to manipulate this data somehow, we might take that data and break it up such that this node carries out operations on a third of the data, this node carries out operations on another third of the data, and the first node carries out operations on the remainder of the data. So the map operations happen in parallel on chunks of the original input data. The result of these maps are then sent to one or more reduce nodes. There is a shuffle, if you have multiple reduce nodes, each of those reduce nodes processes one key or one set of keys. So the output of the maps get sent to the appropriate reduce nodes, and each of the reduce nodes then calculates the aggregate, maybe a count, maybe a sum, maybe a mean, on the values for that particular key and that's basically what gets written out. So the idea behind MapReduce is, A, you take your big problem and you break it into two kinds of steps. Steps that have to be done in parallel on all of the data. Those are your map operations. 

And then, aggregations that have to be carried out on many roles at a time and that's your reduce operations. So your map operations are highly paralyzable. Your reduce operations tend to be things that you compute on many rows at a time, aggregates that you compute on many rows at a time. But usually all of the rows have a common key to them. So for example, we may have a bunch of tax data that we need to process. And we would take all of the tax returns that we have in our system and we would basically go through each of those tax returns and might, maybe calculate the marginal tax rate on each of those returns.

And then, we might emit out of the map node the state and the corresponding,

so the state of Alaska and how much they returned. And the next person might live in Washington and what rate they paid. And the next person might live in California and what rate they paid. And one more person in California, one more rate that they paid. And this is basically what's going to come out of the map, so that data is all of the tax returns. You've split them up such that a few of the tax returns got processed by one of the nodes, a few others by the second node, and a few others by the third node. And having then each of these tax returns are going to get processed by these map nodes. And for every tax return you're computing a marginal tax rate. Something you're computing from that tax return and you're emitting the information, for example, Alaska 31.2%, Washington 31.8%, Washington 32.3%, Washington 19%. We may have multiple people living in Washington, for each one of them you are essentially emitting the tax rate. And then the reduce node, essentially what it would do is that it might be computing the average tax rate paid by all of the people that we have in our database who live in the state of Washington. And then, you may have multiple reduce note where, for example, everybody from Alaska may be processed on one reduce node and everybody from California might be processed on another reduced mode. And that's the basic idea behind a MapReduce approach, right. To basically take a large data set break them into pieces and process them. Now, you can do everything that you do in MapReduce in Dataflow. So, in Dataflow, the parallel processing, which is done by the Map operations in a MapReduce, is carried out using a parallel do. So, a parallel do acts on one item at a time. So in our tax form example, the parallel do, the transform inside the parallel do will be the transform that processes one tax return. Now there are gonna be many instances of this class because we may have millions of tax returns that we need to process, which means that you have lots and lots of classes. Maybe you have 14 different instances of your tax processor, which is your Map operation. Now, each of these Map operations needs to just process the data that it is given and emit the data out. It should not keep history. So the map operations are not maintaining any state, you don't want your map operation to compute the average tax rate across users. Instead, you want the map operation to only calculate information that it can calculate from a single return, right, from a single tax document. So that's essentially what a parallel do is. So, that's exactly what a Map is, right? So a map is processing one item at a time, so does a parallel do. So what do you use it for? You can use it, as we just talked about, to process one item at a time. You can also use it to filter. So you get a tax return, and you emit the tax return only if that tax rate was greater than 10%, because we don't want to include students and juniors and teenagers etc. in our calculation, for example. So we might filter our data by the age of the customer or we might filter the data based on the length of employment, right, so that only full-time employees are considered. So that's essentially what filtering is – you're choosing which things that you want to emit for the next step of the processing pipeline. It can also be useful for convergence. So let's say we want to write out the average tax amount, right, and you ought to write out the tax amount – it's a floating point number – but our output, for example, TextIO wants a string. So what we will do is our input might be a parallel collection of floats, my output will be a parallel collection of strings. It could also be useful for extracting parts of your input. Let's say we have our tax return again, and what we want to do is we want to just extract the overall income of the person from their tax return – so you're extracting a part of the input. So, the input is a parallel collection of tax documents, the output is a parallel collection of floating-point numbers, where the floating point-number is the salary or the income of the person that's been identified in the tax document. You can also use it to calculate values from different parts of your inputs. This was my original example, where you have your tax document coming in and you want to use the tax document to calculate the percent of income that this person paid in taxes. So you might have two pieces of input that you're extracting from the tax document, you might be extracting their salary and you might be extracting the final tax amount and you're dividing one by the other and that is what you're emitting. So a parallel do is useful as long as you're processing one item at a time. Recall that all of my examples here, whether I'm choosing which tax document to have it move on, whether I'm converting a float to a string, whether I'm extracting the salary from a tax document, whether I'm computing the average return from a tax document, all of those are on one tax document. That's a point of a parallel do, a Map operation, operates on one item at a time.
