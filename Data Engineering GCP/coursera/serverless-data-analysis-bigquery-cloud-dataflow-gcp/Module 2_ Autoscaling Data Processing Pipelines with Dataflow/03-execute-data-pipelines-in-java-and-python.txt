To ingest data into a pipeline, we need to read data. So we can read data from text, from big table, from BigQuery, from Pub/Sub - from a variety of different sources. So from text, you basically do a TextIO.read from some file on cloud storage but it doesn't need to be a single file. You could have a wildcard in here. So you're reading from a bunch of files on cloud storage, and this lines now represents the PCollection of lines that are read from these files, whether it's one file or multiple files but it's a PCollection. So what that means is that it's not all in memory at the same time but you will get to process all of these lines in your data pipeline. Now, the other input instead of of TextIO, you could have pubsubIO, in which case you're reading from an input topic. Or if you you may be reading from BigQuery, so you could have a query and you could say BigQueryIO read from the particular query passing in the query itself and what you will get back here is a PCollection of TableRows. So you can go to the table row and you can get the value for a specific column. Now, whatever you can read, you can also write. So, you can write into TextIO. So, TextIO.Write.to/data/output with a suffix. Now the thing to remember is that data flow is meant for big data, streaming data, so what this means is that when you write things out, they are going to get sharded. They are going to get split and written out into multiple files. But what if you're just writing just a really small file? You're writing a file of 100 numbers. It can be annoying to basically see the sharding syntax. 0 of 36, 1 of 36, etc. So if you don't want sharding of your output, if you want the output to explicitly go to the file that you set, /data/output.csv right without the 0 of 36 and 1 of 36 kind of stuff, then you can add it withoutSharding. But remember that whenever you do the withoutSharding, you essentially forced all of the writing to happen on a single machine. So definitely not the recommended thing to do but if you have a really small file and you want to control the naming of this file in a very deterministic way, then you can use withoutSharding. 

No. Now, TextIO only writes out strings. So if you have a PCollection of something else, if you have a PCollection of integers, if you have a PCollection of anything else, PCollection of your user defined objects, then what you will have to do is that you'll have to take your PCollection and then transform each of the elements in that PCollection to a string, so that you get back a PCollection of strings, and it's that PCollection of strings to which you can do the apply of a TextIO. To run the pipeline, you essentially just run the main program, right? Typically your pipeline is in a main and you just need to execute it. So you can take Java minus classpath, provide all of the classes, classpath, all the jar files, and then the name of the class, where the name of the class, the fully qualified name, com.google.something. Now, it can get very tiresome to keep typing in the classpath each time and the classpaths can be really really really long. So a better way to do this will be to use Maven because Maven will also take care of downloading dependencies for you and managing them. So use Maven if you are using the Java version of data flow and when you are using Maven you can execute the program by saying mvn compile exec:java where exec's main class is this fully qualified name, com.something.something. By default this runs locally, right, the default runner is a local runner. If you want to run it on the cloud, then you need to submit the job to cloud data flow on GCP and the way you could do this is you do the same Maven commands as before but you add a few things. You specify a project. And the reason you specify a project is because a project controls billing. Right, so because data flow is going to launch some compute nodes for you, we need to know where to send the bill to. So that's where the project is. In addition, data flow will have to take all of your code and stage it on the cloud storage bucket. So you provide the staging location and you also provide a temporary location. Actually that's optional. You can provide either the staging or the temporary but you can do both. They can be different. And then finally you specify a runner and the runner here is DataflowRunner to say that you want to run it on the cloud. To execute in Python, exact same thing, you need to run the main program. So you can say Python./grep.py, where grep.py assuming is the code that contains your main that actually has your Python pipeline. To run it on the cloud you need to do very much the same things that you had to do in Java. You need to specify the project, the staging location, the runner etc. and they work exactly the same way. In Python you have to also specify a job name and the job name should be unique. You should not have submitted a job with that name before. So probably the best thing to do is to have like a date as part of your job.
