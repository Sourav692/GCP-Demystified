So let's look at how to write such data pipelines, data pipelines that work both on batch data and on streaming data. Now, before we get started writing the data pipelines ourselves, let's talk about a few concepts. So we looked at this pipeline earlier. This is a pipeline; so a pipeline is a set of steps and this entire pipeline together, now all of these steps, they get executed on the cloud by a runner. So, Dataflow is a runner, is an execution framework for this code that we have written. The code could be in Python or Java, so we have Apache Beam code that we are writing, and the steps are all elastically scaled so they can be scaled and run on as many machines as possible. So, we have a pipeline, and the pipeline is a set of steps. Each of these steps is called a transform, and this particular transform, its source is BigQuery and its sync is Cloud Storage. So a typical pipeline goes from a source to a synch, involves branching, involves a number of transforms, and each of those transforms – so, say for example, this transform that says needs help. What is the input to that transform? The input to that transform is a parallel collection, OK, it's a PCollection. So each transform on the pipeline, its input is a PCollection. So when you do an apply, you're doing an apply to a parallel collection or a PCollection. A PCollection is just like a list – it's a collection – so it's a list of items, for example, or it's a map of items, if it's a key-value pair. But you essentially have a collection, but that collection need not be bounded. That collection need not fit into memory, that's why we're calling it a PCollection. So this is a PCollection, and then you are doing your apply in your transform to each element of the PCollection. 

So a Pipeline is a directed set of these steps, of these transforms. A typical Pipeline reads in data, does some transformations to it, and then writes it out. And in between it can do branches, it can do merging, it can do anything that code can do, because it is code. So here is an example of a Pipeline. This is Java. I'll show you Python in a little bit. So here I'm creating a Pipeline. And having created the Pipeline, I'm starting off by saying TextIO.Read.from cloud storage. The first step of my Pipeline reads its input from the source. The source here being TextIO. And then it applies the next step is an application of a transform. Here it's called count words, and that's going to be a Java class that does a transform, that probably counts the words in my input file. And then the next transform is to write things out to the sync, so I'm doing TextIO.write. Now, at this point, all I have done is that I have created the directed graph. I've not yet run it. So we've created the graph, and we even want to execute it. We say p.run, and that execution now happens on the runner that we specify. So part of the command line args here is going to be what runner do you want this Pipeline to execute on. And the runner could be what's called a direct runner, in which case it's running locally, right on your laptop. Or it could be a dataflow runner, in which case, this graph gets launched on the cloud, and all of the compute is now happening on the cloud. The Python API is very similar. We are still reading from a source, we're still reading from a source, applying a transform, and writing to a sink. So in this case, I'm reading from text and then I'm applying a map that for every line of the input, I'm counting the words. So I'm for every line of the input, I'm basically calling up function count_words and the result of the count_words function is what I'm then going on to write to text. And the pipe operator in Python has been overloaded to mean apply. So, essentially, we're applying one transform after the other just as we did in Java, but instead of the.apply, in Python, we're essentially using the pipe operator to carry out one transform after another. And just as in Java, we first create a graph, and then we run it.

The data in a pipeline, as we talked about, is represented by a parallel collection, so every transform is applied to a parallel collection. So the input to a transform is a parallel collection, the output of the transform is a parallel collection. So if you are to take a parallel collection and we didn't apply, what we get back is a parallel collection. So in this case, for example, after we read this from text, what we get back is a parallel collection of lines. This is the same whether it's Java or Python except that in Java, because Java is a type safe language, you explicitly define it. So we know that lines is a parallel collection of strings. Now that we have a parallel collection of strings, we can now apply the second transform to it. So I'm going through all of the lines, and I'm applying a transform, and ignoring all the Java boilerplate, what am I doing? For every line that comes in, I'm computing the length of the line and I'm outputting the length of the line. So the input to the transform, which is the line, the output of the transform, which is the length of the line. Now because this is Java, there is a bit of boilerplate here, and what I'm doing is I'm calling lines that apply parsing in a do function. This is an anonymous do function. In other words, I'm inheriting from do function and implementing a method called process element with this annotation add process element, and taking this do function and saying that this needs to be run in parallel. So at this point then, for every line in my input, I have a corresponding integer, and the sizes now represents that parallel collection. Again, a parallel collection doesn't need to be in-memory, it can be unbounded, because it may be streaming data that you're reading, and so you're going to get this parallel collection of integers, that you can then do the next thing with. And the next thing might be to write it out, but the next thing might be to do something with these integers. It might be to compute the moving average of these integers over the past 60 minutes. In Python, the code is very similar, the concepts are very similar. All of the data is represented again by a PCollection, 'P' for 'parallel'. It is not in memory collection, it can be unbounded, but the code is a lot simpler. So where we said 'line is p.apply', here we basically say 'p with a pipe symbol'. And so now we get back our lines. We want to apply a transform to those lines, we say lines, the pipe symbol again, and what we want to do is we want to do a map and for every line that comes in, lamda line, for every line that comes in, return the length of the line. Now this map itself, if you look in the previous slide in Java, whenever we apply the transform, we can also provide the transform and name. And the reason that this name is useful is that this name becomes what you see on the monitoring console. So that's not the only reason for the name. There are other advantages to having the name. One of the neat things that dataflow will let you do is that you can have a running pipeline, and you can change the code and you can essentially replace the running pipeline. Why is that important? Well when you replace a running pipeline, you don't lose any data. So if some of the data got processed by the old pipeline and any data that wasn't processed by the old pipeline will get processed by the new pipeline. You can obviously see the advantage if you're processing streaming data, right? But in order for that replacement to work, your transforms have to have names - unique names. So best practice, always, whenever you write a transform, give it a name that's unique within the pipeline. So I'm giving it a name. In Python, the way you give it a name is that you give it a name length and then you have this overloaded greater than greater than operator. That means that this length name is being applied to this particular map. So you have two overloaded operators: the pipe symbol and the greater then greater than symbol
