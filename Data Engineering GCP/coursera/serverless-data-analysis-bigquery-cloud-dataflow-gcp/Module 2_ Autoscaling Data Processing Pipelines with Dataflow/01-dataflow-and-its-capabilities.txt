Autoscaling data processing pipelines. In this chapter, we look at Cloud Dataflow, which is a way to execute Apache Beam data pipelines on Google Cloud platform. We look at how to write such data pipelines and how to carry out MapReduce kinds of programs in Dataflow and also some concepts such as how to deal with side inputs and how to carry out a streaming. 

So what is Dataflow? Dataflow is a way by which you can execute data processing pipelines on the cloud. So in this case, for example, we're using Dataflow to carry out a pipeline that reads from BigQuery and writes into Cloud Storage. And it does so in a series of steps.

And the key thing about Dataflow is that these steps, called transforms, can be elastically scaled. So, for example, if it turns out that one of these steps needs to get executed in parallel on 15 machines, then it can get auto-scaled to those 15 machines. And the results can then move onto the next step of the transform, which itself may get scaled only to five machines. So you essentially have something that's elastic, that each of whose steps are automatically scaled by the execution service. The chord that we write is in an open source API called Apache Beam and dataflow is not the only place that you can execute a Apache Beam in pipelines. You can execute them on Flink and Spark et cetera, but we will look at cloud dataflow as the execution service for when we have a data pipeline that we would like to execute on the cloud. So the way this works, is that you first create your pipeline and then you do a series of applies. So in this pipeline I'm reading from TextIO and I'm reading from Google Cloud storage. That's why there's a GS and then doing some kind of filtering, some kind of grouping, some kind of filtering, some kind of transformation and then writing those results out to another storage file on cloud storage. Now each of these, Filter 1 group 1, Filter 2 Transform 1, all of these steps are user defined code. This is code that we write. In this case because this is Java, each of these are Java classes and Filter 1 exhibits something that's kind of neat, is that it's not just that we're applying Filter one directly, we're applying Filter one within the context of a Parallel Do. And what this means is that Filter one is going to get auto scaled, run on a whole number of machines scaled out, and then the results of Filter 1 are going to go streaming into Group 1 and the results of Group 1 again are going to get applied to Filter 2 but Filter 2 is again going to be completely parrallelized and run in parallel on many machines and the results of Filter 2 are going to stream in just Transform 1 and the results of Transform 1 are going to go into the output file.

Now my use of stream there was explicit because what data flow lets you do is that it lets you write code that processes both historical batch data, data that's complete, what we'll call bounded sources and syncs. As well as data that is unbounded in nature. So we may have a data pipeline where we want to do Filter1, Group1, Filter2 and Transform1. And in the previous case, we applied Filter1, Group1, Filter2 and Transform1. We were reading from text and writing to text. In this case, we are reading from Pubsub, which is a messaging system which can work in real-time, right? So you're reading from Pubsub, and then you're writing back to Pubsub.

But then, the question becomes, when I do a group, how can I do a group of something that is unbounded in nature. Well, you cannot, because you do not know if you are going to get some new member of the group. So anything that you calculate, let's imagine that you are calculating an average and mean, the mean is going to change over time as new data come in. So, when you're reading streaming data, typically what you do is that you also apply a window to it. So in this case, we are applying a sliding window of 60 minutes. And then, each of these groups, each of these transforms if you are doing a mean, it is a mean within those sixty minutes, in other words it is a moving average, right? So if you want to do things like moving averages on real-time data, on streaming data, this is basically what you do. You change the input and output to read from something that's unbounded, so, for example, from Pubsub So, Dataflow lets you ingest data both from batch and from stream, use the same code, the same filtering, transforming, grouping code to carry out operations on both batch data and on streaming data. Of course, when you do a mean on batch data, you would get a mean over the entire dataset, whereas if you're doing means on a streaming data, you will have to do a window, a window of, for example, 60 minutes, but you could do windows based on other things, like the number of records, et cetera. Of course, you can apply the same windowing transforms to batch as well. So that's the whole idea, is to apply the same code that processes streaming data is also the same code that processes batch data.
