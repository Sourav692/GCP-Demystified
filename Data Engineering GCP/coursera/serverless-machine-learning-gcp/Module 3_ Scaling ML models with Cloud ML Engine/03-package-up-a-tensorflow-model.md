So in order to do this, we will tak our TensorFlow model and we want to be able to scale out our TensorFlow model on the cloud. So we need to submit our TensorFlow cord. And the way you submit your TensorFlow cord is that you take your TensorFlow cord and package it up as a Python module. So what this light shows is the folder structure that Python expects for something to become a library. The key thing here is that every directory has an init.py. So we're going to put our task.py and model.py. These are going to be the two files that we write. So all of the TensorFlow code that we wrote in the previous chapter is essentially going to go into task.py and model.py, actually mostly into model.py. And because it's in a directory, we create an empty file called init.py. And that, we'll basically put into this directory structure.
1:02
And at that point, once we tar this up, you will get a Python module.
1:09
Now what's in task.py? task.py is essentially a way by which we will take our code and have it be executable as a program. So if you have a main and if you take the command-line parameters and use those command-line parameters to set up things like the training data path, the number of epochs, etc. So for each of those, you would just use arg parse. The standard Python library to parse command-line arguments. And you would say parser.add_argument --train_data_paths. And make sure that they provided number of epochs, etc. And once you get those things, those are the train_data_paths and eval_data_paths that you will use when you construct your experiment.
1:57
So that you can create your training input function and your evaluate input function.
2:03
But you'd have to write those functions. Those functions will me in model.py. So you will have it get_eval_metrics() function for example in model.py. So in model.py is where all of the code that we wrote in the previous chapter, in terms of the estimator API, in terms of the input functions and so on, that's where they go. So for example, here is an input function that we want, so it's given a file name, see this is something that creates an input function. So we will go ahead and match our file names once, create the filename_queue, create the TextLineReader. Go ahead and decode the CSB, return the features and labels. This is essentially the same input function that we wrote in the previous chapter that just goes into model.py. So now we take all the code, we put it to model.py. We write a main in task.py to call the model.py to basically create an experiment and call learn runner. And at that point, we now have a Python package. It's a good idea to try it out to make sure that it works. And you can run a Python package from the command-line using python -m, where -m is a module. And we need to tell Python where to find its modules. So in order to do that, we set the PYTHONPATH environment variable. So you've set the PYTHONPATH environment variable. You passed in the module name. You pass in the train_data path, the eval_path, the output_dir. So these are all things that we're going to use arg pass to get out. The job-dir is something that Cloud ML requires. So in your program, you would basically take a job-dir because Cloud ML is going to give you a job-dir. You can ignore it or you can use that job-dir as your output-dir. But it is essentially where Cloud ML is going to export things out once it's done training. So you can specify the job-dir and make sure that it works.
4:12
Once you know that your Python module works,
4:16
then it's a question of, how do I now scale this thing out? Because even when you run it as a Python module, you're still running it on your local machine. In order to scale it out, we will use gcloud. So we'll say gcloud ml-engine, so we're now submitting to cloud ml-engine and so let's look at the second one. So gcloud ml-engine jobs submit. So we are now submitting a training job, giving it a job name, telling it what region the compute node should be in. This tends to be the same region where you have your data, because you want your compute nodes to be in the same data center as where your data exists. Specify the name of the module, name of my module was called trainer. And it had task.pi which had the main. So I specify the module name, specify the output dir, which is the job-dir. Specify a staging bucket where our temporary files need to be stored and specify a scale-tier. The scale-tier essentially controls the kind of resources that you want this program to take. So you have BASIC, which is essentially, it changes over time. But it's a least number of machines, three machines. Or you can try standard, which at the time I'm doing the recording, is like ten machines. Or you can do premium, which is equivalent to 75 of those, or you can do GPU, etc. So a scale tier essentially controls the amount of resources, not necessarily the number of machines. But you can think of it as a number of VMS, the amount of disks and all of those things that are there all by controlled in terms of very simple like tiers. So you have basic tier, you have a standard tier, you have a premium tier, you have a GPU tier. And over time, you will have more tiers, and there will be different costs associated with each of these. So this is the way you submit a job to the cloud. But before you submit it to the cloud, let's say you want to try it our locally. You can try it out locally by, instead of saying gcloud ml-engine jobs submit training, you can say gcloud ml-engine local train. And then, you can just pass your path to a local passage in a local module name. And then everything else is exactly the same as before, and you can see if it works.
