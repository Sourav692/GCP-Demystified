0:00
So, we are now talking about the right-hand side of this diagram. So, we have a TensorFlow model, and we're talking about running TensorFlow at scale using Cloud ML Engine. Now, many machine learning frameworks can handle toy problems. So, if you have a problem where your entire data set fits in memory, you have lots of options and you can write your machine learning code in pretty much any of these frameworks and they will work just fine. So you take your data, it all fits in memory, you train your model and you have your model. If this is all you need to do, you have lots of options. But, if your data size is large, so as your data size becomes much larger, then you need to take your data and you need to be able to split it into smaller batches. You need to be able to run your model on the cloud on many machines. So batching and distribution become important if your data are larger than can comfortably fit in the memory of a single machine. But in addition, you don't just take your data and train your model, you actually tend to do a few things. You tend to do preprocessing of your inputs. You may take your data, you might do quantization of it, we'll talk about why you may want to do these things but you might quantize your data, you might apply the log to it, you might clip it, you may also go out and create new features, you might create embeddings, you might do a feature crosses, you might combine features, you might square your values, you may do a variety of things in terms of creating new features. Now, this is now part of your pipeline as well. So you take your input data, you have to split it but in addition, you don't just take your raw data and directly input it to the model, you want to take your raw data, you want to do preprocessing, you want to do a feature creation and it has these features after preprocessing that your model expects. So fine. We want to do these things and you want to do these on very large datasets. So, you want to do them on the cloud as well. In addition, you may not be sure about the exact model. Well, should you have four layers? Or five layers? Or six layers? Should you have 64 nodes in the layer? Or 120 nodes in the layer? Or thousand nodes in the layer? You might start of the rule of thumb but you might want to carry out hyper-parameter tuning to explore different values to make sure that you're not missing something here. So, you might want to explore different model architectures. So this then is a typical training cycle on a large data set. But you don't just stop with training. The whole purpose of a model is to take this model and predict with it. So, how do you take a model and predict with it? Well, you want to take your model and make it a microservice. So you may take a model, deploy it to the web, and have it be a web application. Why do you do that? Well, remember that when we have our models, the TensorFlow model say, so you have estimator, you fit it, you have your estimator. If you want to predict with that model, what do you need to do? You need to take the estimator and call the predict method on it. So, that thing needs to be in Python. Are all the clients of your code in Python? Will they have access to the model directory so they can construct the estimator object? No. Will they know the feature columns that you used when you created the model? No. So you want to shield your clients in such a way that if the clients need this model prediction, they can simply set a REST API call with all of the input variables that are needed by the model and send it along to this web service and this web service will essentially take all these input variables and convert them into tensors, send them to your TensorFlow model, get the results back, convert them back into something that travels over the network and the client is none the wiser. All the client has done is that it issued an HTTP request and got back a HTTP response. But on the server side, the TensorFlow model has run. Now, you want to take this TensorFlow model and you want to auto scale it. Why do you need to auto scale it? Well, because you may have millions of clients. You may have lots and lots and lots of requests coming in at the same time. So, you want to take your web application and you need to have it support whatever throughput you need to support. So the problems in training and the problems in prediction are different. The problem in training is that of scaling it out so that you can train over a large data set. The problem in prediction is that you want to deal with high throughput so that whenever a user request comes in, you basically handle that user request as fast as possible and give the user back the response to their request. This is why the first generation of the TensorFlow processing unit, the TPU, was primarily around prediction, it was not about training. The first generation of the TPU was primarily around prediction or inference. It was a second generation of the TPUs that actually supported both training and prediction. So, the first thing that we built at Google was a way to do prediction at scale. Because we said, if somebody comes in, they do a Google search, and we need to run a number of machine learning models to give them the best result for that Google search, we need that fast. So, that was a higher priority, prediction. And the prediction speed was a higher priority than training because with training, you can always scale it out, get more machines. But for prediction, we really needed speed. The fly in the ointment is that what the client wants to give you is a raw input data. The client has no idea that you've taken the input data and you've done preprocessing of it. It doesn't know that you've done a log transform, or you've quantized the data, or you've done feature crosses, or you've done any of a variety of things that you might do in a machine learning model. The client doesn't know. That's a whole point of using a web service. We want to shield them from all of these details. A client wants to give you the raw data. The raw data is all the client knows and that's what they want to give you. So, who's going to carry out this preprocessing and feature creation? Somebody has to do it because the model that you trained, the TensorFlow model, it was trained on these features that were created. And somebody has got to go ahead and create those features out of the inputs. Now, this is something that's truly hard because if you don't address this, you have a serious problem. Your client code might be written in Java and now you're taking all your preprocessing code that you wrote in Python and you'll now porting all of the preprocessing code into Java so that this web application or this client carries them out. And oftentimes, when you code even the same thing in two programming languages, they're never identical. Even worse, your data scientists as they go along might come back and change their preprocessing. Meanwhile, the client is still doing the old version of the preprocessing. And so, this is something that's a well-known problem. It's something that happens all the time. In fact, it has a name, it's called Training Servings Skew. What you do in training and what you do in serving or prediction or inference is exactly different and therefore your results exhibit a skew. So, we need to handle this problem. Luckily, Cloud ML does this for us. So, with Cloud Machine Learning Engine, you get something by which you can scale out your training. You get something by which you can repeatedly apply the same preprocessing and feature creation to your training pipeline as you do to your prediction pipeline, and you can carry out hyper-parameter tuning so that you get the best model possible. So, Cloud ML Engine is about making your TensorFlow model grow up. The way you work in Cloud ML Engine is that you work in Datalab, so we've already done this. You start with CSV files, you will do your data exploration using pandas, using Matlack Live, using a variety of tools that you're already familiar with. You do your transformation, you do your preprocessing, you do your feature creation, if necessary, in Apache Beam. And the reason you use Apache Beam, is that Apache Beam is a programming framework that lets you deal with historical data and streaming data in exactly the same way. Why is this important? Think back to this pipeline. Your input data and your training, the top half of this diagram is training, your input data and training is a batch pipeline. So you're basically having old data, historical data and you want to take this data and carry out preprocessing of that data, carry out feature creation from that data, and then you take those features, and you train the model. So that's a batch pipeline. In real-time though, you're going to take your data and you're going to have to apply the same preprocessing and feature creation to it but your real-time data is streaming data. You keep on getting your requests all the time, so any kind of processing that you need to do there is going to be on stream. And if you want the same code to happen both in batch as well as in stream, Apache Beam is a good way to do it. So, you would do it in Apache beam and then you would write a model in TensorFlow, and this will be on a small dataset, you will do your data exploration, you will do your training, you will make sure that everything works. But now that everything has worked, and this is what we've done so far, now, that everything has worked, you want to take this thing and scale it out. And the way you would scale it out is to use Cloud ML Engine and Cloud Dataflow. So, Cloud Dataflow is a way of scaling out Apache Beam pipelines, so you can run things in direct runner locally, and then you can use dataflow runner to run the one on large datasets. Similarly, you can run things in TensorFlow locally, and then you can use Cloud ML Engine to execute this TensorFlow code on GCP, and roll it all out. 
