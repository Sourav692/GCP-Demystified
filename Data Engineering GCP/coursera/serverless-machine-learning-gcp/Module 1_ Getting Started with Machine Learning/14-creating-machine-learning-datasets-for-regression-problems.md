0:00
So, now that we've looked at a number of machine learning concepts, let's go on and do a Lab to create machine learning datasets. The problem that we're going to try to solve is a problem of predicting the taxi fare. The idea is, that you want to go ahead and you want to predict the fare amount. Let's say we want to predict a fare amount. And you have a dataset that relates a fare amount to the distance that has been traveled. So what is the error measure that we're going to use to optimize this? Well, this is a regression problem, because we are predicting a continuous number, the fare amount, and therefore, we should use mean squared error. So great. We'll use mean squared error to optimize this regression problem. Now here is model number one. So we've gotten all of those points, each of these points reflects one data point. So, we had somebody travel this distance, and they paid this amount in fares. And you have a bunch of data like this, and you've basically fitted to a line, and that's your model prediction, and it turns out that the route mean squared error is 22. The reason we're talking route mean squared error is that it allows us to talk in terms of dollars, rather than squared dollars, which is not very intuitive. But the mean squared error and route mean squared error are very closely related, its just a square root of the other one. So let's say we have a fare amount and our error in predicting the fare amount, a route mean squared error, is 22. So here is a second model, and this model has a route mean squared error of zero. Which model is better? I think intuitively, we all feel that this model is better than this model, because this model doesn't look as if it would generalize very well. In other words, if we have some new data, remember the purpose of a machine learning model is to be able to predict for new data, for unlabeled data. So, we go get some data that were not used in training, and intuitively we feel that the first model is going to be more general than the second model, which is going to be overfit to this data that it was used in training. And indeed, model one, if the old RMSE in training was 22, and the new RMSE on this new dataset that we're using is also about 22, pretty similar indicates it's pretty good. So, model one generalizes very well, but model two, now the squiggles do not pass through these new points anymore, and the old RMSE was zero, and the new RMSE is 32, and that is a big red flag to us. It's a red flag that the error on the new dataset is so different from the error on the training dataset.
