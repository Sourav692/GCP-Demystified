0:00
You also have to think about what kind of error measure you're going to use when you do your training. Remember that when you do your training, you're starting a model off with random weights. And then, you're taking your labeled dataset, your examples, for every set of input, you have the predicted output, the output that you want to predict, the truth, the ground truth. And then you calculate the error on that labeled dataset. And then you change the weights on this batch, and then you go back and you recalculate the error and you do this over and over and over again. And then every once in a while, you check if the model is good enough, and then you stop. What this relies on, is that you need an error measure. So, how do you calculate this error measure? Well, you do this starting off with the outcome for a single sample. Let's say we have a machine learning model that's going to help you win at the race track. And this machine learning model, says that you're going to win $89. But on that one horse, in truth, when the race is done, you lose $14. So, what's the error? Well, the error is the truth which is negative $14, the prediction which is $89, so truth minus prediction means that it is $103 off its estimate. So, the machine learning model said you would win $89, in truth you lost $14, and the distance between $89 and $14 is $103. And that outcome for this one experiment is the error, for this one time that you use the model. But you're not going to use this model only once, we're going to evaluate this model on many, many, many runs. And so, you go ahead and take all of those outcomes, you lost 103, you gained 75, you lost 10, you gained 99, et cetera. You take all of those outcomes, and all of the errors on each of those outcomes, and then you square the error, so that whether it's a gain or a loss, it's still a distance away. So you square the error and you calculate the mean, and that becomes a mean squared error. So mathematically, Y is a model estimate, Y-cap is the labelled value. So Y would be, say, the model estimated that you're going to win $89, Y-cap is the labeled value, the model in truth, you lost $14, so that's the error. You calculate the error, you square it, and you take the mean of it, and that becomes the error that you use for a continuous number for a regression problem. But if it's a classification problem, then your labels are zeros and ones, but your model is providing an output that's between zero and one. And because of this, you want to use an error measure that's differentiable. And one way to do this is to use an error measure called cross-entropy. And the way cross-entropy works is this, assume that Y-cap, which is a model estimate, a number from zero to one.


0:00
And the way cross-entropy works is imagine that y-cap, the model estimated the number from zero to one, but y, assume that y is zero. If y is zero, then you can see that the first term goes away, and essentially, then the error becomes log of one minus y-cap. The rest of y is one, then this entire term goes away, and it's only the first term, so it's then then log of y. So, you're essentially taking the number from zero to one and taking the logarithm of it, and that gives you the log loss. It's another way that you can think about this. That becomes a cross-entropy. However, cross-entropy is not very intuitive. You can't go to your boss and say, "I have a machine learning model its cross-entropy is 0.8." It's not very intuitive. No one on the business side is going to completely understand what exactly this means. So, we'll use cross-entropy to optimize our classification models. So, if your computing doing gradient descent, if you're doing gradient descent, we'll use cross-entropy to do gradient descent. But if you're trying to decide if the model is good enough, we're trying to talk about the performance of our model to business users, then the better way to do it is to use what's called a confusion matrix. So, for classification problems, we'll use a confusion matrix to describe the performance.

0:00
The confusion matrix is simply making a table that says, for example, if we are trying to predict whether something is a cat or not, and then machine learning system says, "That's a cat," or "It's not a cat." Well, we go back and look at images of cats, and we say in what fraction of the cats that the machine learning system find, and those are your true positives. And what fraction of cats does a machine learning system say it's not a cat, that's a false negative. Or If it's not a cat, and the machine learning system says it's a cat, it's a false positive. But if it's not a cat and machine learning system says it's not a cat, that's a true negative. So, you go ahead and you fill out this table with the actual sets of number of points that fall into these, and that gives you the confusion matrix. 
