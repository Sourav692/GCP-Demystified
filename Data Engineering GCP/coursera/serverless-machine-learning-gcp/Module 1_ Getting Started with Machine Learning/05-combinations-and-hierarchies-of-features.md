0:01
So we couldn't use a single line to separate these. What did we have to do? We had to add an layer in between; and the layer in between is essentially a combination of features. So each of those neurons ends up combining the X1s and X2s in a specific way, and in this case because my activation function was a linear activation function, it ended up being a line. So now you have the first neuron essentially forming that line, the next neuron forming this line, and the third neuron forming that line, and this guy, this neuron, its rate is almost zero so it ends up not mattering. So those are the three neurons and they together form a triangle, each one of them is its own line. Okay. So, every neuron essentially is a combination of input features.
0:55
But how about this?
0:57
Can you now use a set of lines to do a separation? Now, there's no way that you can draw a polygon, because if you had dozens of lines, you would have a polygon with dozens of sides where it's still a polygon. A single polygon is not going to work. But on the other hand, if you could add, if each of these, each of the ones on the first thing is a single line and each of the ones in the second layer then is a combination of lines, and a combination of lines becomes a polygon, then what we have because of the second layer is a set of polygons. So now we can basically see that each of these is now basically capturing a single polygon and now we have a bunch of polygons together to separate out the blue from the orange. But again one thing that we've got to realize is that the neural network is are only learning the data that you have provided. As humans we know "know that this blue set of darts keeps going up". But it wasn't there in the data and because it was not there in the data, what the ML Model has learned has been a polygon that is just completely truncated there. So that's something important to realize; that a neural network is only as good as the input data that you provided. If you provided data beyond what its inputs are, the results are going to be completely up in the air. You don't know what they are. So let's go ahead and try this as well.
2:51
So here's our, Spiral.
2:56
And we can see that over time the model is going to learn one polygon. And then it's going to move on to learning the next polygon, and the next polygon, and so on. And then over time it's going to have a bunch of polygons that it's learned. And those together will form a pretty good separation between the blue dots and the orange dots.
3:28
And you can see the gradient descent at work as the initial polygon gets created and slowly grows to encompass the other points. And at this point we've kind of stabilized it. Now obviously this is sub-ideal at the very top, but then we had no points. We had no orange point, had no in between, and so we basically had them. If we had an orange point in the in between, then the network would have learned to kind of truncate at this location. But in this case it doesn't, so it doesn't know that. So again, the completeness of your data set, the coverage of your data set is extremely important. Because all that we're doing is that we're learning from examples. 
