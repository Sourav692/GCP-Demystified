0:00
Now that we have looked at what machine learning is, let's apply machine learning concepts to something that allows us to play with these concepts a little bit. One of the best ways to learn something is to develop an intuitive understanding of what it is. And so, that's what we will be doing in this next section on Playing with ML. We're going to be developing an intuitive understanding of what neural networks are, what neurons are, what gradient descent is, etc.
0:33
So machine learning is a way by which we make a lot of very similar decisions based on data. So this is important to realize, machine learning is not about making one off decisions. For example, if your boss comes to you and says, why are we having delays in our supply chain, that is not a question that machine learning can answer. That's a question for human insights. That is a question for you to go ahead and, taking all of the context that you have to answer that one off question, to find a problem in your supply chain. But on the other hand, if your boss comes to you and says, we need to figure out which packages that we are delivering are being delivered wrong,right? Or, which packages that we are delivering, we are delivering to somebody's home when we should have been delivering it to their workplace, right? We need to learn where to deliver our packages better. Now, this is a decision that you're going to make over and over and over again, because you have millions of packages that you're delivering over the course of a day, over the course of a week. And you want to make that decision that given a package, if it's addressed to one of your customers, do you deliver it to them at their home address or at their work address, right? And that now is a perfectly good candidate for machine learning. So machine learning is about, you're trying to make a decision over and over again based on data. So for example, if you're trying to do email classification. So you got an email and you want to say, this email is spam, or this email is not spam. That is a perfect candidate for machine learning. Or, we have a mother that walks in and we'd like to estimate how long her pregnancy's going to last, is it going to be 36 weeks or 41 weeks or 43 weeks or 38 weeks? That is a repeated decision, a very similar decision, based on a large amount of data. It's not a one off decision that we're making, we're making the decision over and over again. So when we talk about machine learning, we're talking about how do we take a data point and make a decision based on that data point. And you make the decision on that data point based on all the history that you've learned over a very large dataset that we'll call our training dataset. For example, let's say we want to classify emails as being spam or not spam and we may have for every e-mail, something about the content of the e-mail and something about the tone of the e-mail and maybe even re-mapped it. All of the thing emails that are not spam are are in blue and all of the e-mails that are spam are in red. Now, maybe in another universe, you again have e-mails, some of which are spam or some of which are not spam, except that their distribution is more like the one on the right. Now take a look at these two distributions. Which of these two problems of identifying spammers is not spam do you think is easier? If you said, the diagram on the left indicates an easier problem, you're right. And the reason it's easier is that mentally we can draw some kind of a line that separates out most of the spam messages from most of the not spam messages. Now the location of that line is something that we have to learn from the data. Should the line be out here starting from maybe one? So one come a three and a half connecting to two come up negative two. Right. So is that the line or does the line cover some other, sorry, is this the line? Is that the line? And at that point it depends on how you identify errors and how you measure them et cetera. But I think regardless of how we do our error measures, most of us would agree that it is much easier to separate not spam from spam if our data were distributed like the diagram on the left than if they were distributed like the diagram on the right. Now keep this in mind.
5:18
[INAUDIBLE] And let's say that these blue dots are spam and these yellow dots are not spam. So again, instead of having words there, I'm just using dots and I have two inputs in the case of spam and not spam. We had the input as content and tone, some other input. Maybe the length of the email and whether or not the word Viagra occurs in the email. So these could be the two inputs that we use in our email. And based on this, we're trying to decide if an email is spam or not spam. Now if the distribution is like this, all of the spam emails are here, and all of the not spam emails are here, is this an easy problem to classify?
6:03
Yes, and how would you classify these points? You would draw a line. And you would say that everything below this line is whatever the class is that corresponds to orange. And every point that's above this line is whatever the class is that corresponds to blue. Let's say all the blue are not spam. And then if we got an email that's out here with x being -1 and y being 5, based on our decision surface, we know that this would probably be blue, all right? So we can classify these points by drawing a line, and what's the equation of a line? The equation of a line is ax + by = c, right? So you have two weights. We have a weight on x, we have a weight on y. And then you have an intercept, a constant. And all the points below the line are where it's less than c and all the points above the line are where it's greater than c. So [INAUDIBLE] In terms of a neural network, if we have two inputs, x1 and x2, we can find a weight, w1, that we apply to x1, a weight, w2, that we apply to x2, and we can check if it's greater than some bias. So ax plus by, is it greater than c?
7:31
That's in your network terms, is it w1 times x1 plus w2 times x2 is it greater than b? And w1, w2 and bias are now the three things that we can tweak to find the best possible line to separate out all of the points that we have. So this could be a machine learning model. And this thing where we are taking all of the inputs, weighting them, that we can call a neuron. A neuron is a way to combine all of the inputs, and make a single decision on those inputs. As a way to separate inputs, if you're doing classification.
8:12
So graphically, a single neuron w1x1 + w2x2 is it greater than b, that single decision translates to be a line. And so this becomes a line that separates the two sets of points.
8:32
How do we find good values for w1, w2 and b? The way we do that, is to use a process called gradient descent, and the way that gradient descent works. The way the gradient descent works is that we start off at some arbitrary value for the weight. And this works for W1, W2 and for B but let's just take one of them, let's say, W1, and we start with an arbitrary value for W1. And we calculate the error. The error might be a as simple as how many points did we misclassify and that could be an error, okay? How many blue points did we say should be orange? How many orange points did we say should be blue and that might be our error. It might be something more continuous. It could be, for example, the distance between the point and the line. The idea being that the more the points are distant from the separation line, the better it is, right? So we could basically be looking at those distances, we could be looking at the number error, we can have different types of errors. We'll talk about possibilities here, but let's say we have an error. We have an arbitrary value of the weight and we start off with that error. And then what we do is that we take the weight and we increase it slightly and we decrease it slightly. And we compute the error again. Now having taken the weight and tweaked it in one direction, and tweaked it to the other direction, when we compute the weight we can now say in which direction does the arrow go down? And the error typically that we use, here is a mean squared error. So MSE, so a typical error that you might use might be the mean squared error, and they say in which direction does a mean square error go down? And depending on that direction, we decide what the next point is going to be. Because in this case, increasing the weight resulted in a lower error. At the next iteration, our point is the next point and we do this for all three weights. For W1, W2 and the bias. To all three parameters, we tweak them. And having tweaked them, we go back and we repeat the process again. We tweak all three sets of weights again and see which direction to move it next. And we do that
11:08
So the idea with the whole process is you start a model with random weights.
11:15
And having started the model with random weights, you calculate the error on a data set for which you know the answers. So that's what we'll call a label data set. We calculate the error on the label data set and then we change those weights and we try out different changes of weights in the neighborhood of that arbitrary weight that we started off with. And we change them so that they're going to a direction that the arrow goes down. And then we go back and we calculate the error again, and we tweak the weights and we calculate the error again. And we tweak the weights and we do this over and over again. Now, when we do this tweaking of the weights, do we have to do it over the entire data set?
12:04
Well, we could, but doing gradient descent on the entire data set will tend to be very slow, right? Your data set may be many millions of rows long, and if every tweak requires you to go ahead and calculate the value of the error on all 1 So instead we do this process of tweaking on a small batch, and typical sizes of batches tend to be like 30 samples to about a hundred samples, right? Not very much more than that, maybe 500 at the max. We're not talking millions here, so batches tend to be a few dozens, to a few hundreds of examples. So, those are the things that we calculate the errors on and we go through and reiterate over and over and over again.
13:00
But then we have to decide when to stop. Because as I said, gradient descent is not going to guarantee
13:10
that it gets you to this global minimum. It just gets you to a reasonable spot. [COUGH] Sorry.
13:19
So every once in a while, you will want to go ahead and calculate, is this model good enough? Should I stop? So you may tend to do this once every 10 epochs, say, what an epoch is Is a traversal through the entire data set. So let's say we have a data set of a 100,000 samples and just to make the math easy, let's say our batch is a 100 samples.
13:49
Which means that every 1,000 steps is one epoch. Remember that we have 100,000 samples, each batch is a 100. So, if we go through this process of changing the weights a thousand times, we have gone through one epoch.
14:09
So maybe every 10 epochs, we say okay, let's stop. Let's go back and calculate the error on the entire data set.
14:20
So now we're no longer doing it on 100 samples, because, face it, 100 samples tends to be kind of small. And any error that you calculate on 100 samples is going to be kind of noisy. So you don't want to basically make a decision to stop your training based on an evaluation of just 100 samples.
14:39
Instead, you may decide that every 10 times that I go through my entire data set, I'll stop and I'll evaluate. And when I evaluate the model, I'll evaluate over the entire data set. So this could be done every 10 epochs. Another way that you could do it is you could say I'm going to do it once every 30 minutes. I'm going to do it once every hour, regardless of how many steps have actually gone through. So, they're both ways can be done. But the point being, you don't tend to do this every few steps, you tend to wait quite a few number of steps. And you tend to do your evaluation over a large data set, not over a small batch.
15:23
If the model is good enough, then you stop, and you export the model, you use the model for prediction. 
