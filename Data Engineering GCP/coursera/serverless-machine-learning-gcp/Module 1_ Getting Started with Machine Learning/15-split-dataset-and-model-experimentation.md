0:00
So, we can formalize this by taking our original dataset and splitting it. So, we can take our original dataset and we say, here's my training data, here's what I'll call my validation data, and then, I can choose how well or how complex my model needs to be by looking at the performance of the model on new data, data that it was not used in terms of training. So the idea is, you take your model, you train your model on the training data, and you evaluate the model on the validation data, and you look at the error, and you increase a model complexity, you go back and you train the model, you evaluate the model, and you look at the model. So, maybe if these were our dataset, and we initially started off the model that's a straight line, then we increase the module complexity, we get something that looks like that, and then we increase the model complexity some more, and we get something that looks like that, and then we go back and evaluate this model on the validation dataset, we learn that the first model is an underfit, the second model is perfect, and the third model again, is overfit, right? The error basically, it's starts to increase in the validation dataset. So, at that point then we can say the right level of model complexity is the one in the middle. So, the idea behind this is that you can take your data and you can split your data into training datasets, and validation datasets, and use this to decide how complex your model needs to be. So in other words, this is something that we'll call hyperparameter tuning, and we can do this, the hyperparameter here is the model link, how many neurons do we have in our neural network? How many layers do we have in our neural network? That's how you increase the complexity of a model. And you can keep increasing it until the point when your errors on the validation dataset start to increase. 
