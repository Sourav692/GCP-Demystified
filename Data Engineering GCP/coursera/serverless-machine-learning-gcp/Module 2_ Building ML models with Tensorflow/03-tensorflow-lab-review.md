So, when we left off, when we left off we were talking about TensorFlow. And we said, okay, let's learn how to use TensorFlow. It's Python, so import tensorflow as tf, import numpy as np. And because I'm paranoid, I always print out the version of TensorFlow, make sure I'm using the version. In this case, I'm using version 1, right? So just make sure that you use – the Datalab image comes, like we update the TensorFlow version in it, so you will be using the right one. But after a while, especially if you're using an older Datalab image, right, and Tensor Flow 1.1 comes out, you may want to update your Datalab image so that you're using the right version of TensorFlow. So that's good. Just keep that in mind. Watch that, okay? And then basically this is what was on the slides. We look at how to add numbers. print c, in this case, it's a direct add. In this case, it's a lazy ad so you basically, at this point, we've only built the graph and when we print c, we basically said this is the first add operation I've seen. So you see the Add:0 and the shape is three. And we certainly – go and play around with it a little bit. So let's do that. Let's play around with it a little bit. So suppose I were to go out here and change it to be seven, would it work? Can I add a four-by-one array to a three-by-one array? No. Right? Now let me just do this in TensorFlow. It's not gonna work. And we basically see that TensorFlow tells us, right, after all of that, the dimensions must be equal, but are four and three for Add. Right? So even though it's a lazy evaluation, because it was a constant, TensorFlow kind of figured out that this one was a four-by-one array and this one was a three-by-one array. And so it said you can't add those, so the shape is important. And you've got to get used to that error message because, you know, you will see it quite a bit. You will see this quite a bit when you're writing code where you're basically doing the wrong. Now, there are things that are just not possible in math. Right? And when we try to do one of those things, you are gonna get that. The other thing to watch out for is the data type. So I can run this and it's fine. But if I were to change this to be 3.0, now a becomes the type is float32, b is int32. It's inferred from the data, right? And it's gonna tell us that it can't add float32 to int32, which isn't really true. It could make them both floats, but it's not gonna do that, right? a is a tensor of type float 32, b is a tensor of type int32, and they are not commensurate. It's not gonna do anything with them. Right? So the way that we would have to do this is either go ahead and change all of them or, I believe, you can set the dtype equals, right, float32 here. And you can do the same thing here. Right? And then it doesn't matter. It would work. So that's the other option. OK. Whatever. OK. There's a way to set the D type, I forget. So you didn't – you guys didn't see that. I'm sorry. Oh, there's a comma missing. Okay. Maybe that was what the problem initially was. Anyway, never mind. Okay. But you need to basically have things be exactly the same type. Then once you have this, we can now go ahead and run c, and we will get the actual answer, right? That's the build step, and this is the evaluation step. There's another way to basically do the same thing. You can either do it this way. This is not a good way to do it. Because if you think about it, where are a and b specified in the graph, right, at the time you're building it? This is equivalent to – when you're writing code, you have hard-coded these things, numbers in the program, and you've compiled it. Right? That's the way you've got to think about this. This is equivalent to hard-coding the number and compiling it. It's compiled into the graph. I cannot use any other numbers for a and b. All right, I can only run this once, so that's not very useful, is it? We want to be able to pass different numbers for a and b each time. Does that make sense? So, the way we do this is we say that a is not a constant, but a is a placeholder. a is not a constant, it's not hard-coded, but a is a placeholder into which we will inject values later. And similarly, b is a placeholder. And you see that when we created the placeholder, I specified the dtype. Fair enough? Yeah. So now I have a and b and their shape, right? When I said None, essentially it's gonna be inferred from the data. So it's essentially gonna be the batch that I provide. How many of our numbers are in that batch is basically what it's gonna be. OK. So it's basically gonna be an array, very similar. I say c=tf.add and at that point I only built the graph. I've not run the graph yet. To run the graph now, unfortunately, I can't just say session.evaluate(c), because c is gonna say where is a and where is b? a and b are placeholders, right? So, I feed, right, there's a feed_dict, and this is just Python syntax for the value of a, supply this array, for the value of b, supply this array. Right? And then when I run it, I get the same answer. But you notice that the two things are different. In the first case, a and b were hard-coded into the graph, and in the second case, a and b were passed in at runtime. Fair enough? Does everybody see the difference between the two? It's very, you know, it's good to see it in a very, this very small snippets of code. Right? So, we've gone from direct evaluation in NumPy, to lazy evaluation in two steps in TensorFlow, to lazy evaluation in TensorFlow but with a feed_dict which makes it dynamic. So what is the kind of code that we will actually be writing? The last one. OK? But when you look at code, you've got to understand that that's what it's doing, right? That's why you set up a placeholder and you feed data into it. So that the placeholder and that graph is inside the GPU and then you can just pass in numbers and all the computation happens and you don't have to have that context switch. So now, let's go and put this in a real-world scenario. And the real-world scenario is I have a method called compute_area, given the sides of a triangle. Right? And if I have a triangle with its sides, I want to get all the a, b, and c. Those are the three sides. So basically, I get the first column, second column, third column, right, because each triangle is gonna be a row. Everybody OK with that? So this is a triangle. This is another triangle. Are you OK with that? They're both triangles. So this is a triangle and that comes in, I want to get the first side, which is gonna be zero, right? Whichever row number it is, all the rows, get the first column, get the second column, get the third column. Everyone OK there with the slicing? So that's how you get the a, b, and c. And then I put the formula s=(a+b+c)/2. I could have done tf.add, but these are overloaded. It makes perfect sense. So we just do a+b+c. But remember that underneath it's really tf.add because a is a tensor.
8:19
Fair enough? Okay. And then I basically compute the area squared, right? s*(s-a). This is basically the formula, right? s*(s-a)*(s-b)*(s-c). And then I basically take the square root. This is my graph. Right? Given a set of sides, it's gonna compute the area. Now my question becomes, how do I get a set of sides, right? So in order to do that, I say compute the area please. Right? Compute the area passing in tf constants. This is equivalent to hard-coding those numbers into the graph. It's not gonna do anything else, right? There's no feed_dict here. There's no way to change and go through. Does this make sense to everyone? All right. I've hard-coded these numbers. I'm computing the area. But in this case, I'm making sides a placeholder for three numbers. Right? So that size number of triangles, each triangle has three sides. Does that make sense? And then each time I run the area, I pass in this side, this time is a batch size of two. So I mean, that's the essential concepts in TensorFlow. Right? So it's a little bit of a learning curve because you have these levels of indirection, OK, but that's essentially it. You set up placeholders and you feed data into it. So as soon as you get that, then everything else starts to fall in place. 
