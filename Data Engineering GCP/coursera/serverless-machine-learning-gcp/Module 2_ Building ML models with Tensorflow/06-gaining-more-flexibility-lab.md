0:01
So far what we have done is that we know how to create a machine learning model, how to train the machine learning model, how to evaluate with it, how to predict with it. But everything that we have done so far is only about in-memory data set, it's about smaller data sets. So what is that we have to do? We now know the basics of how to create a model intent to flow, what we have to learn is how to do this on larger data sets, right? The pandas data set is limited by the fact that it has to all be held in memory.
0:41
In the real world, our data sets are going to be larger. So how do we deal with large data sets in a better way? Also, we've taken our input data, and treated them directly as features. But oftentimes your model will be a lot better if you carry out some amount of what is called feature engineering, creating your own features. So we'll look at that. And finally, it's a good idea to also look at different model architectures, so we'll look at that as well. So those are the three things that you will have to do beyond the basics in order to build effective machine learning.
1:20
So we need to be able to refactor our tf.learn model, so we take our simple tf.learn model that we have, and we will refactor it. What's refactoring?
1:33
Refactoring is a software engineering term that essentially means that you're taking your program and you're changing the design of your program without adding any extra features. The reason you are changing the design of your program is so that you can do extra things with it. So what are some of the extra things that we want to do with it? Number one, in order to deal with big data, we want to be able to deal with data that's out of memory, larger than your memory. In order to do feature engineering, we'll refactor our model so that we can add new features easily. And finally, we'll also look at our model training architectures. And one of the refactorings that we'll do is that we'll do our evaluation as part of our training. So that it makes it easier to try out different model architectures and choose the one that works best.
2:38
So let's go ahead and look at the part that we skipped, the middle layer, which is about components that are useful when we're building your network models.
2:49
So in order to read data from out of memory, out of the read data that cannot be fit into memory, what we would like to do is to store our data in files, not one file but many files, and read them.
3:06
So this is essentially what we're going to do. We're going to have our data in, say, three files, A, B, and C.
3:14
And remember that when we are doing gradient descent, we may want to go ahead and do our model training for a number of epochs.
3:24
If you think about it, going through our data, our entire training data 50 times
3:32
is the same as going through our entire training data once
3:40
as long as we repeat these file names 50 times. So if you create a file name Queue that has 50 As, 50 Bs, and 50 Cs in it, then going through this file named Queue once is the same as going through our data set 50 times. So that's the first thing. So we'll take our Filenames and we'll use it to populate the Filename Queue that has each file num epoch times. And then we will basically tell tensor flow, go through this data once and then training is done.
4:26
We could just take our file names A, B, and C and simply repeat them A, B, C, A, B, C, A, B, C, etc. But when you're doing distributor training that becomes somewhat problematic. And the reason it becomes problematic is that it's usually the case that these files are not all the same size, and they're not all the same complexity. And also it might turn out that your processes are done in different machines and some machines are faster than others. So it might turn out that one of these files is going to basically cause a slowdown. And you would like to ensure that that slowdown is not caused at the exact same machine each time. So a good idea is to randomly shuffle when you add. So we're going to take these filenames, and rather than add them in order A, B, C, A, B, C, A, B, C, etc, each time we add, we're going to permute the A, B, and C and add them. So we're going to do a random shuffle, add it into the Filename Queue. And now we have a filename queue, we'll go ahead and set up a number of readers. These readers will all be on different machines even and each reader is essentially going to go to the Filename Queue and say, give me the next item in the queue, and the next item in the queue, and the next item in the queue. And these readers are going to take these filename queues and read the data and remember that a file may contain multiple examples. And an example is an input plus a label. So what the readers are going to do is that it's going to take these files, decode them and keep adding examples to the Example Queue. And our tensorflow code is going to read the data out of this Example Queue. So that's the way we're going to set up reading data in batches.
6:33
So how does a code work? So let's say we are reading a CSV file. So what we'll do in our input function is that we will say, go ahead and match all filenames with the filename. The idea being that you can have a wild card in your file name, train.* and you will get all of the train.*files. All the files that start with train., okay? More commonly, what you will have will be sharded at files train-0 of 36, 1 of 36, 2 of 36, etc., so now you can just do train-*, and then you will get all of your training files. So that's your input file names. That essentially plays the role of A, B, C. So input filenames is like A, B, C in this diagram.
7:24
And then using that, we basically say, take those input file names and repeat them num_epoch times in a shuffled way, and then we get our filename_queue. So we now have our filename_queue by taking these file names, and randomly shuffling them, and adding them num_epoch times. And then we need to do the second part of this. We need to set up our readers that are going to do the decoding. The reader in our case is a TextlineReader because these are CSV files. And I tell the reader each time, read a batch of records.
8:09
So read a batch of records from the filename_queue and take that record, which at this point is just a line, it's a scalar, and we make it a tensor with viewing expand dims, a tensor with the same shape. And that basically becomes our value. It's now just a string. And we take that and we ask TensorFlow to do a decode of the CSV. So decode this as a comma separated value string.
8:42
But when we're doing our comma separated values we need to tell TensorFlow what the data types are, and what to do if in the CSV, that field is empty, right? Remember that because it's a CSV you could have comma comma comma, and a comma comma essentially means that that value in CSV is null, it's missing, it doesn't exist. But TensorFlow is an anti-neural networks are adding and subtracting machines. So you cannot just add and subtract missing numbers, you need to put the value in there. So what we're going to do is for every one of the CSV columns, we will specify a default value. So for example, for the fare_amount, the DEFAULT value is 0. For the pickuplon which is the second column, the DEFAULT value is -74, etc. Now TensorFlow is going to use these default values to fill in missing data. But it's also going to look at the data type of these. So if this were to be an integer, TensorFlow would treat it as an integer. If this is a double as it is, TensorFlow will treat it as a floating point number. In this case, this is a string, so TensorFlow will know that this is a string. So the record defaults get used two ways. One way to figure out what a default value is, and secondly, to determine what the type of the column is. So at this point, we have our columns, but these columns are just the values. It's a CSV file, every line only has the values. But remember that our features, from our input function, have to be a dictionary, a dictionary where each column has the name of the column in it. So what we're doing is that we're doing a zip. So zip in Python basically takes two arrays and associates a first item of this array with the first item of the second array. So we are basically taking the values here and saying the first value is the fare amount. Second value is a pickuplon, third value is the pickuplat, etc. So we do that, we combine them and we make them a dictionary so at this point we'll have a dictionary. One key is fare amount, and it will have a tensor associated with it. Next key will be pickuplon and it will have a tensor associated with it. Third value would be pickuplat, it'll have a tensor associated with it. Each of these tensors is going to be batch size elements long.
11:19
Maybe except for the very, very, very last one which can be partially filled, but in general, right? It's going to be a batch size element long. Now those are our features. Except that fare_amount is what we are trying to predict, it's not really a feature. So we remove the LABEL_COLUMN from, so the LABEL_COLUMN's the fare_amount. We basically remove it, and we call that the label and we return the features, and we return the label. So the features now are pickuplat, pickuplon, etc., and the label is everything else. Label is the fare amount.
12:03
So one of the cool things is that the TextlineReader in TensorFlow not only reads from local files, it also reads from Google Cloud Storage. And the read_up_to each of these reads is one batch_size. And what these lines of code do is that they decode the CSV, they create a dictionary of features, and then we're returning the label, and the dictionary of features from the CSV.
12:32
So let's go ahead and refactor our tf.learn model to read from a potentially large file, in batches. 
