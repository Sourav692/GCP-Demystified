{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "- Employ BigQuery and Cloud Datalab to carry out interactive data analysis\n",
    "- Train and use a neural network using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Random Access\n",
    "\n",
    "Let's look at some transformative changes, first Fast random access. \n",
    "\n",
    "\n",
    "Many programming languages are object oriented, which means much of the software that you write deals with objects and the good thing about objects is that they're hierarchical. Let's say for example that in your program you have a hierarchical \n",
    "\n",
    "![](img/42.png)\n",
    "\n",
    "data structure like a Player. So you have players who are either footballers or cricketers, and cricketers not all of whom bowl. So you basically have some cricketers have bowling averages, but everyone bats, so all cricketers have batting averages. So you have this hierarchy, and you want to represent this in a relational database, because you want to persist the data. If you use a relational database to persist an object hierarchy, you get into an object relational impedance, there's a mismatch here. For example, we said that all cricketers have batting averages, but not all of them have bowling averages. However the table itself, if you go look at it there'll be a Name, there'll be a Club, there'll be a BattingAverage, there'll be a BowlingAverage. What is a BattingAverage of a football player? Makes absolutely no sense, but we have to have it. So we basically go ahead and put in a null in there, and once you do that you basically have data integrity problems from this point onwards. \n",
    "\n",
    "![](img/43.png)\n",
    "\n",
    "So how do you prevent this kind of an issue with an object to relational mapping? Well one way is if you can store objects directly, and that's what Cloud Datastore on GCP lets you do. So Datastore scales up to terabytes of data, where in a relational database typically goes into a few gigabytes, Datastore can go up to terabytes and what you're storing in a Datastore conceptually is like a Hashmap. So it's a key or an id to an object, so you store the entire object in. So when you are writing to Datastore you're writing an entire object, but when you are reading from it, that's searching, you can search with the key but you can also search by a property. So you can look for all cricket players whose batting average is greater than 30 runs a game, right? \n",
    "\n",
    "2:26\n",
    "And you can basically do this by taking one of those indexed feeds, which we'll look at shortly. \n",
    "\n",
    "2:34\n",
    "You want to update, again you can update just the batting average of a player, and you can update this in a transactional way. So Datastore supports transactions, it allows you to read and write structured data. It is a pretty good replacement for use cases in your application, where you may be using a relational database to persist your data. However, this replacement is something that you would have to do explicitly. Unlike the things that we've talked about in the previous chapter you can't just, for example in the previous chapter we said you have a spark program that you are running on a Hadoop cluster On-Premise. You want to run it on GCP just run it on Dataproc, pretty much all of your code just migrates unchanged. If you have a MySQL code base, well whatever you're doing to your MySQL On-Premise you can do MySQL on Google Cloud using Cloud SQL, those are easy migrations, right? Take what you have, take those use cases that you have, just move them to the cloud, but when we talk about something like Datastore, now it's not that easy a migration. \n",
    "\n",
    "\n",
    "![](img/44.png)\n",
    "\n",
    "You have to change the code that you're doing, where the way you interact with Datastore is different from the way you'd interact with a relational database. So how do you interact with Datastore? Well, the way you work with Datastore is that it's like a persistent Hashmap. So for example let's say we want to persist objects that are author objects. You'd say I have an author class, it's an Entity, that's the identity. It's an annotation that you add and I'm showing you Java here, but it works with a variety of object oriented languages. And you say that the author is distinguished by their email address, the email address is an Id column, so you say add Id. We want to search for authors by name, so we'd like the name property to be indexed, and just to show you that you can have hazard relationships, an author has a bunch of different interests. Same thing about guestbook entries, you store guestbook entries, each entry has an id that makes it unique, it basically has a Parent Key and Author. \n",
    "\n",
    "![](img/45.png)\n",
    "\n",
    "These are the people who wrote the GuestbookEntry and that's a relationship. You have messages, we're never going to search apparently because it's not indexed. We're not going to search for guestbook entries based on the text of the message and we have dates, right? And that's something that we might want the search based on. So once you have an Entity, you have an Author, there's an Entity you have, an Author has an email which is the id, and a name which is the index.\n",
    "\n",
    "![](img/46.png)\n",
    "\n",
    "So you want to create an Author, you basically call the constructor, just as you would do for any plain old job or object. A new Author xjin@bu.edu, name is xjin, you have your Author object, but at this point the Author object is only in memory. You want to save it, you basically call save passing in the entity.\n",
    "\n",
    "![](img/47.png)\n",
    "\n",
    "ofy here is the objective file library, it's one of several Java libraries that help you deal with Datastore. So in this case this code is showing you objective file, we save the entity and at this point the xjin object has been persisted. \n",
    "\n",
    "![](img/48.png)\n",
    "\n",
    "If you want to read it, if you want to search for it, what you can do is say load all authors and filter them by name Ha Jin, and because name is an indexed field we can do this. We can filter by name Ha Jin, and we will get back an iterable of authors. \n",
    "\n",
    "\n",
    "Why iterable and not a list of authors? \n",
    "\n",
    "\n",
    "Well because Datastore scales up to terabytes, so one of these columns that your search is based on, what comes back could be gigabytes of data, might be much more than can fit into memory, so we give you back an iterable. Well if you know you're going to get back only one item, such that's the second one here, you're loading authors and you're finding id xjin@bu.edu, at that point you're going to get one author back, so you best get back the author object. \n",
    "\n",
    "![](img/49.png)\n",
    "\n",
    "We call it JH within the code, and now we can update the name of jh, you can say jh.name = Jin Xuefei and then save that entity. \n",
    "\n",
    "\n",
    "At this point now we basically have a newly persisted object, the object that's persisted basically has a new name, and then if you want to delete the entity, we just say delete entity jh. \n",
    "\n",
    "\n",
    "![](img/50.png)\n",
    "\n",
    "\n",
    "So create, read, update, delete, you can pretty much do everything that you do in a relational database from the transactional way using Datastore. \n",
    "\n",
    "![](img/45.png)\n",
    "\n",
    "\n",
    "![](img/51.png)\n",
    "\n",
    "Another access patron, so these are again options to using a relational database, you could use Datastore if you need transactional support for hierarchical data, something that relational databases don't handle very well. \n",
    "\n",
    "\n",
    "Another reason that relational databases may not work very well, and we've discussed this in the module review section of the previous chapter, is if you have high throughput needs. If you have sensors that are distributed all across the world, and you're basically getting back millions of messages a minute, that's not something the Cloud SQL can handle very well. That's not something a relational database can handle very well, that's essentially a pen-only operation. We're just getting you data and we're saving, and we don't need transactional support. And because we are willing to give up transactional support, the capacity of Bigtable is no longer like the Terabytes that the Datastore can support, but Petabytes. On the other hand what we've given up is the ability to update just a single field of the object, we have to write an entirely new row. The idea is that if we get a new object, we basically append it to the table and then we read from latest data and go backwards. So that the very first object that we find at the particular key is the latest version of that object. \n",
    "\n",
    "\n",
    "\n",
    "So Bigtable is really good for high throughput scenarios, where you want to be not in the business of managing infrastructure, you want something to be as NoOps as possible. With Bigtable you basically deal with flattened data, it's not for hierarchical data, it's flattened and you search only based on the key. And because you can search only based on the key, the key itself and the way you design it becomes extremely important. Number one, you want to think about the key as being the query that you're going to make, because again you can only search based on the key. \n",
    "\n",
    "![](img/52.png)\n",
    "\n",
    "You cannot search based on any property, and because you can't search fast based on properties, you're going to be searching based on keys, you want your key itself to be designed such that you can find the stuff that you want quickly. And the key should be designed such that there are no hotspots, you don't want all of your objects, all of your rows, falling into the same bucket, you want things to be distributed there. Tables themselves should be tall and narrow, right? Tall because you keep appending to it. Narrow, why? The idea being that, if you have Boolean flags for example, rather than have a column for each flag, and have the value be 0 or 1. Maybe you just have a column that says, these are the only true flags on this object. This kind of thing becomes extremely useful if you're trying to store, for example, user's ratings. A user may rate only like five out of the thousands of items in your catalog, and browser has thousands of columns, one for every item. You simply store object comma rating for the things that they've actually rated, and that could be a new column. \n",
    "\n",
    "\n",
    "Even though we said that your columns have to be flattened, there is this concept of a column family. So you can basically say for example here MD is market data. So MD colon symbol, MD colon last sale, this is a way to group together related columns. \n",
    "\n",
    "\n",
    "The reason to use big tables and it's NoOps, it's automatically balanced, it's automatically replicated, its compacted, it's essential NoOps. You don't have to manage any of that infrastructure, you can deal with extremely high throughput data. \n",
    "\n",
    "![](img/53.png)\n",
    "\n",
    "This is how you work with Bigtable, you work with it using the hbase API. So that's why what if you're importing is org.apache.hadoop.hbase. So you work with it the way you would normally work with hbase, you basically get a connection. You basic go to the connection and you get your table, you create a put operation, you add all of your columns, and then you put that into the table and you've basically added a new row to the table. \n",
    "\n",
    "\n",
    "If you're familiar with hbase it's exactly the same way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive, Iterative Development & Datalab Demo\n",
    "\n",
    "So, in this section, again we're talking about transformational cases. The next transformational case we want to talk about is about notebook development. \n",
    "\n",
    "![](img/54.png)\n",
    "\n",
    "\n",
    "As a data scientist, one of the major changes that happened in the way I work with. \n",
    "\n",
    "No, but data is with Datalab. I think we recorded this in person already, so we should use that here. \n",
    "\n",
    "![](img/55.png)\n",
    "\n",
    "So, let's, the way you work with Datalab, is that you have a web page, that's your Datalab webpage. And in this webpage, you can basically write code in Python, and once you write the code in Python, you can run that code by either hitting Shift Enter, or by clicking the Run button at the top of the menu. You can look at the output, you can go back, change the code, run the cell, change the code, run the cell and at that point some time you're basically satisfied with the way the code works you can go in and you can write some commentary right? And the commentary can be mark down format, it can have titles, it can have bold emphasis and all of those kinds of things. It can have links, images, you write your commentary your documentation, and then because it's just a web page you share it and other people can come in. And they can execute your notebook and they can make changes to your code and rerun your notebook, it becomes an extremely good collaborative environment. \n",
    "\n",
    "\n",
    "But one issue still exists, and the issue is where does the web server run? Remember that this is the client client site, the web page is a client but you still need the web server And if you run the Datalab or iPython or Jupiter Web Server on your own laptop, then as soon as you close down your laptop, nobody can access your notebook. So, you really want to have this thing running on a cloud computer, and Datalab Is a way to run it on GCP. \n",
    "\n",
    "![](img/56.png)\n",
    "\n",
    "To work with this then, you have to think in terms of two things, where does a server run and where does a client run. So, one option is to run both locally. And this is if you're doing local development. Your own CPU, you store your Notebooks on disk And you access your notebook using localhost:8081/ for example, that's your port. And data log comes in a docker container, so you're basically running a docker image locally. So, that's good as long as you are the only one who needs to access this notebook. The second option, this is good if you have multiple people going to access a notebook, is to run the Docker container on Compute Engine. \n",
    "\n",
    "\n",
    "So you basically get a Google compute engine instant up, and going and on that computer gen instance you run that Docker Container and this way then whenever you need to connect to the notebook Use an ssh tunnel CloudShell will let you do this. So, you'll use an ssh tunnel where CloudShell to connect to this GCE \n",
    "\n",
    "\n",
    "instance and then inside your browser you're working with it, but remember that everything that you're running the code itself is getting executed on the computer engine instance. \n",
    "\n",
    "3:18\n",
    "A third way to do this is to basically have it again on a computer engine instance, but access it through a gateway, so you basically have a proxy set up and this involves a little bit more in terms of setting up your browser in such a way that you are not going through cloud shut. For the purposes of this class, the CloudShell approach is what we will use. But if you are going to be doing this a lot. If you are a data scientist, you do quite a bit of data science work. You might want to explore the third option, because remember that CloudShell is an ephemeral VM. And every 60 minutes or so it'll get recycled. And then you'd have to go create the SSH tunnel again. It's pretty easy to create an SSH tunnel, it's just a single script that you run, but it is still something that you might want to avoid doing, it can get pretty frustrating. So, if you're going to be doing this a lot, and if you're going to be doing this for more than 60 minutes, let's say if you're going to be doing this for more than 60 minutes, you might want to look at the third option. \n",
    "\n",
    "![](img/57.png)\n",
    "\n",
    "But for the purposes of this class, we will use the second option because it's very simple, very easy to get going. We already have CloudShell. It's easy to create a Compute Engine VM. \n",
    "\n",
    "\n",
    "So, let's go ahead and do that. So, that code lab, that link, gives you directions for doing all three of these things. So, let's click on this link. And you will see that it takes you to option one. \n",
    "\n",
    "![](img/58.png)\n",
    "\n",
    "One option, two or option three. Option one from G crowd SDK, option two from crowd shell and option three from local machine. These are actually reversed from the order in which we covered them but this is basically the order of recommendations if you will, okay. So, this is the one that we recommend, this is the one that's an okay thing, This is the one that you would have to be able to install software on your machine, etc. \n",
    "\n",
    "\n",
    "So, this cord lab is organized from the best to the not so best. So, let's go ahead and, in this class we'll basically use option number two. To run Datalab from Cloud Shell. If you want, you can follow along with me, pause the video if necessary so you can catch up, and we can go on. So, here what I'm going to be doing is, number one, I'm going to open up Cloud Shell. And, if necessary, I'm going to clone the following Git Repository. I already have the Git Repo, So that I can copy this. And so we need to open up cloud shelf. So, how do we open cloud shelf? The way you open cloud shelf okay is to base go to cancel this okay \n",
    "\n",
    "![](img/59.png)\n",
    "\n",
    "The way you open up CloudShell is to go to console.cloud.google.com. And this takes you to the JSP web console. \n",
    "\n",
    "...\n",
    "\n",
    "ทำตามใน Guideline เลย"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warehouse and Interactively Query Petabytes\n",
    "\n",
    "![](img/60.png)\n",
    "\n",
    "This is Arthur C Clarke. And one of his famous quotes is that, any sufficiently advanced technology is indistinguishable from magic. And to me, that perfectly captures my feelings when I first encountered BigQuery. BigQuery is magical. But rather than me telling you what the query is, let's see it in action. So I'm going to go to bigquery.cloud.google.com. \n",
    "\n",
    "![](img/61.png)\n",
    "\n",
    "And in the BigQuery console I'm going to compose a query and that's basically going to be my query.\n",
    "\n",
    "![](img/62.png)\n",
    "\n",
    "I'm going to be searching on 10 billion rows of the Wikipedia data. And for every row, I'm going to do a regular expression match on the title to see if it matches Google. And I'm going to see which language that mention of Google was in, and how many views there were of those mentions. So we're going to look for pages in different languages that mention Google in the title and count the number of views of those. And we'll go to the Show Options, and say don't cache the results. \n",
    "\n",
    "![](img/63.png)\n",
    "\n",
    "BigQuery by default will cache the results for it for a few days, so that if you rerun the exact same query, you're not paying for it a second time. But we don't want to cheat, so we'll not cache any results. And let's go ahead and hide the options, run the query and 1, 2, 3, 4 seconds, 5 seconds, 6.3 seconds later, 416 GB of data have been processed. \n",
    "\n",
    "![](img/64.png)\n",
    "\n",
    "And we find out that most mentions of Google that people viewed were that in English and secondly it was in Español.\n",
    "\n",
    "\n",
    "I mean, commons is obviously not a language. And the third was in German and Italian, followed by French, Japanese at number 8 and Dutch at number 9, etc. But the cool thing, realize, is that in 6 seconds, we were able to process nearly half a terabyte of data. This is awesome. This is magical. \n",
    "\n",
    "![](img/65.png)\n",
    "\n",
    "So, this is what we just did, we did a demo of BigQuery. This is a query. We looked at 10 billion rows of data. We did selection, then we grouped it, we ordered it and we got all that done in about 6 seconds.\n",
    "\n",
    "![](img/66.png)\n",
    "\n",
    "So what BigQuery gives you is an interactive way to analyze petabytes of data. The queries that you write can be standard SQL, SQL 2011, so it's a very familiar language to most people. You can also write User Defined Functions in JavaScript. And you can access data, CSV data, JSON data on Cloud Storage and run a query on them without actually ingesting them into BigQuery. \n",
    "\n",
    "![](img/67.png)\n",
    "\n",
    "Ingesting data into BigQuery though is very straightforward. If you have your data on disk, if it's small enough data, or if you have your data uploaded to Google Cloud or in Cloud Datastore, it's as easy as going to a web console and basically saying, here's my file. This is the destination. Here is a schema. These are the columns. These are the types. Load them in. It's very, very, very easy. And what you can do from the web console, you can also do from the command line using the BQ command. You can stream data in with Cloud Dataflow. And this is very, very helpful if, for example, you're receiving sensor data in real time, blog data time in real time, you can process them with Cloud Dataflow and stream them into BigQuery. And even as the data are streaming in, you can run queries on that data. Besides batch data on cloud storage or streaming data via Cloud Dataflow, a third option is to not load the data into BigQuery at all. Is to leave your data in raw form as CSV files or JSON files or Avro files and set up what's called a federated data source. You're basically creating a link to it and saying, here is the name, here is the schema, there is my file, so don't make a copy of that file and directly query that file. That last option there, Google Sheets, is extremely interesting. The idea is that you can have some data in a Google Sheet and query it with BigQuery. And now you are saying, wait a second, a Google Sheet, what? I probably have like 30 rows in my Google Sheet, why would I use a SQL BigQuery query to query it? Well, it's not about the data in the Google Sheet. It's about what you can join it with. So you could, for example, have millions of rows, billions of rows of data in BigQuery and join it with the smaller data that you have in Google Sheets. So you may have your customer data in Google Sheets, your sales data in BigQuery, and you'll be able to basically correlate that customer. Join them with this much more massive data that you have in BigQuery. So being able to join a table in Sheets with a table in BigQuery is extremely powerful.\n",
    "\n",
    "![](img/68.png)\n",
    "\n",
    "You can also write user defined functions in BigQuery, so you're not limited to SQL 2011. Here for example, I'm calling a method called urlDecode. That method is defined as a user defined function and it's written, it's implemented in JavaScript. And you can have natural JavaScript, so people have done even more very complex things like natural language processing. There's a JavaScript library that does natural language processing, and so they're able to basically write a UDF to process, for example, Stack Oveflow questions, right? So that's free-form text. You can now process it with an NLP API and you can do queries on it. This is also a very powerful feature that makes BigQuery even more powerful than being able to run SQL queries. It's because you can now take JavaScript libraries, that can do a lot more than SQL, and combine them with the capabilities of your SQL programs. \n",
    "\n",
    "![](img/69.png)\n",
    "\n",
    "You can work with BigQuery from the console, as I just showed you. But running it from Datalab gives you another level of flexibility because Datalab is a way by which you can run Python programs. And the good thing about Python is that it has really nice data analysis capabilities, data visualization capabilities. And now you can basically combine BigQuery with these very powerful graphical tools and data science tools. This is the way it works. You would basically go to a cell and you would mark that cell as a SQL cell. And that basically tells Datalab that this is not Python code that's going to follow, but a BigQuery SQL code. So you say, there is SQL, I'm going to give it a name, wxquery. Then in another cell, which is a Python cell, you would create a query, wxquery, which would basically link to that. Say, whenever I call wxquery, this is the SQL query that I want to run. I want to select the day of the year and all of those things and you supply a YEAR=2015. And what this does is that in this query, wherever $YEAR occurs, that's going to get replaced by 2015. So if we run this query and we will get back a BigQuery result set, but we say take that BigQuery result set and convert it into a Pandas dataframe. Pandas is a Python data analysis library. So what comes back, whether, is a Pandas dataframe. And now all of the Pandas, Matplotlib and NumPy, all of those capabilities that data scientists in Python use, they are now completely available off the result of a BigQuery data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Create ML Dataset with BigQuery\n",
    "\n",
    "![](img/70.png)\n",
    "\n",
    "### Overview\n",
    "In this lab you use BigQuery from within Datalab to create a Pandas dataframe that will be the training data for predicting taxicab demand.\n",
    "\n",
    "### What you need\n",
    "To complete this lab, you need:\n",
    "- A project created on Google Cloud Platform\n",
    "- A launched Datalab instance\n",
    "\n",
    "### What you learn\n",
    "In this lab, you:\n",
    "- Use BigQuery and Datalab to explore and visualize data\n",
    "- Build a Pandas dataframe that will be used as the training dataset for machine learning using TensorFlow\n",
    "\n",
    "### Start the Codelab\n",
    "https://codelabs.developers.google.com/codelabs/cpb100-bigquery-dataset/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning with TensorFlow\n",
    "\n",
    "To do machine learning, we're going to use TensorFlow. TensorFlow is a machine learning library that underlies many of Google's products. We open-sourced this in 2015. And TensorFlow is actually a C++ engine. The reason it's C++ is so that we can use GPUs, we can use CPUs, we can run on Android phones, etc. \n",
    "\n",
    "![](img/71.png)\n",
    "\n",
    "But people don't want to write code in C++, so you have an API, and that API is in Python. The Python API talks to C++, gets the job done. \n",
    "\n",
    "\n",
    "And TensorFlow is essentially a numerical processing library, but it has a variety of features that make it particularly good for deep neural networks and training of deep neural networks. \n",
    "\n",
    "\n",
    "![](img/72.png)\n",
    "\n",
    "So first of all, so because we are going to be talking about neural networks, what exactly is a neural network? Let's go ahead and look at this pretty cool site called Playground. So I'm going to go in to playground.tensorflow.org. Let's go ahead and remove all these so that we have an idea what it is that we want to do. What it is that we want to do is that we have some data, and the data is that we have blue dots and we have orange dots. And the idea is that given a dot, we want to be able to predict whether it's orange or it's a blue. And in order to do that we have two pieces of information. We have the x and we have the y. Okay, the x here is from -66, and the y here is from -66. And given the x and y, we want to be able to predict if a dot at this point for example. x is 5 and y is 4. Is that dot going to be blue or orange? What do you think? \n",
    "\n",
    "\n",
    "Well, I think you would say orange, because everything far away seems to be orange. But at this point, the background image is a prediction. And the prediction is that it's going to be blue. Everything to the right of this is going to be blue. Everything to the left of this line is going to be orange. And the way this prediction comes about is by taking the two xs, this x and this x, adding them together, and that's basically what my result is going to be. All right, so it's basically going to be a sum of the x and y with a certain weight. So -0.18 times x plus -0.28 times y. Add the two things up. If it's less than 0, it's orange, and if it's greater than 0, it's blue. And because these two weights are negative, you can kind of see that all the negative data is here and all the positive data is here. So that's basically a prediction that's pretty bad, right? The prediction is that everything here is going to be blue, everything there is going to be orange, that's not true. But let's see if we can change these weights. There's a weight here on x, there's a weight here on y. Let's say, go ahead and tune these weights to come up with a better prediction. And as you can see, it is not possible to come up with a better prediction that can linearly combine x and y to basically separate blue dots and orange dots. So let's stop this, this is going nowhere. And let's think back about this problem. Remember when I said when x is 5 and y is 4, what was the color? You can intuitively said it would be orange. The reason that you thought it would be orange was because of the distance. Everything that was close to the center was blue. Everything far away from the center was orange. \n",
    "\n",
    "\n",
    "And going back to elementary school, what was the formula for a distance? \n",
    "\n",
    "\n",
    "It's square root of x squared plus y squared. So there's an x squared term and there's a y squared term. Let's, instead of just using x and y, let's add x squared and let's add y squared. So now we have four inputs, not just x and y, but x squared and y squared. So given these four inputs, let's come up with weights for all four of these in such a way that it separates blue dots and yellow dots. So let's start, and lo and behold, that's my prediction now. The prediction is that everything inside of this is going to be blue, and everything outside of that is going to be orange, and that seems to capture the data pretty well. \n",
    "\n",
    "![](img/73.png)\n",
    "\n",
    "It captures our intuition of what this data say very well. So this idea is called feature engineering. So one of the ways that we can improve our machine learning models' predictions is to kind of get human insight into the problem. And the insight that we had was that this was based on distance. We knew that distance involved x squared and y squared. So we threw that into the network and we said, train yourself with weights. But let's say we don't have that insight. \n",
    "\n",
    "![](img/74.png)\n",
    "\n",
    "All I have is x and y and I want to basically do this prediction. So rather than do feature engineering, another thing that we can do is that **we can create a neural network**. So I'll create a layer of these and what it's doing is, this guy is x and y added together, and to that, I'm applying some function. I could do rectified linear unit, tan hyperbolic, sigmoid, whatever. It doesn't really matter which one we choose, let's just pick Tanh. So I'll do Tanh there, Tanh here, Tanh here, Tanh here, Tanh here, so five different Tanhs. Add them all up. Why did I pick five? Who knows? I just picked five. Why did I pick Tanh? Who knows? I just picked Tanh, right. I just picked something. I have a neural network. I'll basically go ahead and train it. So go ahead and find. Know it's now no longer just two sets of weights. It's two weights here, two weights here, two weights here. So that's ten weights here plus five weights here. So 15 weights that we get to basically tweak around, so go find me a set of weights that capture this data. \n",
    "\n",
    "\n",
    "And it takes a little bit long, and it basically comes back with, well, everything inside this triangle like shape is going to be blue, and everything outside it is going to be orange. \n",
    "\n",
    "\n",
    "Is that reasonable? \n",
    "\n",
    "![](img/75.png)\n",
    "\n",
    "Yea, right, its not going to be perfect, but it's a pretty reasonable approximation to this data, and it'll help you predict with pretty darn good accuracy how well this is going to do. And that's the point of a neural network. The idea is to basically capture what the data are in such a way that you can do the prediction later on. \n",
    "\n",
    "\n",
    "So, notice that what we did here was rather than take our human insight, we were able to use a neural network to essentially get at a good enough end result. So this is a neural network with one hidden layer, that's one layer of these neurons, so there are five of those nodes. We could also create extra hidden layers. \n",
    "\n",
    "![](img/76.png)\n",
    "\n",
    "So now we have ten sets of weights here, ten sets of weights here, and then five sets of weights, so that's now a whole bunch more weights, right? Now this is ten, but each of these has five, so that's 5 times 5, that's 25, so 10 plus 25 is 35, plus 5 is 40 weights. So we now have a model that's a lot more complex. And you basically get, again, reasonably good results. \n",
    "\n",
    "\n",
    "And **the basic rule of thumb** is to go with the simplest possible network that gives you good enough performance. So in this case we would go with just one hidden layer, but here we have to choose a number of nodes, and let's say we start with two nodes, and let's see, does this do well? And it turns out that no, two nodes are not enough for this problem. It doesn't do very well. There are all these errors here in terms of capturing them. So let's stop that. Let's say we add a third node, right? And then we say, start this. \n",
    "\n",
    "![](img/77.png)\n",
    "\n",
    "And with three nodes, it seems to be fine. Except maybe some of these guys. The ones at the edges are probably not completely right, but it's close enough. So in this situation, I would probably go with just three nodes, right? That's the simplest neural network that gives me good enough performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and creating a Neural Network Model\n",
    "\n",
    "![](img/78.png)\n",
    "\n",
    "So what does that have to do with ours? Remember, what we have is that we have temperature. We have rain. We basically have the days of the week and so on. So that's our input. So that's the x and y in my playground. So I have rain and maximum temperature. Those are my inputs. I have a bunch of hidden nodes that I'm going to create. And I'm going to predict, and not predict whether it's orange or blue, but predict the taxicab demand. And so this is my model, my neural network. And I'll basically keep adjusting the weights on this model based on this data set such that given a particular rain and a temperature, it gets as close to the observed number of rides on that day as we can. \n",
    "\n",
    "![](img/79.png)\n",
    "\n",
    "So in order to use TensorFlow, step number one, we need to basically collect the predictors. In our case, our predictors are the weather and the day of the week. And the target data, and in our case the target data is the number of taxicab rides that happen that day. Then we create a model. Creating a model is figuring out how many nodes you need, how many layers you need. And then train the model based on the input data that is adjusting the weights. And now you have a trained model. Use this model to do your predictions. So that's basically how you use TensorFlow. So let's look at how that looks in code. \n",
    "\n",
    "![](img/80.png)\n",
    "\n",
    "To collect predictors and target data, this is exactly what we did in the previous lab. We used bit query. We made a Pandas DataFrame, and we have our DataFrame. Typically we'll write this out untill something like a CSV file and we'd use that for training. But one thing that you will have to do, is that you will have to go through your data and figure out which data sets are numeric, and which ones aren't. Because in neural network, all its doing is adding and subtracting numbers, so it needs to all be numbers. So let's go ahead and take a look at our data. So let's say, minimum temperature 28.9, is that a number? Yeah, maximum temperature 37.9, number? Yup, sure. Range 0.01, number? Sure, how about the day of the week, day of the week is 4, is that a number? \n",
    "\n",
    "It's a tricky one. \n",
    "\n",
    "It is not the number. \n",
    "\n",
    "Well, yeah sure we're representing it by a number but that's like what's called a sparse representation. In reality though, the day of the week is a categorical variable. What is Sunday divided by Thursday? It makes no sense, right? So you can basically divide one by four and get 0.25, right? Sunday divided by Thursday is not 0.25. \n",
    "\n",
    "So day of the week, is a categorical variable and what you do with categorical variables is that you basically do what's called one hard encoding of them. So you say, is it Sunday, is it Monday, is it Tuesday, is it Wednesday, etc. And then, depending on which one it is, you put a 1 in that column and everything else is a 0. The other thing that you need to be careful about is you need to look at your input and say, do I have at least five examples of these. That's a rule of thumb, five to ten examples of that particular value. So do you think you will have five to ten examples of days in which the minimum temperature was 29 degrees? Probably, maximum temperature is 38 degrees, five examples of those, yeah sure. Five examples of days in which it rained 0.01 inches, yeah, probably. Five examples of day number 77. Remember that we trained, our training data set is essentially two years of data, 2014 and 2015. So how many April 23rds are there in our dataset? Only two, so we've got to throw away the day number. The day number is way too specific. If we incorporate the day number into our predictions, then the neural network will essentially just memorize that on day number 77, the answer is 51,635. And I'd be quite happy to tell you that on April 23rd, any year that you throw in, it's going to be 51,635. So that's not what we want, so lets throw away the day number. \n",
    "\n",
    "![](img/81.png)\n",
    "\n",
    "Once you have that, now you want to create a neural network model. That is, you know the number of predictors, you know the number of outputs. In our case, we have one output which is the taxi fare. \n",
    "\n",
    "It's the number of taxicab rides. \n",
    "\n",
    "We know the number of predictors, which is like the number of weather variables and so on. So we basically have the number of predictors and then we basically choose the number of hidden nodes. Again, that's something that you choose arbitrarily, but you do some experimentation to figure out what works. So let's start with, I'm going to have one layer with five hidden nodes. So I'm going to create a deep neural net regressor with five hidden nodes and basically pass in all of my feature columns. \n",
    "\n",
    "![](img/82.png)\n",
    "\n",
    "That's pretty much it, so I now have my neural network model. I want to train the model. To train the model I called the fit function, passing in the predictors, passing in the targets and we're done.\n",
    "\n",
    "![](img/83.png)\n",
    "\n",
    "So we want to use the neural network model, right? You don't actually need the taxicab rides anymore. All you need is the original inputs. So to do your prediction, you basically go ahead and create a dictionary consisting of all of your predictor variables. So let's say we want to predict for three days. Thursday, Friday. No, actually, this is Wednesday, Thursday, Friday. 4, 5, and 6. And we'll basically pick minimum temperature on those days. Let's say the weather forecast is for 60, 15, and 60. And the maximum temperature is 80, 80, and 65. And the rainfall amount on those three days is 0, 0.8, and 0. So that's my data. And we basically say, estimator, please predict for this data and we get backup predictions. \n",
    "\n",
    "\n",
    "That's it, right? So those are our four steps. Let's recap them. You collect your predictors and data. You throw away very specific information, like day number that identifies a row. And you make sure that all of your predictor columns are numeric. They're not categorical data. If they are categorical data, then you basically won't have them coded. And then you basically create a neural network model. And that involves specifying the number of hidden nodes and deciding if it's a regression problem or we didn't quite talk about our classification problem. A regression problem is where you're predicting a number, and a classification problem is where you're predicting orange or blue, right? You're predicting a category. \n",
    "\n",
    "\n",
    "And then you train the model, passing in the predictors, passing in the targets, and passing in the number of iterations. And at that point you now have a trained model and can use it to carry out your predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Carry Out ML with TensorFlow\n",
    "\n",
    "![](img/85.png)\n",
    "\n",
    "### Overview\n",
    "In this lab continue the Datalab notebook from Lab 4a to create a TensorFlow neural network to predict taxicab demand.\n",
    "\n",
    "### What you need\n",
    "To complete this lab, you need:\n",
    "- A Datalab notebook with a Pandas dataframe corresponding to the training data [Lab 4a]\n",
    "\n",
    "### What you learn\n",
    "In this lab, you:\n",
    "- Use TensorFlow to create a neural network model to forecast taxicab demand in NYC\n",
    "\n",
    "### Start the Codelab\n",
    "https://codelabs.developers.google.com/codelabs/cpb100-tensorflow/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully build machine learning models & lab\n",
    "\n",
    "In the previous section we looked at doing machine learning from scratch using TensorFlow. \n",
    "\n",
    "![](img/86.png)\n",
    "\n",
    "But, there is a full spectrum of machine learning models available. You would use TensorFlow if your machine learning researcher interested in extending the open source SDK. If you're interested in creating new machine learning models for research, etcetera. On the other hand if you're in industry as a data scientist and you want to build a machine learning model on your data set. And the machine learning model that you're building is something that's pretty well understood, you would want to use Cloud ML Engine, right. This is basically going to give you no-ops, so that you are not in the business of managing infrastructure, to be able to do machine learning at scale over real world data sets. In the previous lab, no we were doing it with TensorFlow on a very small data set just about 700 rows long. Simply because that's all that would fit, right? The real world, you would do it on a large data set, and you would need much more powerful infrastructure, and rather than manage all that infrastructure yourself, you would typically just submit the job to Cloud ML, and have that taken care of. \n",
    "\n",
    "\n",
    "The third option in the spectrum is this idea that there are variety of machine learning models that already exist and it's kind of painful to reinvent the wheel. So if what you're looking at is how do I take some audio and transcribe it, well don't go around building your own speech model. Just use a speech model that has already been developed because how are you going to get the amount of data that's needed to train that model? So the machine learning APIs are all about taking pre-built models and incorporating them into your applications. So you're using machine learning, but you're not training a machine learning model, when you use ML APIs. \n",
    "\n",
    "![](img/87.png)\n",
    "\n",
    "And the reason that you want to use the ML APIs is that the quality of a machine learning model increases with the amount of data that you have in it. So as the amount of data increases, your accuracy goes up. And the reason your accuracy goes is up, as you increase the amount of data, is because you can use larger and larger and larger newer networks you also have a huge computer problem. And that the scale at which Google operates, very deep networks, to use TensorFlow processing units, we do distributer training and we can do no-ops. So cloud ML gives you all these things if you have a data set that's large enough to solve the problem. \n",
    "\n",
    "\n",
    "You tend to have that kind of a data set for things that are relevant to your business. But maybe not for something like classic image recognition or classic speech recognition and natural language processing. This is where you basically want to take advantage of the amount of data that Google has collected in order to build our vision models and our speech models and our language models. And the good thing is that the speech and vision and language models that Google has created are available to you as machine learning API's. \n",
    "\n",
    "![](img/88.png)\n",
    "\n",
    "The machine learning APIs and other words are built off Google's data. So if you ever use the Android app, where you can point the application, add a foreign language sign and get it translated. Well, that app uses translation, it uses optical character recognition and OCR, Optical Character Recognition is part of the Vision API and translation is part of the Translate API. Similarly if you're being on Android and you've done a voice on Google or you use Google maps and you've done a voice search, you've basically using the capability that goes into the speech API, right? Being able to take audio and turn it into words so you you can do something with those words. Or the sentiment analysis of things like reviews. Is it a positive mention? Is it a negative mention? Those are the kinds of things that these models already exist. Google has built them off of Google's collected data. So we can take advantage of it with the ML APIs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Machine Learning APIs\n",
    "\n",
    "![](img/89.png)\n",
    "\n",
    "### Overview\n",
    "In this lab you use Machine Learning APIs from within Datalab.\n",
    "\n",
    "### What you need\n",
    "To complete this lab, you need:\n",
    "- A launched Datalab instance\n",
    "\n",
    "### What you learn\n",
    "In this lab, you:\n",
    "- Learn how to invoke ML APIs from Python and use their results.\n",
    "\n",
    "### Start the Codelab\n",
    "https://codelabs.developers.google.com/codelabs/cpb100-translate-api/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "![](img/90.png)\n",
    "\n",
    "![](img/91.png)\n",
    "\n",
    "![](img/92.png)\n",
    "\n",
    "![](img/93.png)\n",
    "\n",
    "![](img/94.png)\n",
    "\n",
    "![](img/95.png)\n",
    "\n",
    "![](img/96.png)\n",
    "\n",
    "![](img/97.png)\n",
    "\n",
    "![](img/98.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resource \n",
    "\n",
    "- Cloud Datastore: https://cloud.google.com/datastore/\n",
    "- Cloud Bigtable: https://cloud.google.com/bigtable/\n",
    "- Google BigQuery: https://cloud.google.com/bigquery/\n",
    "- Cloud Datalab: https://cloud.google.com/datalab/\n",
    "- TensorFlow: https://www.tensorflow.org/\n",
    "- Cloud Machine Learning: https://cloud.google.com/ml/\n",
    "- Vision API: https://cloud.google.com/vision/\n",
    "- Translate API: https://cloud.google.com/translate/\n",
    "- Speech API: https://cloud.google.com/speech/\n",
    "- Mark as completed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
