{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Data Processing Architectures\n",
    "\n",
    "In this last module of the class, we'll do a very quick overview of message-oriented architectures and serverless data pipelines. We will look at serverless data pipelines, in particular cloud data flow, in a lot more detail in the course on serverless data analysis, which is part of the data engineer track. In this module though, we will look at it very, very briefly, very quickly. So you know what it is and you know where to find it if you need it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Message-oriented Architectures\n",
    "\n",
    "Asynchronous processing is a great way to absorb shock and change. \n",
    "\n",
    "![](img/101.png)\n",
    "\n",
    "For example, if you have an application and you've built this application for 100 users. And in order for it to handle 100 users, it's doing all of these things that it needs to do, and then you basically get a spike in usage.\n",
    "\n",
    "Instead of getting 100 users, you now suddenly have 4,000 users. At this point you have two approaches, one is for your application to just crash. The other is for you to kind of queue things up, such that people get responses but they're a little bit delayed. \n",
    "\n",
    "And that's what asynchronous processing helps you do. The idea is that when somebody submits a job, it goes in. And rather than giving them a response immediately, you basically have the receiving code and the processing code separated. \n",
    "\n",
    "And they're separated by a messaging system. So in other words, new requests come in, they go into a message queue and then you have consumers of this message queue actually processing these requests. So something like this something like asynchronous processing is a very common design paradigm if you need to build highly available systems. \n",
    "\n",
    "The idea being that any request that's sent to the system will get processed. Well, what happens if you have an outage? Well, if you need high availability, then asynchronous processing is one solution. Another thing is to basically balance load across multiple workers, balance high throughput. And the third reason to do that is to reduce coupling. You may have the people producing images separate from the people consuming those messages. And rather than having the system producing messages held up by the fact that the people receiving the message aren't yet able to accommodate a new library or new way of doing things. You can basically separate them, reduce the coupling by using an asynchronous system, by using a message queue. And this allows more agility within your organization. \n",
    "\n",
    "It's a great way to reduce latency so that you can accept requests really close to the network edge. The idea being that the person making the request doesn't have to make the request all the way up to the service. And instead, they can make a request to the closest point on the network. And then the request can travel on an internal Google fiber all the way through. And it's a good way for you to manage consistency. Such that you can apply the exact same security policies to message processing, regardless of where the message comes from. Whereas, if you're relying on the client to process these messages, then you may have some issues. \n",
    "\n",
    "![](img/102.png)\n",
    "\n",
    "So on DCP, the way you can do message oriented architectures is to use Cloud Pub/Sub. Cloud Pu/Sub offers reliable, real-time messaging that's accessible through HTTP, right? So you can have your HR system basically sending a new hire event or vendor office sending a new contractor event and these are decoupled sources. \n",
    "\n",
    "They know nothing about each other, but they publish their events to a common Pub/Sub topic, the HR topic. And then you could have multiple consumers, each of whom has a subscription to this HR topic. Some of these subscriptions could be pull subscriptions. In other words, whenever the system, the client is ready to process a new message, it goes ahead and asks, are there any new messages? Or it could be a push in which, basically, the client system says, call this endpoint whenever there's a new message for me. \n",
    "\n",
    "And that new endpoint would get called by Pub/Sub whenever there's a new message. So in this way, Pub/Sub can give you reliable delivery, you can get completely decoupled workers. And so Pub/Sub is a good way to handle asynchronous processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serverless Data Pipelines\n",
    "\n",
    "The other architecture that we want to talk about is this whole idea of building data pipelines. \n",
    "\n",
    "![](img/103.png)\n",
    "\n",
    "Dataflow, Cloud Dataflow, is the execution framework for Apache Beam pipelines. Apache Beam is an open-source API that let's you define a data pipeline. So this API here that we are showing you is Apache Beam. And you're basically creating a pipeline, reading some text, and the text that we're reading is from Cloud Storage. So we're reading it line by line and for every line, we're applying Filter1. The result of that goes to Group1, the result of that goes to Filter2, the result of that goes to Transform1. And then the result gets written out to another Cloud Storage. The neat thing is, Filter1 is wrapped into a ParDo, parallel do. And what this means is that every line that's read from the input source basically is processed by Filter1. But this is done in parallel across tens of machines, hundreds of machines, as many machines as you need. And you don't need to create these machines beforehand. Dataflow manages the provisioning of these machines, the auto-scaling if necessary, of your pipelines. Such that Filter1 just happens at scale, completely distributed and then everything comes streaming back into Group1. And then the results of Group1 again get done in parallel using Filter2. And then Transform1, and then it basically goes into this sync. Now, each of these things, Filter1, Group1, Filter2, Transform1, these are all classes that you write in Java. You can do Beam in Java or you can do it in Python. I'm showing you Java here, but you can do it in Python. And the code that you write, the Beam pipeline is an open source API. And you have other executors besides Google Cloud Dataflow. So you can execute it on fling, on Spark, and a variety of other things. So with Dataflow, what you get is a completely NoOps data pipeline. So you have a job, you submit it to the Cloud, and that job gets executed. In this case, all of this processing happens on some text input, and some text output gets written out. \n",
    "\n",
    "![](img/104.png)\n",
    "\n",
    "A very neat thing about Dataflow is that it can have that intermediate processing be identical. Filter1, Group1, Filter2, Transform1. But you can swap out the input. Rather than reading from text, in this case, I'm now reading from Pub/Sub. So I'm now reading messages that come in into a particular topic. And remember that I'm doing grouping. Grouping is an aggregation operation, so doing grouping where? If you're accounting or a sum, you need to basically do these things \n",
    "\n",
    "\n",
    "within a particular window. In this case I'm saying the window that I'm going to do all of these things on is over a period of 60 minutes. So I'm applying a 60 minute window and although I don't have a .every here, the default of .every is 1 minute. So I'm applying a 60 minute window every minute. And every minute, I'm basically writing out a new message to the output topic. That consists of filtered, grouped, Filter2 transformed data. That's a new message that now goes to Pub/SubIO. I can change the second PubSubIO, the output sync, to be BigQueryIO. And then I'll be writing to BigQuery, I'll be writing table rows to BigQuery. And then now other dashboards etc., can basically run BigQuery queries on the streaming data as it comes in. And that's the way that you generally get real-time insight. The cool thing is the processing that you do. Whether the data comes from Pub/Sub or the data comes from Cloud Storage, the processing that you do in Dataflow remains the same. \n",
    "\n",
    "![](img/105.png)\n",
    "\n",
    "So Dataflow helps you do ingest, helps you do transformations. Help you do load, so it can do filtering, you can do grouping, and you can do windowing. So Dataflow is where we see a lot of data pipelines migrating. Because you really want to be able to process historical data and real-time data in and identical way. That is the only way that you'll be able to build a machine learning pipeline. For example, that is trained on historical data that operates on real time arriving data. And Dataflow is essential glue in order to be able to do this kind of data processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "![](img/106.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resource\n",
    "- Cloud Pub/Sub: https://cloud.google.com/pubsub/\n",
    "- Cloud Dataflow: https://cloud.google.com/dataflow/\n",
    "- Reliable task scheduling on Google Compute Engine: https://cloud.google.com/solutions/reliable-task-scheduling-compute-engine\n",
    "- Real-time data analysis with Kubernetes, Cloud Pub/Sub, and BigQuery: https://cloud.google.com/solutions/real-time/kubernetes-pubsub-bigquery\n",
    "- Processing logs at scale using Cloud Dataflow: https://cloud.google.com/solutions/processing-logs-at-scale-using-dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
