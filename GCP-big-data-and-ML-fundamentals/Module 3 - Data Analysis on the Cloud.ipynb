{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stepping stones to transformation\n",
    "\n",
    "> Why are we talking about migration to the Cloud and products that help you do migration?\n",
    "\n",
    "To do that, it's probably good to look at this, a little, from a historical perspective. \n",
    "\n",
    "![](img/24.png)\n",
    "\n",
    "The first product, the first cloud product that Google had was App Engine. And the way App Engine worked was, you would develop a web application, write it in Java, upload this application to App Engine, and that was it. App Engine would, essentially, scale your code and server to users. If you have 20 users it may run it on one machine. And as the number of users ramped up, maybe at lunchtime you had 200 users, it'd automatically scale out to three machines. And then at one point your app goes viral, you have millions of users, no problem. App Engines just scales. So in 2008, Google was already doing a server-less, fully managed web application framework. So, this is great, but people found it very hard to get started, why? \n",
    "\n",
    "\n",
    "Well, because we required people to write their code in Java and that was it. We required that you use the App Engine framework. And people said, well, no, I have web applications already, I write them in Tomcat, what do you mean I cant' run Tomcat in App Engine? Or people would say, well I would rather write things in PHP, thank you very much, and App Engine was a no-go at that point. \n",
    "\n",
    "\n",
    "It took us a while to basically recognize this issue. That while a Google engineer was perfectly happy to give up control and say well take care of this for me. Here's my code, just run it, scale it, deploy it, manage it. I'm quite happy to just write the code and have the service take care of all of the auto scaling and reliability considerations. This was not where the rest of the industry was. And so, this is what Eric Smith, who's the executive chairman of the board at Google. This is what he was referring to, when he said that there was something fundamentally wrong with what we were doing in 2008. \n",
    "\n",
    "![](img/25.png)\n",
    "\n",
    "He's talking about App Engine here. And what was wrong? What was wrong was that we didn't get the right stepping stones into the cloud. We weren't meeting our customers where they were. We were, instead, giving them a solution and saying look, this is what we use, come use it too. And, that works for people who have greenfield development needs. They don't have any legacy systems. They don't have systems that already work. They just want to have them continue working. That's good for start-up companies, starting new. So, we do have very large customers like, for example, Snapchat uses App Engine. It's one of the largest web traffic companies in the world and they essentially run completely on App Engine. And that allows them to basically get all kinds of scaling. Any issues just call Google, Google fixes it, that's great. \n",
    "\n",
    "\n",
    "However, if you're not a new companies, doing greenfield development, there was no way to get onto App Engine. There was no nice, easy ramp up. And so, that's what Eric Schmidt was talking about when he said that we are doing something fundamentally wrong in 2008 that we need to fix. And we're fixing it now. So, the idea is that initially, we would just have your code, Java code. It would basically run in App Engine, and that was all you needed. And App Engine would provide all of the infrastructure. But now, we basically give you Google App Engine, which lets you run things that are not Java. So, you can run in a flex environment, you can run Python Flask apps, you can use other web frameworks. It's still your code, it's still going to get auto scaled. It's still going to, basically, take advantage of all of the capabilities that App Engine provides. But you get a lot more flexibility in terms of, taking code that you already have, and running it in App Engine. But let's say you don't have a web application, you actually want to take the application that you have and not run it in App Engine. You want to run it as is in, Tomcat. Well, containerize it and put it into a dock or container. And we will basically orchestrate those containers and manage them for you with Google Container Engine. \n",
    "\n",
    "\n",
    "You don't want to even containerize it. It's running on bare metal, on-premise, and you'd like to move it to the cloud, and have it continue running in bare metal? No problem, we'll give you Compute Engine. And the idea behind Compute Engine is, you take your workload and you just have it run as is, on the cloud. So these are all of the stepping stones to take something, like a web application. Ideally, we hope that over time, you will \n",
    "\n",
    "\n",
    "not have need to run all of this core infrastructure yourself. And instead allow Google Cloud to manage those infrastructure needs for you, so that you can concentrate on your business needs. But, we give you the stepping stones so that, over time, you can do this migration on your own schedule, rather than it being all or nothing. And what is true of App Engine is also true of everything else. \n",
    "\n",
    "![](img/26.png)\n",
    "\n",
    "So, when we look at the Google Cloud, you have all of these hexagons. Because in many cases, these are different entry points into the cloud. Such that, if you're running a certain type of database on-premise and you want to move those workloads to the cloud, we want to give you an opportunity to do that. And that's essentially what many of these hexagons are about. \n",
    "\n",
    "![](img/7.png)\n",
    "\n",
    "So, when you are looking at running things on Google Cloud, part of it might be simply taking the same workload that you have, and simply changing where you do the computation.\n",
    "\n",
    "![](img/27.png)\n",
    "\n",
    "And the reason that we are not going to change the where is because Cloud can be cheaper. Cloud can be a little more secure, and that's basically what you want. \n",
    "\n",
    "\n",
    "But on the other hand, there could be other instances where the reason that you're moving to the cloud is because you want additional scale and additional reliability. \n",
    "\n",
    "\n",
    "![](img/29.png)\n",
    "\n",
    "Maybe you want very reliable large scale messaging and you're very interested then in Cloud Pub/Sub, which is a serverless. You don't have to actually even launch a server to get a message system going. A messaging service that you can, basically, publish to and subscribe to without actually having a cluster of POP subservers running, for example. \n",
    "\n",
    "\n",
    "Similarly, you have needs around scalable, reliable data processing, data flow, dataproc. Those are all different ways to do that. But the idea is that these are all stepping stones to the transformation the cloud can provide. And that’s where we get to the changing how you do your computation. \n",
    "\n",
    "![](img/30.png)\n",
    "\n",
    "In terms of data expiration, in terms of business intelligence, in terms of machine learning, and all of those kinds of things. So, for example, for data warehousing, we believe that Google BigQuery is probably the best solution for a whole host of companies But you may not be ready to move into a so completely serverless, fully managed data warehouse solution yet. And so, a stepping stone to that could be, take the workloads that you have. And run them on Cloud Dataproc. And over time, maybe migrate some of your data sets, some of your workloads into BigQuery so you can forget about the infrastructure management that you're probably doing today. So the idea behind this module is to talk about those migrations. \n",
    "\n",
    "![](img/31.png)\n",
    "\n",
    "The other thing that we want to talk about, and this is something that's throughout this course, is this transformation that's happening in our industry around machine learning. So this is Eric Schmidt again, the Executive Chairman of the Board at Google. In many ways he's a visionary, the person who says this is where we're headed and the whole company heads that way. So this is Eric Schmidt a couple of years ago, talking about machine learning. And he says, machine learning is the next transformation. \n",
    "\n",
    "\n",
    "It's the new thing that we're going to be doing. And what's new about it? Well, the thing that you're changing is a programming paradigm. Instead of you programming a computer, Eric says, you're going to teach a computer to do something. And it's going to do what you want. \n",
    "\n",
    "Notice here that the way Eric's talking about machine learning is that he's not talking about it from the point of view of data. We know that machine learning is about learning from data. So data is very important, but Eric is actually having us think a little bit more about machine learning and saying this is about logic. It's about replacing the way you do things and, of course, wired saw this and the headline is, soon we won't program computers, we'll train them like dogs. \n",
    "\n",
    "![](img/32.png)\n",
    "\n",
    "But that kind of gets at the point that Eric Schmidt's also making. The way that dogs learn is not in terms of rules saying do this, do this, do this, if this do that. But instead by being exposed to a number of scenarios and figuring out what's correct and what's incorrect behavior in those scenarios. And over time, the dog learns, and the idea is that over time our computer programs can also learn. \n",
    "\n",
    "Now, machine learning has been around for a very long time. Neural networks have been around since the 70s, for example. \n",
    "\n",
    "![](img/33.png)\n",
    "\n",
    "But I think machine learning basically came into people's consciousness, only with recommendation engines. This is where you go to an e-commerce site and then you basically see a message that says, people who bought this also bought this. Or you may have gone to a music service and the music service is basically giving you a playing list of songs that you might like. \n",
    "\n",
    "\n",
    "Daily song list. That's kind of personally curated for you, except that it cannot have been really personally curated by a person, because that doesn't scale. Instead, it's that computer algorithm that's somehow figuring out the kind of songs that you would like and putting together a playlist for you. Or movies, where you basically get movie recommendations. So what's a recommendation engine? It's all of these kinds of things where it says, you would like this product because you've liked products like this in the past. So who uses them? E-commerce sites, movie sites, etc. So the example that we're going to pick up is that we're going to try to basically talk about rental houses and say as our example that we want to be able to recommend to people. Here are some houses for you to go look at. \n",
    "\n",
    "\n",
    "So if somebody's hunting around for houses and we would recommend houses to them. \n",
    "\n",
    "So how would they work? How do recommendation agents work? \n",
    "\n",
    "![](img/34.png)\n",
    "\n",
    "The way it works often is that you start with ratings. So either explicitly or implicitly, users go through a catalog and over time they've rated some houses. So you could rate a house with actually going and visiting the house. A user comes back and says that was not a house they liked. I’m going to give that a two or a five. And they give you a rating over time, that you've got ratings from that user for a few houses and you can use that as part of your training dataset. But you could also have implicit ratings, and implicit ratings could be the user clicked on a link for that house. That basically indicates that they are interested in the house. Or if you have a mobile app, you can actually look at how long somebody looked at the house before they scrolled past it. So that could be part of the rating. So you could have explicit ratings in the form of stars or implicit ratings in the form of links or time that you've looked at it, etc. So you have those ratings. The thing to realize is, how many of the products actually get rated? So let's say you have a catalog of 10,000 houses. How many of those houses per individual user have rated? \n",
    "\n",
    "Maybe five, maybe ten. That's pretty much it. \n",
    "\n",
    "So it's from these ratings. So let's say every user has rated 5 houses. If we have a million users, and we have 100,000 objects, we essentially have a matrix that's got 1 million rows and 100,000 columns. And that matrix is extremely sparse because every user has rated only maybe 5 or 10 of those items and the remaining 995 objects are unrated. But we need to come up with the rating for each of them. So that's basically what the ML model needs to do. It needs to predict what a user would rate a house that they haven't yet visited. \n",
    "\n",
    "In order to do that, we need to build a ML model. \n",
    "\n",
    "But let's say we have that machine learning module that can basically say, this house the user would rate it 2.1, this house they would rate it 3.4, this house they may rate it 1.7, etc., etc. Essentially the recommended part of it is simply to go through that catalog of 100,000 houses. Take for every user, and then take the ratings, find the top five ratings and basically suggest those to them. That's essentially all they're recommending. Their recommending is a very cheap operation, it's just a sort find the top five, return the top five. The interesting thing is how you build the ML model. So how would you build this model to predict the rating of a house for a user? \n",
    "\n",
    "Intuitively, we kind of know how this ought to work. \n",
    "\n",
    "We will basically say that if we have two users that happen to have rated house A, the same, because they happened to have visited and the first user rates it a four, second user also rates it a four. We could kind of say intuitively that let's go to the first user, find out all the other houses that they rated, and say these two guys now if they rated this house both a four, maybe they will share the rating for everything else. \n",
    "\n",
    "So in other words, you could basically say who is this user like. \n",
    "\n",
    "![](img/35.png)\n",
    "\n",
    "So go through all of those users for those houses and say, has this user rated the same house as somebody else, and let's kind of propagate that rating that way. Remember the whole idea is to fill out the metrics of every user, let's say a million users, and every item, let's say 100,000 items. So this 1 million by 100,000 matrix needs to get filled out. And could get filled out starting by looking at whether two rows, which each row corresponds to a user, are not similar to each other. The other way that you can do this is to look at the houses themselves, and seek popular houses will tend to be like. Unpopular houses will tend not to be like. So you might have a house that everyone who's looked at has agreed that it's a dump. They're basically rating it one or two out of five. \n",
    "\n",
    "You could kind of, from that infer that everyone is going to rate it at one or two. It's a dump. The house is a bad house, everyone's going to rate it badly. So in the absence of other information about a particular user, and the kinds of things that they like, go with the majority vote, and that's the second way that you do it. So essentially we need to cluster users, we need to cluster items, and combine the two of them to produce a rating. \n",
    "\n",
    "So how often do we need to compute this predictive rating? Remember that computing the predicted rating is filling out that huge matrix. It takes a long time. It may take hours. But when do you need to recompute the rating? \n",
    "\n",
    "You will need to recompute the rating whenever the user comes in and rates a new house, or something else happens. Maybe some other user rates a house that causes your rating to change. Now this is not a very common occurrence. It's not something that you may have to do up to the minute. So this is the kind of thing that many people do as a batch job. They may say well, I'm going to do this once a week. Based on all the ratings that I have, I'm going to create my new recommendation model. And that is the recommendation model that I'm going to basically use for doing product recommendations. \n",
    "\n",
    "And because this is the batch job run occasionally, a good place to run it would be Hadoop. So we will look at running this using PySpark, that's Spark in Python, on a Hadoop cluster. Which on GCP, the Hadoop cluster is called Dataproc. So we're going to use the Dataproc cluster to run a Python Spark job. And that's how we're going to compute the predicted rating. \n",
    "\n",
    "The second part of the question is where do you see the results? \n",
    "\n",
    "And if you're going to talk about where to save it, let's think about what to save. What are we saving? \n",
    "\n",
    "What we're saving are the top five houses for every user. So if you have a million users, you have five houses per user, that's 5 million houses. This is small data. So this is the kind of thing that you could store in a relational database. It's transactional, because every day you may want to keep updating these recommendations. So you might want to do it in the context of a transaction for example and so a relation database makes perfect sense. So the second part of this use case is to store our results in a MySQL database. And the way you do a MySQL database in Google Cloud is to use Cloud SQL. It's a managed MySQL offering. So in order to solve this recommendation problem, we're going to look at MySQL on Cloud SQL, and we're going to look at Python Spark on Dataproc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your SQL database in the cloud\n",
    "\n",
    "So let's start off by looking at how to do a relational database in the cloud. \n",
    "\n",
    "![](img/36.png)\n",
    "\n",
    "Okay, so as we mentioned, the data that we have is relatively small, a few hundred gigabytes maybe, maximum. So that's the kind of thing that Cloud SQL handles perfectly well. So we would use Cloud SQL. The capacity of a gigabyte is fine for a relational offering. You use it like a relational database. You do selects, you do inserts. You do updates, you use deletes. And you can update an individual field. So that's what Cloud SQL is. \n",
    "\n",
    "Later on we'll look at other access patterns that you could use if you needed to go at larger scale or we want to basically deal with objects rather than relational data. What if we need to deal with high throughput? What if we need to deal with petabytes of data and still get very quick, timely responses? But right now based on our access pattern of relatively small data that we want to use in a very familiar way, Cloud SQL is great service. \n",
    "\n",
    "![](img/37.png)\n",
    "\n",
    "So Cloud SQL is essentially MySQL. MySQL is an open source database and Cloud SQL is Google-managed MySQL. At this point you've got to be wondering, wait a minute, in the last chapter we looked at Cloud Launcher. And if I wanted to run WordPress on Google, I could just go to Cloud Launcher and get a Compute Engine VM that's pre-installed with WordPress ready to go. \n",
    "\n",
    "Why can't I do that with MySQL? \n",
    "\n",
    "Why does Google have to manage MySQL? \n",
    "\n",
    "Isn't it just getting a Compute Engine VM and installing MySQL on it? Well, not quite. By having Google Cloud manage MySQL, we get a few extra benefits. First benefit, flexible pricing. One of the things that you could do is to say, well, I have this database and it's going to be used only between 9 and 5. So between 5 and 9 just passivate the database. And then so you don't have to pay for it when it's passivated, so that's great. \n",
    "\n",
    "So and that's something that you can just have Cloud SQL do for you. You don't have to write any extra infrastructure handling to passivate the database when you're not using it. So this is particularly useful for things like tester databases. A database that needs to be run only when a unit test is running. If a unit test isn't running, we don't need the database, we don't have to use it. \n",
    "\n",
    "Second advantage, it's familiar. It's MySQL and so every workload that you have that uses MySQL, \n",
    "\n",
    "it's a prime candidate to run on Cloud SQL. \n",
    "\n",
    "But we're talking about the advantages that running it on Google can give your relational database workloads. So first of all, is that flexible pricing. You don't have to pay for a machine if you aren't using it. And this is particularly useful for machines that aren't used 24/7. Secondly, Google manages the backups so you can just go to Cloud SQL and say I would like you to back up this database every Thursday at 2:00 AM and it will happen. \n",
    "\n",
    "Thirdly, you can connect to it from anywhere. It's on the Cloud so you don't have to worry about your company firewalls and things like that. You have a database that needs to be connectable from anywhere. \n",
    "\n",
    "This is a good solution for it. \n",
    "\n",
    "You get automatic replication if you need to. \n",
    "\n",
    "You also get very fast connections from GCE, Google Compute Engine, and GAE, Google App Engine. Why is that? \n",
    "\n",
    "Well, because Cloud SQL is on a machine that lives in Google Cloud, and all these things are also in Google Cloud. So as long as you're running these apps in the same region, \n",
    "\n",
    "\n",
    "they get to take advantage of Google's petabit per second interconnect networking. And that by section bandwidth off 1 petabit per second. So that's extremely, extremely fast. And so you get really fast connections between MySQL and other applications that are running on Google Cloud. And finally, not the least, is this idea that you have Google Security taking care of your instances. So you're not at the mercy of a randomly rotating set of security people. You basically have it being secured by people who do it as their job day in and day out. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab : Working with Cloud SQL\n",
    "\n",
    "![](img/38.png)\n",
    "\n",
    "### Overview\n",
    "In this lab you populate rentals data in Cloud SQL for the rentals recommendation engine to use.\n",
    "\n",
    "### What you need\n",
    "To complete this lab, you need:\n",
    "- A project created on Google Cloud Platform [Lab 1]\n",
    "- A Cloud Storage bucket created [Lab 2b]\n",
    "\n",
    "### What you learn\n",
    "In this lab, you:\n",
    "- Create Cloud SQL instance\n",
    "- Create database tables by importing .sql files from Cloud Storage\n",
    "- Populate the tables by importing .csv files from Cloud Storage\n",
    "- Allow access to Cloud SQL\n",
    "- Explore the rentals data using SQL statements from CloudShell\n",
    "\n",
    "### Start the Codelab\n",
    "https://codelabs.developers.google.com/codelabs/cpb100-cloud-sql/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managed Hadoop in the cloud\n",
    "\n",
    "So now that we have created a cloud SQL instance loaded the data, the next thing that we want to do is to use Hadoop. Spark in particular to create recommendations. Now, There's been a rich open source ecosystem that's come around the big data space.\n",
    "\n",
    "![](img/39.png)\n",
    "\n",
    "Hadoop, which is the Canonical open source MapReduce framework that was created by Doug Cutting after he read the papers out of Google that talked about MapReduce. So essentially, Hadoop was written, it was to reuse Hadoop, even though you had some streaming packages and other things. The primary way in which you would use Hadoop was to write MapReduce programs in Java. This could get relatively verbose, so people looked for simpler ways to use Hadoop. And several simple solutions came about, simple, very powerful solutions came about. One was called Pig, and the idea behind Pig was that you hd a scripting language. And you would write your MapReduce programs in that scripting language. And it could be much higher level, it is almost like an ETL language, extraction, transformation, loading of data language. So you would say load data from here, do this counting, do that kind of different datasets, do joins, and then you would basically store the data back. \n",
    "\n",
    "So Pig provided a very convenient scripting language, and once you wrote those scripts, those script would get converted into MapReduce programs, into Hadoop MapReduce programs, and they would get run on the Hadoop cluster. \n",
    "\n",
    "Another simplification that came about was this idea that most of the time you are dealing with structured data anyway. And if you have structured data, what if you have a schema, then you associate that schema on top of your structured data stored on a distributed file system? In the case of Hadoop, you would store it on HTFS. So you would basically have your distributed file system, you would store it. And then you'd be able to basically do queries on your hive using Hive. \n",
    "\n",
    "So those were two simplifications that came about. But now, modern day programs on the Hadoop ecosystem tend to get written in Spark. Spark is very fast, it's interactive, and it has a bunch of libraries that allow you to deal with SQL, streaming data, machine learning, etc. \n",
    "\n",
    "So when people say that they're using big data on Hadoop, typically, they're talking about now they have Hadoop, MapReduce jobs, the low level ones. They may have Pig Scripts, they may have Hive statements, they may have Python Spark programs, Spark car programs, etc. So when Google says that we want to be able to take your Hadoop jobs to meet you where you are and to be able to run them on Google Cloud. We basically give you a way to run Hadoop, Pig, Hive, Spark, Presto, a variety of different Hadoop ecosystem things on the Google Cloud platform. And the way you do this is with Dataproc. \n",
    "\n",
    "![](img/40.png)\n",
    "\n",
    "\n",
    "The Dataproc is Google managed Hadoop, Pig, Hive, Spark programs. \n",
    "\n",
    "\n",
    "So, here, Dataproc is a cluster, right, so it's not just a question of creating a single machine. \n",
    "\n",
    "\n",
    "Like as we did with MySQL when Cloud SQL. But this is over a cluster of machines that are automatically all set up. You have master nodes, you have worker nodes, you can set and all of those things. The creation of them is extremely simple and straightforward. You can get a Dataproc cluster up and going in less than two minutes, so that's great. You can also resize it. So you can have a Dataproc cluster, and you can say well, I want to add a few more worker nodes. I want to remove a few worker nodes, but keep the cluster up. And know that kind of resizing is extremely simple and very straightforward. At the same time, it's very familiar, because it is your Hadoop, Pig, Hive, Spark jobs, they work unchanged. If you want, You can have HDFS and you can have your data in HDFS on the Dataproc cluster and read from HDFS. \n",
    "\n",
    "\n",
    "However, a better way to do this is to take advantage of the very nice integration that Dataproc provides with GCP. \n",
    "\n",
    "\n",
    "What kind of integration? \n",
    "\n",
    "\n",
    "Instead of storing your data on HDFS, you can store your data on Google Cloud storage. Now, what's the difference? \n",
    "\n",
    "\n",
    "If you store your data on HDFS, you're basically taking your data, you're splitting it up into pieces, and storing it on the Dataproc cluster. \n",
    "\n",
    "\n",
    "What that means, and this is probably what you're doing today if you're using HDFS, your cluster has to be up. Because if you delete your cluster, typically your data also goes away, and you can't do that. So you're paying for compute even though all you need is a storage. \n",
    "\n",
    "\n",
    "But if you store your data on Google Cloud Storage, you get the huge advantage that your cluster life cycle and your storage life cycle are now separated. So you can have your data on GCS. Bring up a cluster, have it do a job, and then delete the cluster. So that's an important distinction. When you look at the cost of Dataproc, you should realize that you don't tend to keep a Dataproc cluster up and running 24/7 for months at a time. \n",
    "\n",
    "\n",
    "Instead, you think of a Dataproc cluster as a job specific resource. Every job that you want to run, create a new Dataproc cluster. And when the job is done, delete it. But if your going to be deleting a cluster, you can't be moving data back and forth to it. And this is where storing your data on GCS is so powerful. You can store your data on Google Cloud Storage, and your data remains there. And they get, no, moved to the Dataproc cluster. And the reason that we can do this, again, comes down to the quality of the networking within the Google data centers. Make sure that you store your data on GCS, ideally in a single region bucket. And create a Dataproc cluster in that same region. \n",
    "\n",
    "\n",
    "And then you don't have to keep your Dataproc cluster up and around for way too long. In addition, Dataproc works very well with flexible virtual machines. This is basically the preemptable VMs that we talked about. So when you create your Dataproc cluster with a bunch nodes, create a few of them that are standard virtual machines, and a lot of them that are flexible virtual machines. If you get those virtual machines, you basically save yourself a lot of money. If you don't, well, to bad, but you still got your work done as you would have normally gotten them done using the standard machines. \n",
    "\n",
    "And in addition to all of that, you basically get all the advantages of running on the Google Cloud, in terms of security, in terms of automated cluster management, in terms of image versioning. To make sure that all of your versions of Hadoop and Pig etc, they all work very well together. \n",
    "\n",
    "So bottom line, Dataproc reduces the amount of cost that you're putting in. It reduces your complexity. It let's you just create a job and run it without worrying about managing that infrastructure yourself. Think about a Dataproc cluster as something that's specific to just that job that you're actually executing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab : Providing recommendations with Cloud Dataproc\n",
    "\n",
    "### Overview\n",
    "In this lab, you carry out recommendations machine learning using Dataproc.\n",
    "\n",
    "### What you need\n",
    "To complete this lab, you need:\n",
    "- A Cloud SQL instance populated with rentals data [Lab 3a]\n",
    "- A Cloud Shell terminal with the git repo corresponding to the labs [Lab 3a]\n",
    "\n",
    "### What you learn\n",
    "In this lab, you:\n",
    "- Launch DataprocRun Spark\n",
    "- ML jobs using Dataproc\n",
    "\n",
    "### Start the Codelab\n",
    "https://codelabs.developers.google.com/codelabs/cpb100-dataproc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](img/41.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
